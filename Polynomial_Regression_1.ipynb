{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ec2150",
   "metadata": {},
   "source": [
    "Polynomial Regression using the loss function: \\begin{equation}|x - x(i)|^{4}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69934cc2",
   "metadata": {},
   "source": [
    "Here we use this loss function to fit the line:\\begin{equation}y = ax^{2} + bx + c\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8a8c4",
   "metadata": {},
   "source": [
    "where we calculate the the two coeffecients of the quadratic polynomial a and b and the y-intercept c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa30ecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdDElEQVR4nO3df5RU9Znn8ffTTTONadKoTHoUdCBnIGPWHyF0JInOpnuiY6/jQhJNQDdudMOwSRaPG43nkM0ex5g5J0RXJzHhjBJ1NNlIT+JOSCdhhj1ntI6bRBQQRyIsWUYxduOviM3pRhi6m2f/qCqoLqq6bv24VXXv/bzO4Vh17+XW99uN96nv83zv95q7IyIiydXS6AaIiEhjKRCIiCScAoGISMIpEIiIJJwCgYhIwk1rdAPKNXv2bJ83b17J4w4dOsQ73vGO8BvUZNTvZFG/k6Wafm/fvv137v77hfZFLhDMmzePbdu2lTwulUrR09MTfoOajPqdLOp3slTTbzN7qdg+pYZERBJOgUBEJOEUCEREEi5yNYJCxsbGGBwc5MiRI8e3dXZ2snv37ga2qjE6OjoYGxujra2t0U0RkYiIRSAYHBxk5syZzJs3DzMDYGRkhJkzZza4ZfXl7gwODjI4OMj8+fMb3RwRiYjQUkNm9qCZvW5mvy6y38zsHjPba2bPmdn7K/2sI0eOcPrppx8PAkllZnR2dk4aGYlI9G3cMcRFax9j59BBLlr7GBt3DNX0/GHWCB4C+qbY/++ABZk/q4C/qebDkh4EsvRzEImXjTuG+PLf72Ro+DAAQ8OH+fLf76xpMAgtELj7E8CBKQ5ZBnzP07YAs8zsjLDaIyISRXdu3sPhsYlJ2w6PTXDn5j01+4xG1gjmAC/nvB/MbHsl/0AzW0V61EBXVxepVGrS/s7OTkZGRiZtm5iYOGlb2H72s59xzTXXsG3bNhYuXFj0uHXr1nH99ddzyimnVPQ5P/jBD3jmmWe46667Tto3MTHBkSNHTvoZxd3o6Gji+gzqdxKsOGsEzkq/7poBN583ntkzUrOfQSSKxe6+HlgP0N3d7fl31u3evfukwnAjisUbN27k4osvZmBggK9+9atFj7v33ntZuXJlxe1rb29n+vTpBf/+yMgI7e3tLFq0qKJzR5XuNE2WJPR7444h7ty8h6HhE5fpm88b566d6fdzZs3ghv/QU5PPauR9BEMcj3MAzM1sC1228DJ/zc9rVngZHR3lF7/4BQ888AD9/f1A+tv5l770Jc4991zOP/98vv3tb3PPPfewf/9+ent76e3tBdJTPrMeffRRrrvuOgB++tOfsmTJEhYtWsQll1zCa6+9VnU7RaT55dcF8s1oa+WWy95Ts89r5IhgAFhtZv3AEuCgu5+UFqq17A84m3PLFl4APrZoTsXn/clPfkJfXx8LFy7k9NNPZ/v27Tz99NPs27ePZ599lmnTpnHgwAFOO+007r77bh5//HFmz5495TkvvvhitmzZgplx//33c8cddxRMB4lIvBSqC2TNmTWDWy57T1XXq3yhBQIz2wD0ALPNbBD4S6ANwN3vBTYBlwN7gbeB68NqS66pCi/V/GA3bNjAjTfeCMCKFSvYsGEDL774Ip/73OeYNi39Yz7ttNPKOufg4CDLly/nlVde4ejRo7o3QCQh9hcZCQD8cs2f1vzzQgsE7n51if0O/JewPr+YYj/gqX7wpRw4cIDHHnuMnTt3YmZMTExgZnzgAx8I9Pdzp3zm3gNwww03cNNNN7F06VJSqRS33XZbxW0UkeaXrQt4kf3TW8PJ5iduraEzZ80oa3sQjz76KNdeey0vvfQS+/bt4+WXX2b+/PlccMEF3HfffYyPp6v8Bw6kZ9POnDlz0oymrq4udu/ezbFjx/jxj398fPvBgweZMyc9Snn44Ycrbp+INL8gdYGuzvZQPjtxgeCWy97DjLbWSduqLbxs2LCBj3/845O2XXnllbzyyiucffbZnH/++VxwwQU88sgjAKxatYq+vr7jxeK1a9dyxRVX8OEPf5gzzjhxK8Vtt93GJz/5SRYvXlyyniAi0VaqLvD1T5zHrBnhrCFm6QxNdHR3d3v+g2l2797NOeecM2nbVNNHs8Ov/cOHOTOEwksjjYyMMDg4eNLPI+6SMJ2wEPU7Puav+XnBlJABL679c6DqB9Nsd/fuQvsicR9BrX1s0ZzYXPhFJLpyv5S2mDFR4It5NWnroBIZCEREGi1/KnuhIFDr+wWKiU0gcHctuEb65yAiza9YTaDVjGPudU1bxyIQtLe38+abbyZ+KWp35+DBg7S3hzOzQERqp9iU9WPux2sC9RKLQDB37lwGBwd54403jm87cuRIIi+Ihw4d4oILLmh0M0SkiFL3CtSjJpAvFoGgra3tpLtuU6lU4hZeg3S/9ZhKkeaUXxfIV6+aQL5YBAIRkSio9xpCQSkQiIiE7MSS0oXrAkY4awgFpUAgIhKiUukgaExdIFfilpgQEamnqdJB0Li6QC6NCEREQjTVysaNrAvkUiAQEQlBqWmic2bNaGhdIJcCgYhIjTXrNNFiFAhERGqsWaeJFqNAICJSY8XqAo2eJlqMAoGISI004/IRQSgQiIjUQNTqArkUCEREaiBqdYFcCgQiIlVo9uUjglAgEBGpUBSWjwhCS0yIiFQoCstHBKERgYhIhaKwfEQQCgQiImWK0vIRQSgQiIiUIcrTRItRIBARKUOUp4kWo0AgIhJAHKaJFqNAICJSQlymiRaj6aMiIiXEZZpoMaEGAjPrM7M9ZrbXzNYU2H+2mT1uZjvM7DkzuzzM9oiIVKLUNNGvf+K8yNUFcoWWGjKzVmAdcCkwCGw1swF335Vz2H8Hfujuf2Nm7wU2AfPCapOISDniNk20mDBHBBcCe939BXc/CvQDy/KOceCdmdedwP4Q2yMiEli2LlCsOBz1dFAucy8W66o8sdlVQJ+7r8y8vxZY4u6rc445A/jfwKnAO4BL3H17gXOtAlYBdHV1Le7v7y/5+aOjo3R0dNSiK5GifieL+h2ePa+OcHTiWMF901tb6OpsZ9aMtlDbkK+afvf29m539+5C+xo9a+hq4CF3v8vMPgR838zOdfdJP313Xw+sB+ju7vaenp6SJ06lUgQ5Lm7U72RRv2vvxDTRFgolTQx4ce2fh/LZpYTV7zADwRBwVs77uZltuT4L9AG4+5Nm1g7MBl4PsV0iIgXFfZpoMWHWCLYCC8xsvplNB1YAA3nH/Bb4KICZnQO0A2+E2CYRkaLiPk20mNBGBO4+bmargc1AK/Cguz9vZrcD29x9ALgZ+K6ZfZF04fg6D6toISJSQlxWEy1XqDUCd99Eekpo7rZbc17vAi4Ksw0iIqUkZZpoMY0uFouINFQcVxMtlwKBiCRaHFcTLZcCgYgkUpxXEy2XAoGIJE5Sp4kWo9VHRSRxkjpNtBiNCEQkcZI6TbQYBQIRSYykTxMtRoFARBJB00SLUyAQkUTQNNHiFAhEJLayqaD9w4eLpoOSNE20GAUCEYmlIFNEIVnTRIvR9FERiaVSU0Qh2XWBXBoRiEislLpjGNLpoDMTXhfIpUAgIrERJB2U1CmiU1FqSERiQ3cMV0YjAhGJvCDpoKRPEZ2KAoGIRJrSQdVTakhEIk3poOppRCAikaYF5KqnQCAikaQF5GpHgUBEImf48Bhf/ictIFcrCgQiEjmvHTzC4bHCJU6lg8qnQCAikZFNB6046xiF5rpoAbnKKBCISCRMmiZ6VuFjtIBcZTR9VEQiQdNEw6MRgYg0Nd01HD4FAhFpWrpruD6UGhKRpqV0UH1oRCAiTUfpoPpSIBCRphIkHTS9tUXpoBoKNTVkZn1mtsfM9prZmiLHfMrMdpnZ82b2SJjtEZHmFyQd1NXZXscWxV9oIwIzawXWAZcCg8BWMxtw9105xywAvgxc5O5vmdm7wmqPiDS3ctJBsw7+vzq2LP7CHBFcCOx19xfc/SjQDyzLO+YvgHXu/haAu78eYntEpEll00GlgsAv1/ypagIhMPdia/dVeWKzq4A+d1+ZeX8tsMTdV+ccsxH4DXAR0Arc5u7/WOBcq4BVAF1dXYv7+/tLfv7o6CgdHR016Em0qN/JEpd+73l1hKMTx4rubzFjzqkzmDWjDYhPv8tVTb97e3u3u3t3oX2NLhZPAxYAPcBc4AkzO8/dh3MPcvf1wHqA7u5u7+npKXniVCpFkOPiRv1Olqj3+0Q6qIViCYpCs4Oi3u9KhdXvMAPBEJNXBJmb2ZZrEHjK3ceAF83sN6QDw9YQ2yUiTUA3izWPMGsEW4EFZjbfzKYDK4CBvGM2kh4NYGazgYXACyG2SUSahG4Wax6hjQjcfdzMVgObSef/H3T3583sdmCbuw9k9v2Zme0CJoBb3P3NsNokIo2nm8WaT6g1AnffBGzK23ZrzmsHbsr8EZGYUzqoOQVKDZnZjUG2iYhMRemg5hS0RvCZAtuuq2E7RCTGNu4Y4qK1j5VMB339E+cpHdQAU6aGzOxq4BpgvpnlFnpnAgfCbJiIxIPSQc2vVI3gV8ArwGzgrpztI8BzYTVKROJD6aDmN2UgcPeXgJeAD9WnOSISF5odFB2BZg2Z2QiQXYtiOtAGHHL3d4bVMBGJLqWDoiVQIHD3mdnXZmakF4/7YFiNEpFoUzooWsq+s9jTNgKX1b45IhJlmh0UTUFTQ5/IedsCdANHQmmRiESS0kHRFfTO4n+f83oc2MfJzxYQkYTJFoT3Dx+mxYyJKZa1VzqoeQWtEVwfdkNEJFryRwBTBQHNDmpuQVND7wa+RbpA7MCTwBfdXSuFiiRUqYJwltJBzS9osfgR4IfAGcCZwI+ADWE1SkSaV5CCcJbSQdEQNBCc4u7fd/fxzJ//CbSH2TARaT5Bni3caoah2UFRErRY/A9mtob0A+gdWA5sMrPTANxd6w6JJECQ+wN08Y+eoIHgU5n//ue87StIB4Z316xFItJ0tFxEvAUNBOe4+6T7BsysPX+biMSP7g+Iv6A1gl8F3CYiMaPlIuKv1PMI/gCYA8wws0WAZXa9Ezgl5LaJSAMpHZQcpVJDl5F+Etlc4O6c7SPAfwupTSLSYEoHJUup5xE8DDxsZle6+/+qU5tEpEGCjAJA6aC4CVosPtfM/k3+Rne/vcbtEZEGCTIKAKWD4ihoIBjNed0OXAHsrn1zRKRRgiwZoXRQPAVddC73ecWY2f8ANofSIhGpK6WDJOiIIN8ppAvIIhJhSgcJBF99dCcnnlncArwL+FpYjRKRcJUzCtCSEfEXdERwBXAq8CfALGCTu28Pq1EiEh6NAiRf0DuLlwHfB2YDbcDfmtkNobVKREJTTlFYQSAZgo4IVgIfdPdDAGb2DdIPp/l2WA0TkdpSUViKCRoIDMj9CjHBieUmRKTJKR0kUwkaCP4WeMrMfpx5/zHggVBaJCI1o6KwBBGoRuDudwPXAwcyf65392+W+ntm1mdme8xsb+bBNsWOu9LM3My6A7ZbREoI8jQx0JPEpIz7CNz9GeCZoMebWSuwDrgUGAS2mtmAu+/KO24mcCPwVNBzi0hpulNYggo6a6gSFwJ73f0Fdz9K+jGXywoc9zXgG4AeciNSA0EfLq+isGSZu5c+qpITm10F9Ln7ysz7a4El7r4655j3A19x9yvNLAV8yd23FTjXKmAVQFdX1+L+/v6Snz86OkpHR0dN+hIl6ney5Pd7+PAYQ28d5liJ/6+nt7bQ1dnOrBltYTcxFPp9l6+3t3e7uxdMv1e6xETVzKyF9DMOrit1rLuvB9YDdHd3e09PT8nzp1IpghwXN+p3smT7faIofBRoLXp8XIrCSf9911qYgWAIOCvn/dzMtqyZwLlAyswA/gAYMLOlhUYFIlKYpoZKtcIMBFuBBWY2n3QAWAFck93p7gdJ36kMwFSpIREpTkVhqVZoxWJ3HwdWk16uejfwQ3d/3sxuN7OlYX2uSFJs3DHEnldHVBSWqoVaI3D3TcCmvG23Fjm2J8y2iMRJNh30hT8+xlTf55QOkiAaViwWkfLpTmEJgwKBSESoKCxhUSAQaXJBRwGgorBURoFApIkFHQWAisJSOQUCkSYWZGooKB0k1VEgEGky2VTQ/uHDlFoApsWMby5/nwKAVEWBQKSJlJMKmjNrBnNOnVAQkKopEIg0gXIKwrlTQ1OpVPiNk9hTIBBpsKCjAAPOVC1AQqBAINIgmhYqzUKBQKQBNC1UmokCgUgdlTMKAE0LlfpQIBCpk3JHAVorSOpFgUAkZBoFSLNTIBAJkUYBEgUKBCIh0ChAokSBQKTGNAqQqFEgEKkRjQIkqhQIRGpAowCJMgUCkSpoFCBxoEAgUiGNAiQuFAhEyqRRgMSNAoFIGTQKkDhSIBAJQKMAiTMFApESNAqQuFMgEClCowBJCgUCkQI0CpAkUSAQyaFRgCSRAoFIhkYBklQKBJJ4GgVI0oUaCMysD/gW0Arc7+5r8/bfBKwExoE3gP/k7i+F2SaRXBoFiIQYCMysFVgHXAoMAlvNbMDdd+UctgPodve3zezzwB3A8rDaJAInRgD7hw/TYsaEe8m/o1GAxFmYI4ILgb3u/gKAmfUDy4DjgcDdH885fgvw6RDbIwmWm/4xIHvpLxUENAqQJDAP8G2oohObXQX0ufvKzPtrgSXuvrrI8d8BXnX3vyqwbxWwCqCrq2txf39/yc8fHR2lo6Ojih5Ek/p9suHDYwy9dZhjZf5bn97aQldnO7NmtNWiiaHQ7ztZqul3b2/vdnfvLrSvKYrFZvZpoBv4SKH97r4eWA/Q3d3tPT09Jc+ZSqUIclzcqN8nnBgFHCVdpgomSqMA/b6TJax+hxkIhoCzct7PzWybxMwuAb4CfMTd/zXE9kiClFMEBmg145g7Z6oWIAkUZiDYCiwws/mkA8AK4JrcA8xsEXAf6RTS6yG2RRKi3KmgEK0RgEgYQgsE7j5uZquBzaTH5Q+6+/Nmdjuwzd0HgDuBDuBHZgbwW3dfGlabJJ427hjitVdHuG7NzycVgqeSPU6zgURCrhG4+yZgU962W3NeXxLm50v8ZVNAX/jjY0BLoCCgi7/IZE1RLBYpl1JAIrWjQCCRUexegCA0ChApToFAIiF/FlDQIKBRgEhpCgTS1CpJAakQLFIeBQJpOkoBidSXAoE0lUpTQC1mfHP5+xQARCqgQCBNodoU0JxTJxQERCqkQCANU8sUUCqVCqOJIomgQCB1Vezir1lAIo2jQCB1U2n+X7OARMKlQCChqyT/n6WLv0j4FAgkFNXk/0EpIJF6UiCQmsh9DnDnjDYOHR1nbCJ9+VcKSKS5KRBIxYp96x8+PBb4HLr4izSeAoFUpNLCby5d/EWagwKBlKWawm+W8v8izUWBQEqqtvALSgGJNDMFAimo2hu/2lqMjvZpDL89pgfCizQ5BQI5rtqLv771i0STAkHCVXvxz9LFXyS6FAgSqFYXf1DhVyQOFAhiLP8mLzN46+2xqi/+SgGJxIsCQcwMHx7jorWPTXmTly7+IpJLgSAGclM9N583ztBw+tdayQU/ly7+IsmgQBBRtZjbX4gu/iLJo0DQxArl+IffHqt4UbdidPEXSTYFgiYTZCG3chZ1K0YXfxHJUiBokLBm9OTLnm9WzohCd/qKSC4FgpAFueBXO6Mnn77ti0g5FAiqMFUOvx4X/FzZz5ne2sJfL3+fLv4iEpgCQY5SF/agF/kwL/hZxRZ1S6VS9CgIiEgZQg0EZtYHfAtoBe5397V5+38P+B6wGHgTWO7u+2rdjqAX+NyZOMUu7PW4yOdSjl9EwhZaIDCzVmAdcCkwCGw1swF335Vz2GeBt9z9j8xsBfANYHkt25H/JK0gF/hGU45fROopzBHBhcBed38BwMz6gWVAbiBYBtyWef0o8B0zM3ev2ZftOzfvOR4Emo2+7YtIM7AaXnMnn9jsKqDP3Vdm3l8LLHH31TnH/DpzzGDm/b9kjvld3rlWAasAurq6Fvf395f8/NHRUTo6Otg5dLBWXapaa4thwPgxZ3prC12d7cya0VbTz8j2O2nU72RRv8vX29u73d27C+2LRLHY3dcD6wG6u7u9p6en5N9JpVL09PTwlcwCbGEp9K2+s4Hf8LP9Thr1O1nU79oKMxAMAWflvJ+b2VbomEEzmwZ0ki4a18wtl71nUo1gKrkzcUrNGlIaR0TiIsxAsBVYYGbzSV/wVwDX5B0zAHwGeBK4CnislvUB4PhFutSsIV3URSSpQgsE7j5uZquBzaSnjz7o7s+b2e3ANncfAB4Avm9me4EDpINFzX1s0Rxd4EVEigi1RuDum4BNedtuzXl9BPhkmG0QEZGptTS6ASIi0lgKBCIiCadAICKScAoEIiIJF9qdxWExszeAlwIcOhv4Xcmj4kf9Thb1O1mq6fcfuvvvF9oRuUAQlJltK3Y7dZyp38mifidLWP1WakhEJOEUCEREEi7OgWB9oxvQIOp3sqjfyRJKv2NbIxARkWDiPCIQEZEAFAhERBIu0oHAzPrMbI+Z7TWzNQX2/56Z/V1m/1NmNq8Bzay5AP2+ycx2mdlzZvZPZvaHjWhnrZXqd85xV5qZm1ksphcG6beZfSrzO3/ezB6pdxvDEODf+dlm9riZ7cj8W7+8Ee2sNTN70MxezzzBsdB+M7N7Mj+X58zs/VV/qLtH8g/ppa3/BXg3MB34Z+C9ecd8Abg383oF8HeNbned+t0LnJJ5/fmk9Dtz3EzgCWAL0N3odtfp970A2AGcmnn/rka3u079Xg98PvP6vcC+Rre7Rn3/t8D7gV8X2X858A+kH5D4QeCpaj8zyiOCC4G97v6Cux8F+oFleccsAx7OvH4U+KiZWR3bGIaS/Xb3x9397czbLaSfDhd1QX7fAF8DvgEcqWfjQhSk338BrHP3twDc/fU6tzEMQfrtwDszrzuB/XVsX2jc/QnSz2cpZhnwPU/bAswyszOq+cwoB4I5wMs57wcz2woe4+7jwEHg9Lq0LjxB+p3rs6S/PURdyX5nhshnufvP69mwkAX5fS8EFprZL81si5n11a114QnS79uAT5vZIOnnntxQn6Y1XLnXgJIi8fB6qYyZfRroBj7S6LaEzcxagLuB6xrclEaYRjo91EN69PeEmZ3n7sONbFQdXA085O53mdmHSD/t8Fx3P9bohkVNlEcE2QffZ83NbCt4jJlNIz18fLMurQtPkH5jZpcAXwGWuvu/1qltYSrV75nAuUDKzPaRzp0OxKBgHOT3PQgMuPuYu78I/IZ0YIiyIP3+LPBDAHd/EmgnvShb3AW6BpQjyoFgK7DAzOab2XTSxeCBvGMGgM9kXl8FPOaZakuEley3mS0C7iMdBOKQL4YS/Xb3g+4+293nufs80rWRpe6+rTHNrZkg/843kh4NYGazSaeKXqhjG8MQpN+/BT4KYGbnkA4Eb9S1lY0xAPzHzOyhDwIH3f2Vak4Y2dSQu4+b2WpgM+kZBg+6+/Nmdjuwzd0HgAdIDxf3ki6+rGhci2sjYL/vBDqAH2Vq479196UNa3QNBOx37ATs92bgz8xsFzAB3OLukR75Buz3zcB3zeyLpAvH18Xgix5mtoF0YJ+dqX/8JdAG4O73kq6HXA7sBd4Grq/6M2PwcxMRkSpEOTUkIiI1oEAgIpJwCgQiIgmnQCAiknAKBCIiCadAIDIFM/tVCOecZ2bX1Pq8IpVSIBCZgrt/OITTzgMUCKRpKBCITMHMRjP/7TGzlJk9amb/18x+kF3J1sz2mdkdZrbTzJ42sz/KbH/IzK7KPxewFvgTM3s2czOUSEMpEIgEtwj4r6TXvn83cFHOvoPufh7wHeCbJc6zBvg/7v4+d//rENopUhYFApHgnnb3wczqls+STvFkbcj574fq3C6RqigQiASXu4rrBJPX6vICr8fJ/D+WWSZ7eqitE6mQAoFIbSzP+e+Tmdf7gMWZ10vJLBwGjJBeNlukKUR29VGRJnOqmT1HetRwdWbbd4GfmNk/A/8IHMpsfw6YyGx/SHUCaTStPipSpcyDcLrd/XeNbotIJZQaEhFJOI0IREQSTiMCEZGEUyAQEUk4BQIRkYRTIBARSTgFAhGRhPv/AruP0TAHotsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.linspace(2,110,100)\n",
    "\n",
    "#helper function to define a polynomial to map the element of X: (y = 8x^2 + 7x + 9)\n",
    "def function(X):\n",
    "    return 8*X**2 + 7*X + 9\n",
    "\n",
    "vector_func = np.vectorize(function)\n",
    "y = vector_func(X)\n",
    "X = X / np.max(X) #Normalising the data by dividing the elements with the max value\n",
    "y= y / np.max(y)  #Scaling the data by dividing the elements with the max value\n",
    "plt.scatter(X,y)\n",
    "plt.grid()\n",
    "plt.xlabel(\"input\");\n",
    "plt.ylabel(\"output\");\n",
    "plt.legend(['Actual']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9930dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "def calculate_gradient():\n",
    "    lr = 0.001\n",
    "    iterations = 5000\n",
    "    N = float(len(X))\n",
    "    #random values for initialisation\n",
    "    w1 = 3\n",
    "    w2 = 1\n",
    "    bias = 2\n",
    "    weights_and_bias = []\n",
    "    for i in range(iterations):\n",
    "        y_pred = w1*X**2 + w2*X + bias\n",
    "        loss = sum((y_pred-y)**4)/N\n",
    "        der_w1 = (4*sum((X**2)*(y_pred - y)**3))/N\n",
    "        der_w2 = (4*sum(X*(y_pred - y)**3))/N\n",
    "        der_b = (4*sum((y_pred-y)**3))/N\n",
    "        w1 = w1 - lr*der_w1\n",
    "        w2 = w2 - lr*der_w2\n",
    "        bias = bias - lr*der_b\n",
    "        losses.append(loss)\n",
    "        print(\"Training loss: {}\".format(loss))\n",
    "    weights_and_bias.append(w1)\n",
    "    weights_and_bias.append(w2)\n",
    "    weights_and_bias.append(bias)\n",
    "    return weights_and_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39019869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 156.02457880341785\n",
      "Training loss: 113.24513862388343\n",
      "Training loss: 86.34378200615869\n",
      "Training loss: 68.21540617938801\n",
      "Training loss: 55.3678828036493\n",
      "Training loss: 45.905554986834\n",
      "Training loss: 38.720865211386275\n",
      "Training loss: 33.12879885460835\n",
      "Training loss: 28.685939105358536\n",
      "Training loss: 25.09433646832643\n",
      "Training loss: 22.147436298344143\n",
      "Training loss: 19.69819376973105\n",
      "Training loss: 17.639510115417384\n",
      "Training loss: 15.89181425376741\n",
      "Training loss: 14.394944975038728\n",
      "Training loss: 13.102706226991765\n",
      "Training loss: 11.979131500591233\n",
      "Training loss: 10.995868589263377\n",
      "Training loss: 10.130315313701056\n",
      "Training loss: 9.364268751806499\n",
      "Training loss: 8.682931981636846\n",
      "Training loss: 8.074173833325823\n",
      "Training loss: 7.527970380746667\n",
      "Training loss: 7.035978772084138\n",
      "Training loss: 6.591208642482124\n",
      "Training loss: 6.187766317278691\n",
      "Training loss: 5.820653897100892\n",
      "Training loss: 5.4856101353737055\n",
      "Training loss: 5.1789834363683065\n",
      "Training loss: 4.897629754255661\n",
      "Training loss: 4.638829952832584\n",
      "Training loss: 4.4002224897668825\n",
      "Training loss: 4.179748254447793\n",
      "Training loss: 3.9756051094072364\n",
      "Training loss: 3.78621022825967\n",
      "Training loss: 3.6101687353887626\n",
      "Training loss: 3.4462474680283752\n",
      "Training loss: 3.2933529244345108\n",
      "Training loss: 3.1505126504031593\n",
      "Training loss: 3.016859463617538\n",
      "Training loss: 2.8916180309705912\n",
      "Training loss: 2.774093405402659\n",
      "Training loss: 2.6636612014127565\n",
      "Training loss: 2.559759146407545\n",
      "Training loss: 2.4618797916194324\n",
      "Training loss: 2.3695642038903446\n",
      "Training loss: 2.282396490060731\n",
      "Training loss: 2.19999903048375\n",
      "Training loss: 2.122028318441069\n",
      "Training loss: 2.048171318862418\n",
      "Training loss: 1.9781422734501708\n",
      "Training loss: 1.9116798906399746\n",
      "Training loss: 1.8485448682328132\n",
      "Training loss: 1.7885177043667173\n",
      "Training loss: 1.7313967590424135\n",
      "Training loss: 1.6769965339052946\n",
      "Training loss: 1.6251461426016351\n",
      "Training loss: 1.575687947920243\n",
      "Training loss: 1.528476345224277\n",
      "Training loss: 1.4833766744720507\n",
      "Training loss: 1.4402642455022134\n",
      "Training loss: 1.3990234632857443\n",
      "Training loss: 1.3595470415801578\n",
      "Training loss: 1.3217352949067738\n",
      "Training loss: 1.2854955000480837\n",
      "Training loss: 1.2507413193611783\n",
      "Training loss: 1.217392279151663\n",
      "Training loss: 1.1853732971728081\n",
      "Training loss: 1.154614254025703\n",
      "Training loss: 1.125049603853696\n",
      "Training loss: 1.0966180202617963\n",
      "Training loss: 1.0692620738602887\n",
      "Training loss: 1.0429279382410992\n",
      "Training loss: 1.017565121553701\n",
      "Training loss: 0.9931262211613457\n",
      "Training loss: 0.9695666991342695\n",
      "Training loss: 0.9468446765790932\n",
      "Training loss: 0.9249207450174555\n",
      "Training loss: 0.9037577932155989\n",
      "Training loss: 0.8833208480333978\n",
      "Training loss: 0.8635769280089778\n",
      "Training loss: 0.8444949085259741\n",
      "Training loss: 0.8260453975267112\n",
      "Training loss: 0.8082006208379161\n",
      "Training loss: 0.7909343162676106\n",
      "Training loss: 0.7742216357138305\n",
      "Training loss: 0.758039054599087\n",
      "Training loss: 0.7423642880099252\n",
      "Training loss: 0.7271762129795352\n",
      "Training loss: 0.7124547964038775\n",
      "Training loss: 0.6981810281288747\n",
      "Training loss: 0.6843368587885509\n",
      "Training loss: 0.6709051420120403\n",
      "Training loss: 0.6578695806516471\n",
      "Training loss: 0.6452146767150074\n",
      "Training loss: 0.6329256847122646\n",
      "Training loss: 0.6209885681543339\n",
      "Training loss: 0.6093899589610724\n",
      "Training loss: 0.5981171195587628\n",
      "Training loss: 0.5871579074649799\n",
      "Training loss: 0.5765007421758075\n",
      "Training loss: 0.5661345741857469\n",
      "Training loss: 0.5560488559845923\n",
      "Training loss: 0.5462335148882458\n",
      "Training loss: 0.5366789275719897\n",
      "Training loss: 0.527375896185245\n",
      "Training loss: 0.5183156259364545\n",
      "Training loss: 0.5094897040454665\n",
      "Training loss: 0.500890079968801\n",
      "Training loss: 0.4925090468104819\n",
      "Training loss: 0.48433922383781236\n",
      "Training loss: 0.4763735400275838\n",
      "Training loss: 0.4686052185738301\n",
      "Training loss: 0.46102776229336423\n",
      "Training loss: 0.4536349398700771\n",
      "Training loss: 0.4464207728832926\n",
      "Training loss: 0.4393795235694706\n",
      "Training loss: 0.4325056832702023\n",
      "Training loss: 0.42579396152282994\n",
      "Training loss: 0.4192392757531057\n",
      "Training loss: 0.4128367415321875\n",
      "Training loss: 0.40658166336289364\n",
      "Training loss: 0.4004695259625857\n",
      "Training loss: 0.39449598601228303\n",
      "Training loss: 0.38865686434370533\n",
      "Training loss: 0.3829481385378483\n",
      "Training loss: 0.37736593591048306\n",
      "Training loss: 0.371906526861611\n",
      "Training loss: 0.36656631856742694\n",
      "Training loss: 0.36134184899476596\n",
      "Training loss: 0.35622978121930227\n",
      "Training loss: 0.35122689803000084\n",
      "Training loss: 0.3463300968034367\n",
      "Training loss: 0.34153638463265495\n",
      "Training loss: 0.33684287369620836\n",
      "Training loss: 0.3322467768539255\n",
      "Training loss: 0.32774540345679604\n",
      "Training loss: 0.32333615535914534\n",
      "Training loss: 0.31901652312201095\n",
      "Training loss: 0.3147840823972948\n",
      "Training loss: 0.31063649048291964\n",
      "Training loss: 0.3065714830397969\n",
      "Training loss: 0.30258687096196796\n",
      "Training loss: 0.2986805373918013\n",
      "Training loss: 0.2948504348726033\n",
      "Training loss: 0.29109458263145543\n",
      "Training loss: 0.28741106398550564\n",
      "Training loss: 0.2837980238653417\n",
      "Training loss: 0.2802536664494399\n",
      "Training loss: 0.27677625290402297\n",
      "Training loss: 0.273364099222993\n",
      "Training loss: 0.27001557416289823\n",
      "Training loss: 0.266729097268185\n",
      "Training loss: 0.26350313698224176\n",
      "Training loss: 0.2603362088400048\n",
      "Training loss: 0.2572268737381161\n",
      "Training loss: 0.25417373627885265\n",
      "Training loss: 0.2511754431842507\n",
      "Training loss: 0.24823068177703966\n",
      "Training loss: 0.24533817852518847\n",
      "Training loss: 0.24249669764703224\n",
      "Training loss: 0.23970503977411423\n",
      "Training loss: 0.23696204066902568\n",
      "Training loss: 0.2342665699956736\n",
      "Training loss: 0.23161753013953518\n",
      "Training loss: 0.22901385507559013\n",
      "Training loss: 0.22645450928173919\n",
      "Training loss: 0.22393848669563046\n",
      "Training loss: 0.22146480971291954\n",
      "Training loss: 0.2190325282250942\n",
      "Training loss: 0.21664071869508295\n",
      "Training loss: 0.214288483268963\n",
      "Training loss: 0.21197494892216218\n",
      "Training loss: 0.20969926663863078\n",
      "Training loss: 0.20746061062153567\n",
      "Training loss: 0.2052581775341005\n",
      "Training loss: 0.2030911857692789\n",
      "Training loss: 0.20095887474701934\n",
      "Training loss: 0.19886050423793095\n",
      "Training loss: 0.19679535371222598\n",
      "Training loss: 0.19476272171286088\n",
      "Training loss: 0.19276192525185393\n",
      "Training loss: 0.1907922992288056\n",
      "Training loss: 0.18885319587068977\n",
      "Training loss: 0.1869439841920323\n",
      "Training loss: 0.18506404947463198\n",
      "Training loss: 0.18321279276601987\n",
      "Training loss: 0.1813896303958863\n",
      "Training loss: 0.17959399350974678\n",
      "Training loss: 0.17782532761914382\n",
      "Training loss: 0.1760830921677196\n",
      "Training loss: 0.17436676011252122\n",
      "Training loss: 0.1726758175199307\n",
      "Training loss: 0.17100976317563696\n",
      "Training loss: 0.16936810820809534\n",
      "Training loss: 0.1677503757249443\n",
      "Training loss: 0.16615610046187007\n",
      "Training loss: 0.16458482844343536\n",
      "Training loss: 0.16303611665540907\n",
      "Training loss: 0.1615095327281492\n",
      "Training loss: 0.16000465463062022\n",
      "Training loss: 0.15852107037463306\n",
      "Training loss: 0.15705837772892176\n",
      "Training loss: 0.1556161839426838\n",
      "Training loss: 0.154194105478227\n",
      "Training loss: 0.15279176775238182\n",
      "Training loss: 0.1514088048863522\n",
      "Training loss: 0.15004485946369056\n",
      "Training loss: 0.1486995822960975\n",
      "Training loss: 0.14737263219675703\n",
      "Training loss: 0.14606367576093285\n",
      "Training loss: 0.14477238715355864\n",
      "Training loss: 0.14349844790357175\n",
      "Training loss: 0.1422415467047416\n",
      "Training loss: 0.1410013792227652\n",
      "Training loss: 0.13977764790840022\n",
      "Training loss: 0.138570061816423\n",
      "Training loss: 0.137378336430204\n",
      "Training loss: 0.13620219349170254\n",
      "Training loss: 0.1350413608366895\n",
      "Training loss: 0.1338955722350158\n",
      "Training loss: 0.1327645672357501\n",
      "Training loss: 0.13164809101701633\n",
      "Training loss: 0.13054589424036972\n",
      "Training loss: 0.12945773290955398\n",
      "Training loss: 0.12838336823348986\n",
      "Training loss: 0.12732256649335177\n",
      "Training loss: 0.12627509891359082\n",
      "Training loss: 0.12524074153677453\n",
      "Training loss: 0.12421927510211077\n",
      "Training loss: 0.12321048492753596\n",
      "Training loss: 0.1222141607952449\n",
      "Training loss: 0.12123009684055129\n",
      "Training loss: 0.12025809144396538\n",
      "Training loss: 0.119297947126385\n",
      "Training loss: 0.11834947044729648\n",
      "Training loss: 0.11741247190588718\n",
      "Training loss: 0.11648676584497517\n",
      "Training loss: 0.11557217035766375\n",
      "Training loss: 0.11466850719663393\n",
      "Training loss: 0.11377560168598888\n",
      "Training loss: 0.11289328263556968\n",
      "Training loss: 0.11202138225766141\n",
      "Training loss: 0.11115973608601683\n",
      "Training loss: 0.11030818289712045\n",
      "Training loss: 0.10946656463362588\n",
      "Training loss: 0.1086347263298948\n",
      "Training loss: 0.10781251603957537\n",
      "Training loss: 0.10699978476515237\n",
      "Training loss: 0.10619638638941245\n",
      "Training loss: 0.10540217760876054\n",
      "Training loss: 0.10461701786833436\n",
      "Training loss: 0.1038407692988593\n",
      "Training loss: 0.10307329665519119\n",
      "Training loss: 0.10231446725649668\n",
      "Training loss: 0.10156415092801911\n",
      "Training loss: 0.10082221994438424\n",
      "Training loss: 0.10008854897439833\n",
      "Training loss: 0.09936301502729375\n",
      "Training loss: 0.09864549740038003\n",
      "Training loss: 0.0979358776280563\n",
      "Training loss: 0.09723403943214694\n",
      "Training loss: 0.09653986867352032\n",
      "Training loss: 0.09585325330495266\n",
      "Training loss: 0.09517408332520094\n",
      "Training loss: 0.09450225073424941\n",
      "Training loss: 0.09383764948969522\n",
      "Training loss: 0.09318017546424105\n",
      "Training loss: 0.09252972640426084\n",
      "Training loss: 0.0918862018894101\n",
      "Training loss: 0.0912495032932493\n",
      "Training loss: 0.09061953374485103\n",
      "Training loss: 0.0899961980913645\n",
      "Training loss: 0.08937940286150861\n",
      "Training loss: 0.08876905622996874\n",
      "Training loss: 0.08816506798266989\n",
      "Training loss: 0.08756734948290365\n",
      "Training loss: 0.08697581363828383\n",
      "Training loss: 0.08639037486850747\n",
      "Training loss: 0.08581094907390016\n",
      "Training loss: 0.0852374536047227\n",
      "Training loss: 0.08466980723121743\n",
      "Training loss: 0.08410793011437738\n",
      "Training loss: 0.08355174377741299\n",
      "Training loss: 0.08300117107790266\n",
      "Training loss: 0.08245613618060514\n",
      "Training loss: 0.08191656453091674\n",
      "Training loss: 0.08138238282895624\n",
      "Training loss: 0.08085351900426076\n",
      "Training loss: 0.0803299021910746\n",
      "Training loss: 0.07981146270421792\n",
      "Training loss: 0.0792981320155168\n",
      "Training loss: 0.07878984273078184\n",
      "Training loss: 0.0782865285673204\n",
      "Training loss: 0.07778812433196718\n",
      "Training loss: 0.07729456589962132\n",
      "Training loss: 0.07680579019227501\n",
      "Training loss: 0.07632173515852245\n",
      "Training loss: 0.07584233975353452\n",
      "Training loss: 0.07536754391948954\n",
      "Training loss: 0.07489728856644624\n",
      "Training loss: 0.07443151555364853\n",
      "Training loss: 0.07397016767125081\n",
      "Training loss: 0.07351318862245329\n",
      "Training loss: 0.07306052300603551\n",
      "Training loss: 0.07261211629928034\n",
      "Training loss: 0.07216791484127615\n",
      "Training loss: 0.07172786581658873\n",
      "Training loss: 0.07129191723929373\n",
      "Training loss: 0.07086001793736019\n",
      "Training loss: 0.07043211753737638\n",
      "Training loss: 0.07000816644960929\n",
      "Training loss: 0.06958811585338989\n",
      "Training loss: 0.06917191768281587\n",
      "Training loss: 0.06875952461276365\n",
      "Training loss: 0.06835089004520216\n",
      "Training loss: 0.06794596809580192\n",
      "Training loss: 0.06754471358083008\n",
      "Training loss: 0.06714708200432726\n",
      "Training loss: 0.06675302954555615\n",
      "Training loss: 0.06636251304671796\n",
      "Training loss: 0.06597549000092891\n",
      "Training loss: 0.06559191854045036\n",
      "Training loss: 0.06521175742516723\n",
      "Training loss: 0.06483496603130817\n",
      "Training loss: 0.06446150434040165\n",
      "Training loss: 0.06409133292846309\n",
      "Training loss: 0.06372441295540607\n",
      "Training loss: 0.0633607061546745\n",
      "Training loss: 0.0630001748230878\n",
      "Training loss: 0.06264278181089651\n",
      "Training loss: 0.06228849051204225\n",
      "Training loss: 0.061937264854616536\n",
      "Training loss: 0.0615890692915161\n",
      "Training loss: 0.06124386879128703\n",
      "Training loss: 0.06090162882915615\n",
      "Training loss: 0.06056231537824368\n",
      "Training loss: 0.06022589490095329\n",
      "Training loss: 0.05989233434053582\n",
      "Training loss: 0.05956160111282252\n",
      "Training loss: 0.05923366309812341\n",
      "Training loss: 0.05890848863328802\n",
      "Training loss: 0.05858604650392376\n",
      "Training loss: 0.05826630593676911\n",
      "Training loss: 0.05794923659221749\n",
      "Training loss: 0.05763480855698908\n",
      "Training loss: 0.0573229923369468\n",
      "Training loss: 0.057013758850053095\n",
      "Training loss: 0.056707079419465026\n",
      "Training loss: 0.0564029257667636\n",
      "Training loss: 0.056101270005315576\n",
      "Training loss: 0.055802084633763294\n",
      "Training loss: 0.05550534252964159\n",
      "Training loss: 0.055211016943116864\n",
      "Training loss: 0.05491908149084738\n",
      "Training loss: 0.054629510149961015\n",
      "Training loss: 0.05434227725214882\n",
      "Training loss: 0.054057357477870785\n",
      "Training loss: 0.053774725850671994\n",
      "Training loss: 0.05349435773160726\n",
      "Training loss: 0.053216228813770625\n",
      "Training loss: 0.05294031511692864\n",
      "Training loss: 0.05266659298225442\n",
      "Training loss: 0.052395039067160976\n",
      "Training loss: 0.05212563034023074\n",
      "Training loss: 0.051858344076240404\n",
      "Training loss: 0.05159315785127814\n",
      "Training loss: 0.051330049537951714\n",
      "Training loss: 0.05106899730068506\n",
      "Training loss: 0.05080997959110228\n",
      "Training loss: 0.05055297514349629\n",
      "Training loss: 0.05029796297038103\n",
      "Training loss: 0.05004492235812517\n",
      "Training loss: 0.04979383286266564\n",
      "Training loss: 0.049544674305299345\n",
      "Training loss: 0.049297426768551374\n",
      "Training loss: 0.049052070592118385\n",
      "Training loss: 0.0488085863688851\n",
      "Training loss: 0.048566954941013034\n",
      "Training loss: 0.04832715739609927\n",
      "Training loss: 0.04808917506340449\n",
      "Training loss: 0.04785298951014871\n",
      "Training loss: 0.04761858253787248\n",
      "Training loss: 0.04738593617886386\n",
      "Training loss: 0.04715503269264804\n",
      "Training loss: 0.04692585456253995\n",
      "Training loss: 0.046698384492257335\n",
      "Training loss: 0.04647260540259337\n",
      "Training loss: 0.04624850042814867\n",
      "Training loss: 0.04602605291411977\n",
      "Training loss: 0.04580524641314423\n",
      "Training loss: 0.045586064682200694\n",
      "Training loss: 0.04536849167956314\n",
      "Training loss: 0.04515251156180791\n",
      "Training loss: 0.04493810868087241\n",
      "Training loss: 0.0447252675811648\n",
      "Training loss: 0.04451397299672401\n",
      "Training loss: 0.04430420984842749\n",
      "Training loss: 0.04409596324124818\n",
      "Training loss: 0.04388921846155729\n",
      "Training loss: 0.043683960974473905\n",
      "Training loss: 0.0434801764212592\n",
      "Training loss: 0.04327785061675549\n",
      "Training loss: 0.04307696954686807\n",
      "Training loss: 0.04287751936609022\n",
      "Training loss: 0.04267948639506928\n",
      "Training loss: 0.04248285711821431\n",
      "Training loss: 0.04228761818134375\n",
      "Training loss: 0.04209375638937212\n",
      "Training loss: 0.041901258704035646\n",
      "Training loss: 0.041710112241655946\n",
      "Training loss: 0.041520304270940306\n",
      "Training loss: 0.04133182221081944\n",
      "Training loss: 0.041144653628319705\n",
      "Training loss: 0.04095878623647162\n",
      "Training loss: 0.04077420789225214\n",
      "Training loss: 0.04059090659456095\n",
      "Training loss: 0.04040887048223005\n",
      "Training loss: 0.04022808783206539\n",
      "Training loss: 0.04004854705692125\n",
      "Training loss: 0.039870236703805115\n",
      "Training loss: 0.03969314545201388\n",
      "Training loss: 0.03951726211130015\n",
      "Training loss: 0.039342575620067766\n",
      "Training loss: 0.0391690750435972\n",
      "Training loss: 0.03899674957229867\n",
      "Training loss: 0.03882558851999377\n",
      "Training loss: 0.03865558132222449\n",
      "Training loss: 0.03848671753458941\n",
      "Training loss: 0.038318986831105836\n",
      "Training loss: 0.038152379002599004\n",
      "Training loss: 0.037986883955115576\n",
      "Training loss: 0.037822491708363465\n",
      "Training loss: 0.0376591923941753\n",
      "Training loss: 0.037496976254996824\n",
      "Training loss: 0.03733583364239862\n",
      "Training loss: 0.037175755015611316\n",
      "Training loss: 0.03701673094008382\n",
      "Training loss: 0.0368587520860638\n",
      "Training loss: 0.03670180922720069\n",
      "Training loss: 0.03654589323916994\n",
      "Training loss: 0.03639099509831887\n",
      "Training loss: 0.03623710588033356\n",
      "Training loss: 0.036084216758926146\n",
      "Training loss: 0.03593231900454218\n",
      "Training loss: 0.03578140398308856\n",
      "Training loss: 0.03563146315467992\n",
      "Training loss: 0.03548248807240522\n",
      "Training loss: 0.03533447038111246\n",
      "Training loss: 0.03518740181621235\n",
      "Training loss: 0.03504127420249973\n",
      "Training loss: 0.03489607945299348\n",
      "Training loss: 0.03475180956779343\n",
      "Training loss: 0.034608456632954795\n",
      "Training loss: 0.034466012819379416\n",
      "Training loss: 0.03432447038172403\n",
      "Training loss: 0.0341838216573245\n",
      "Training loss: 0.03404405906513633\n",
      "Training loss: 0.033905175104691085\n",
      "Training loss: 0.03376716235506825\n",
      "Training loss: 0.03363001347388252\n",
      "Training loss: 0.03349372119628594\n",
      "Training loss: 0.03335827833398493\n",
      "Training loss: 0.03322367777427186\n",
      "Training loss: 0.033089912479071046\n",
      "Training loss: 0.03295697548399825\n",
      "Training loss: 0.0328248598974348\n",
      "Training loss: 0.032693558899614505\n",
      "Training loss: 0.03256306574172464\n",
      "Training loss: 0.0324333737450193\n",
      "Training loss: 0.03230447629994623\n",
      "Training loss: 0.032176366865286375\n",
      "Training loss: 0.03204903896730529\n",
      "Training loss: 0.031922486198917716\n",
      "Training loss: 0.03179670221886335\n",
      "Training loss: 0.03167168075089513\n",
      "Training loss: 0.031547415582978966\n",
      "Training loss: 0.03142390056650488\n",
      "Training loss: 0.031301129615509606\n",
      "Training loss: 0.031179096705910157\n",
      "Training loss: 0.03105779587474843\n",
      "Training loss: 0.030937221219446583\n",
      "Training loss: 0.030817366897072818\n",
      "Training loss: 0.030698227123617788\n",
      "Training loss: 0.030579796173281153\n",
      "Training loss: 0.030462068377768173\n",
      "Training loss: 0.030345038125596187\n",
      "Training loss: 0.030228699861411027\n",
      "Training loss: 0.030113048085312885\n",
      "Training loss: 0.02999807735219148\n",
      "Training loss: 0.02988378227107102\n",
      "Training loss: 0.029770157504463532\n",
      "Training loss: 0.029657197767731965\n",
      "Training loss: 0.02954489782846161\n",
      "Training loss: 0.029433252505840585\n",
      "Training loss: 0.02932225667004863\n",
      "Training loss: 0.02921190524165438\n",
      "Training loss: 0.029102193191021253\n",
      "Training loss: 0.028993115537721055\n",
      "Training loss: 0.028884667349955935\n",
      "Training loss: 0.028776843743988204\n",
      "Training loss: 0.02866963988357791\n",
      "Training loss: 0.028563050979428052\n",
      "Training loss: 0.028457072288637583\n",
      "Training loss: 0.02835169911416139\n",
      "Training loss: 0.028246926804278247\n",
      "Training loss: 0.028142750752065196\n",
      "Training loss: 0.02803916639487984\n",
      "Training loss: 0.027936169213848977\n",
      "Training loss: 0.0278337547333644\n",
      "Training loss: 0.027731918520585586\n",
      "Training loss: 0.027630656184948723\n",
      "Training loss: 0.027529963377682553\n",
      "Training loss: 0.027429835791330726\n",
      "Training loss: 0.027330269159280326\n",
      "Training loss: 0.027231259255296743\n",
      "Training loss: 0.027132801893064794\n",
      "Training loss: 0.02703489292573589\n",
      "Training loss: 0.02693752824548115\n",
      "Training loss: 0.02684070378305047\n",
      "Training loss: 0.026744415507337357\n",
      "Training loss: 0.02664865942494961\n",
      "Training loss: 0.026553431579785403\n",
      "Training loss: 0.02645872805261515\n",
      "Training loss: 0.026364544960668822\n",
      "Training loss: 0.02627087845722841\n",
      "Training loss: 0.026177724731226086\n",
      "Training loss: 0.026085080006847127\n",
      "Training loss: 0.025992940543138544\n",
      "Training loss: 0.02590130263362231\n",
      "Training loss: 0.025810162605913737\n",
      "Training loss: 0.025719516821344976\n",
      "Training loss: 0.025629361674592996\n",
      "Training loss: 0.025539693593312825\n",
      "Training loss: 0.025450509037774894\n",
      "Training loss: 0.02536180450050778\n",
      "Training loss: 0.025273576505944688\n",
      "Training loss: 0.025185821610075227\n",
      "Training loss: 0.02509853640010096\n",
      "Training loss: 0.025011717494095916\n",
      "Training loss: 0.024925361540670873\n",
      "Training loss: 0.02483946521864241\n",
      "Training loss: 0.024754025236705782\n",
      "Training loss: 0.024669038333111928\n",
      "Training loss: 0.0245845012753489\n",
      "Training loss: 0.024500410859826967\n",
      "Training loss: 0.02441676391156762\n",
      "Training loss: 0.024333557283896873\n",
      "Training loss: 0.024250787858141987\n",
      "Training loss: 0.02416845254333226\n",
      "Training loss: 0.024086548275903407\n",
      "Training loss: 0.024005072019405573\n",
      "Training loss: 0.02392402076421526\n",
      "Training loss: 0.023843391527250377\n",
      "Training loss: 0.023763181351689228\n",
      "Training loss: 0.02368338730669276\n",
      "Training loss: 0.023604006487130347\n",
      "Training loss: 0.02352503601330878\n",
      "Training loss: 0.023446473030704774\n",
      "Training loss: 0.023368314709700674\n",
      "Training loss: 0.023290558245323414\n",
      "Training loss: 0.02321320085698665\n",
      "Training loss: 0.023136239788236132\n",
      "Training loss: 0.023059672306497848\n",
      "Training loss: 0.022983495702829836\n",
      "Training loss: 0.022907707291676464\n",
      "Training loss: 0.022832304410625844\n",
      "Training loss: 0.022757284420170255\n",
      "Training loss: 0.02268264470346958\n",
      "Training loss: 0.022608382666117213\n",
      "Training loss: 0.02253449573590915\n",
      "Training loss: 0.022460981362615633\n",
      "Training loss: 0.02238783701775574\n",
      "Training loss: 0.022315060194374396\n",
      "Training loss: 0.02224264840682231\n",
      "Training loss: 0.022170599190538442\n",
      "Training loss: 0.022098910101834936\n",
      "Training loss: 0.022027578717684824\n",
      "Training loss: 0.021956602635512194\n",
      "Training loss: 0.021885979472984683\n",
      "Training loss: 0.021815706867808586\n",
      "Training loss: 0.021745782477526468\n",
      "Training loss: 0.021676203979316834\n",
      "Training loss: 0.02160696906979652\n",
      "Training loss: 0.021538075464825182\n",
      "Training loss: 0.02146952089931221\n",
      "Training loss: 0.021401303127025678\n",
      "Training loss: 0.02133341992040393\n",
      "Training loss: 0.021265869070368936\n",
      "Training loss: 0.021198648386142067\n",
      "Training loss: 0.021131755695061986\n",
      "Training loss: 0.021065188842404613\n",
      "Training loss: 0.020998945691205195\n",
      "Training loss: 0.020933024122082405\n",
      "Training loss: 0.020867422033064475\n",
      "Training loss: 0.02080213733941747\n",
      "Training loss: 0.02073716797347526\n",
      "Training loss: 0.020672511884471834\n",
      "Training loss: 0.020608167038375123\n",
      "Training loss: 0.02054413141772303\n",
      "Training loss: 0.020480403021461206\n",
      "Training loss: 0.020416979864782667\n",
      "Training loss: 0.02035385997896939\n",
      "Training loss: 0.020291041411235472\n",
      "Training loss: 0.020228522224572266\n",
      "Training loss: 0.02016630049759519\n",
      "Training loss: 0.0201043743243923\n",
      "Training loss: 0.02004274181437461\n",
      "Training loss: 0.019981401092127905\n",
      "Training loss: 0.019920350297266555\n",
      "Training loss: 0.019859587584288727\n",
      "Training loss: 0.019799111122433252\n",
      "Training loss: 0.0197389190955382\n",
      "Training loss: 0.019679009701900983\n",
      "Training loss: 0.019619381154139928\n",
      "Training loss: 0.019560031679057573\n",
      "Training loss: 0.0195009595175054\n",
      "Training loss: 0.01944216292425004\n",
      "Training loss: 0.01938364016784097\n",
      "Training loss: 0.019325389530479783\n",
      "Training loss: 0.019267409307890792\n",
      "Training loss: 0.019209697809193016\n",
      "Training loss: 0.01915225335677382\n",
      "Training loss: 0.019095074286163725\n",
      "Training loss: 0.0190381589459126\n",
      "Training loss: 0.01898150569746736\n",
      "Training loss: 0.018925112915050904\n",
      "Training loss: 0.01886897898554233\n",
      "Training loss: 0.018813102308358656\n",
      "Training loss: 0.01875748129533758\n",
      "Training loss: 0.018702114370621727\n",
      "Training loss: 0.018646999970544045\n",
      "Training loss: 0.018592136543514427\n",
      "Training loss: 0.018537522549907672\n",
      "Training loss: 0.018483156461952595\n",
      "Training loss: 0.018429036763622307\n",
      "Training loss: 0.01837516195052567\n",
      "Training loss: 0.018321530529800113\n",
      "Training loss: 0.018268141020005246\n",
      "Training loss: 0.018214991951018007\n",
      "Training loss: 0.01816208186392865\n",
      "Training loss: 0.018109409310937977\n",
      "Training loss: 0.01805697285525563\n",
      "Training loss: 0.018004771070999504\n",
      "Training loss: 0.017952802543096197\n",
      "Training loss: 0.0179010658671825\n",
      "Training loss: 0.017849559649508028\n",
      "Training loss: 0.01779828250683878\n",
      "Training loss: 0.017747233066361806\n",
      "Training loss: 0.01769640996559081\n",
      "Training loss: 0.017645811852272764\n",
      "Training loss: 0.017595437384295563\n",
      "Training loss: 0.01754528522959663\n",
      "Training loss: 0.01749535406607244\n",
      "Training loss: 0.01744564258148899\n",
      "Training loss: 0.01739614947339336\n",
      "Training loss: 0.01734687344902592\n",
      "Training loss: 0.017297813225233757\n",
      "Training loss: 0.017248967528384822\n",
      "Training loss: 0.017200335094282947\n",
      "Training loss: 0.01715191466808391\n",
      "Training loss: 0.017103705004212252\n",
      "Training loss: 0.017055704866278967\n",
      "Training loss: 0.01700791302700006\n",
      "Training loss: 0.01696032826811597\n",
      "Training loss: 0.016912949380311765\n",
      "Training loss: 0.01686577516313828\n",
      "Training loss: 0.016818804424933876\n",
      "Training loss: 0.016772035982747218\n",
      "Training loss: 0.01672546866226067\n",
      "Training loss: 0.016679101297714643\n",
      "Training loss: 0.0166329327318325\n",
      "Training loss: 0.016586961815746416\n",
      "Training loss: 0.016541187408923953\n",
      "Training loss: 0.016495608379095347\n",
      "Training loss: 0.016450223602181523\n",
      "Training loss: 0.01640503196222293\n",
      "Training loss: 0.016360032351308892\n",
      "Training loss: 0.016315223669508046\n",
      "Training loss: 0.01627060482479911\n",
      "Training loss: 0.016226174733002516\n",
      "Training loss: 0.01618193231771268\n",
      "Training loss: 0.016137876510231155\n",
      "Training loss: 0.016094006249500057\n",
      "Training loss: 0.01605032048203659\n",
      "Training loss: 0.016006818161867987\n",
      "Training loss: 0.01596349825046702\n",
      "Training loss: 0.015920359716688523\n",
      "Training loss: 0.015877401536706047\n",
      "Training loss: 0.015834622693949647\n",
      "Training loss: 0.015792022179043845\n",
      "Training loss: 0.015749598989746615\n",
      "Training loss: 0.01570735213088859\n",
      "Training loss: 0.015665280614313205\n",
      "Training loss: 0.015623383458817218\n",
      "Training loss: 0.015581659690091848\n",
      "Training loss: 0.015540108340664651\n",
      "Training loss: 0.015498728449841742\n",
      "Training loss: 0.015457519063650758\n",
      "Training loss: 0.015416479234784264\n",
      "Training loss: 0.01537560802254383\n",
      "Training loss: 0.015334904492784575\n",
      "Training loss: 0.015294367717860245\n",
      "Training loss: 0.015253996776568914\n",
      "Training loss: 0.015213790754099104\n",
      "Training loss: 0.015173748741976571\n",
      "Training loss: 0.015133869838011335\n",
      "Training loss: 0.015094153146245702\n",
      "Training loss: 0.015054597776902252\n",
      "Training loss: 0.015015202846332733\n",
      "Training loss: 0.014975967476967184\n",
      "Training loss: 0.014936890797263751\n",
      "Training loss: 0.014897971941658856\n",
      "Training loss: 0.01485921005051798\n",
      "Training loss: 0.014820604270086692\n",
      "Training loss: 0.014782153752442365\n",
      "Training loss: 0.014743857655446313\n",
      "Training loss: 0.014705715142696274\n",
      "Training loss: 0.014667725383479512\n",
      "Training loss: 0.01462988755272619\n",
      "Training loss: 0.014592200830963353\n",
      "Training loss: 0.01455466440426927\n",
      "Training loss: 0.014517277464228193\n",
      "Training loss: 0.014480039207885532\n",
      "Training loss: 0.014442948837703654\n",
      "Training loss: 0.014406005561517695\n",
      "Training loss: 0.014369208592492308\n",
      "Training loss: 0.014332557149078327\n",
      "Training loss: 0.014296050454970191\n",
      "Training loss: 0.014259687739063644\n",
      "Training loss: 0.014223468235413707\n",
      "Training loss: 0.014187391183193334\n",
      "Training loss: 0.01415145582665216\n",
      "Training loss: 0.014115661415075862\n",
      "Training loss: 0.01408000720274578\n",
      "Training loss: 0.014044492448898866\n",
      "Training loss: 0.014009116417688214\n",
      "Training loss: 0.01397387837814371\n",
      "Training loss: 0.013938777604133257\n",
      "Training loss: 0.01390381337432418\n",
      "Training loss: 0.013868984972145209\n",
      "Training loss: 0.013834291685748554\n",
      "Training loss: 0.013799732807972509\n",
      "Training loss: 0.013765307636304425\n",
      "Training loss: 0.013731015472843822\n",
      "Training loss: 0.013696855624266091\n",
      "Training loss: 0.013662827401786388\n",
      "Training loss: 0.01362893012112385\n",
      "Training loss: 0.0135951631024662\n",
      "Training loss: 0.013561525670434697\n",
      "Training loss: 0.01352801715404926\n",
      "Training loss: 0.013494636886694144\n",
      "Training loss: 0.013461384206083712\n",
      "Training loss: 0.013428258454228634\n",
      "Training loss: 0.013395258977402435\n",
      "Training loss: 0.013362385126108163\n",
      "Training loss: 0.013329636255045567\n",
      "Training loss: 0.01329701172307844\n",
      "Training loss: 0.013264510893202353\n",
      "Training loss: 0.013232133132512631\n",
      "Training loss: 0.013199877812172554\n",
      "Training loss: 0.01316774430738192\n",
      "Training loss: 0.013135731997346018\n",
      "Training loss: 0.013103840265244596\n",
      "Training loss: 0.013072068498201308\n",
      "Training loss: 0.013040416087253433\n",
      "Training loss: 0.013008882427321742\n",
      "Training loss: 0.012977466917180745\n",
      "Training loss: 0.01294616895942919\n",
      "Training loss: 0.012914987960460813\n",
      "Training loss: 0.012883923330435315\n",
      "Training loss: 0.01285297448324961\n",
      "Training loss: 0.01282214083650942\n",
      "Training loss: 0.012791421811501003\n",
      "Training loss: 0.012760816833163144\n",
      "Training loss: 0.012730325330059546\n",
      "Training loss: 0.012699946734351153\n",
      "Training loss: 0.01266968048176913\n",
      "Training loss: 0.012639526011587757\n",
      "Training loss: 0.012609482766597633\n",
      "Training loss: 0.012579550193079264\n",
      "Training loss: 0.012549727740776652\n",
      "Training loss: 0.012520014862871354\n",
      "Training loss: 0.012490411015956566\n",
      "Training loss: 0.012460915660011555\n",
      "Training loss: 0.01243152825837636\n",
      "Training loss: 0.012402248277726416\n",
      "Training loss: 0.012373075188047871\n",
      "Training loss: 0.012344008462612778\n",
      "Training loss: 0.012315047577954477\n",
      "Training loss: 0.012286192013843488\n",
      "Training loss: 0.012257441253263321\n",
      "Training loss: 0.01222879478238661\n",
      "Training loss: 0.012200252090551502\n",
      "Training loss: 0.012171812670238223\n",
      "Training loss: 0.01214347601704571\n",
      "Training loss: 0.012115241629668663\n",
      "Training loss: 0.012087109009874664\n",
      "Training loss: 0.01205907766248148\n",
      "Training loss: 0.012031147095334684\n",
      "Training loss: 0.012003316819285285\n",
      "Training loss: 0.011975586348167753\n",
      "Training loss: 0.011947955198778022\n",
      "Training loss: 0.011920422890851948\n",
      "Training loss: 0.011892988947043605\n",
      "Training loss: 0.01186565289290412\n",
      "Training loss: 0.01183841425686037\n",
      "Training loss: 0.011811272570194186\n",
      "Training loss: 0.011784227367021439\n",
      "Training loss: 0.011757278184271408\n",
      "Training loss: 0.011730424561666522\n",
      "Training loss: 0.011703666041701845\n",
      "Training loss: 0.011677002169625177\n",
      "Training loss: 0.011650432493417044\n",
      "Training loss: 0.011623956563770967\n",
      "Training loss: 0.011597573934073917\n",
      "Training loss: 0.011571284160386764\n",
      "Training loss: 0.01154508680142516\n",
      "Training loss: 0.011518981418540379\n",
      "Training loss: 0.011492967575700367\n",
      "Training loss: 0.01146704483947102\n",
      "Training loss: 0.011441212778997465\n",
      "Training loss: 0.011415470965985754\n",
      "Training loss: 0.011389818974684376\n",
      "Training loss: 0.011364256381866217\n",
      "Training loss: 0.01133878276681054\n",
      "Training loss: 0.011313397711285079\n",
      "Training loss: 0.011288100799528366\n",
      "Training loss: 0.01126289161823222\n",
      "Training loss: 0.01123776975652421\n",
      "Training loss: 0.011212734805950532\n",
      "Training loss: 0.011187786360458753\n",
      "Training loss: 0.011162924016380913\n",
      "Training loss: 0.011138147372416654\n",
      "Training loss: 0.011113456029616477\n",
      "Training loss: 0.01108884959136526\n",
      "Training loss: 0.011064327663365722\n",
      "Training loss: 0.01103988985362218\n",
      "Training loss: 0.01101553577242447\n",
      "Training loss: 0.010991265032331726\n",
      "Training loss: 0.010967077248156612\n",
      "Training loss: 0.010942972036949598\n",
      "Training loss: 0.010918949017983159\n",
      "Training loss: 0.010895007812736425\n",
      "Training loss: 0.010871148044879675\n",
      "Training loss: 0.010847369340259181\n",
      "Training loss: 0.010823671326881954\n",
      "Training loss: 0.01080005363490083\n",
      "Training loss: 0.010776515896599528\n",
      "Training loss: 0.01075305774637796\n",
      "Training loss: 0.010729678820737396\n",
      "Training loss: 0.010706378758266155\n",
      "Training loss: 0.01068315719962503\n",
      "Training loss: 0.010660013787533057\n",
      "Training loss: 0.010636948166753302\n",
      "Training loss: 0.010613959984078833\n",
      "Training loss: 0.010591048888318735\n",
      "Training loss: 0.010568214530284252\n",
      "Training loss: 0.010545456562775097\n",
      "Training loss: 0.010522774640565764\n",
      "Training loss: 0.010500168420392069\n",
      "Training loss: 0.010477637560937735\n",
      "Training loss: 0.010455181722821096\n",
      "Training loss: 0.01043280056858185\n",
      "Training loss: 0.010410493762668038\n",
      "Training loss: 0.010388260971423067\n",
      "Training loss: 0.010366101863072706\n",
      "Training loss: 0.010344016107712512\n",
      "Training loss: 0.010322003377294948\n",
      "Training loss: 0.010300063345616964\n",
      "Training loss: 0.010278195688307394\n",
      "Training loss: 0.010256400082814688\n",
      "Training loss: 0.010234676208394567\n",
      "Training loss: 0.0102130237460978\n",
      "Training loss: 0.010191442378758268\n",
      "Training loss: 0.010169931790980781\n",
      "Training loss: 0.0101484916691293\n",
      "Training loss: 0.0101271217013151\n",
      "Training loss: 0.010105821577385083\n",
      "Training loss: 0.010084590988910092\n",
      "Training loss: 0.01006342962917341\n",
      "Training loss: 0.010042337193159369\n",
      "Training loss: 0.010021313377541956\n",
      "Training loss: 0.010000357880673515\n",
      "Training loss: 0.009979470402573664\n",
      "Training loss: 0.009958650644918106\n",
      "Training loss: 0.00993789831102771\n",
      "Training loss: 0.009917213105857536\n",
      "Training loss: 0.009896594735986041\n",
      "Training loss: 0.009876042909604296\n",
      "Training loss: 0.00985555733650538\n",
      "Training loss: 0.009835137728073717\n",
      "Training loss: 0.009814783797274655\n",
      "Training loss: 0.009794495258643988\n",
      "Training loss: 0.009774271828277637\n",
      "Training loss: 0.009754113223821404\n",
      "Training loss: 0.009734019164460784\n",
      "Training loss: 0.00971398937091081\n",
      "Training loss: 0.00969402356540612\n",
      "Training loss: 0.00967412147169095\n",
      "Training loss: 0.009654282815009243\n",
      "Training loss: 0.009634507322094935\n",
      "Training loss: 0.009614794721162133\n",
      "Training loss: 0.009595144741895498\n",
      "Training loss: 0.009575557115440747\n",
      "Training loss: 0.009556031574395035\n",
      "Training loss: 0.0095365678527976\n",
      "Training loss: 0.009517165686120363\n",
      "Training loss: 0.009497824811258673\n",
      "Training loss: 0.009478544966522074\n",
      "Training loss: 0.00945932589162516\n",
      "Training loss: 0.009440167327678493\n",
      "Training loss: 0.009421069017179606\n",
      "Training loss: 0.00940203070400403\n",
      "Training loss: 0.00938305213339647\n",
      "Training loss: 0.009364133051961935\n",
      "Training loss: 0.009345273207657072\n",
      "Training loss: 0.0093264723497814\n",
      "Training loss: 0.009307730228968789\n",
      "Training loss: 0.009289046597178835\n",
      "Training loss: 0.009270421207688437\n",
      "Training loss: 0.009251853815083371\n",
      "Training loss: 0.00923334417524987\n",
      "Training loss: 0.009214892045366425\n",
      "Training loss: 0.009196497183895485\n",
      "Training loss: 0.009178159350575307\n",
      "Training loss: 0.009159878306411842\n",
      "Training loss: 0.009141653813670703\n",
      "Training loss: 0.00912348563586916\n",
      "Training loss: 0.009105373537768207\n",
      "Training loss: 0.009087317285364658\n",
      "Training loss: 0.009069316645883454\n",
      "Training loss: 0.009051371387769753\n",
      "Training loss: 0.009033481280681299\n",
      "Training loss: 0.009015646095480813\n",
      "Training loss: 0.00899786560422836\n",
      "Training loss: 0.00898013958017383\n",
      "Training loss: 0.008962467797749453\n",
      "Training loss: 0.008944850032562427\n",
      "Training loss: 0.008927286061387483\n",
      "Training loss: 0.00890977566215962\n",
      "Training loss: 0.008892318613966853\n",
      "Training loss: 0.008874914697042944\n",
      "Training loss: 0.008857563692760346\n",
      "Training loss: 0.008840265383623041\n",
      "Training loss: 0.008823019553259467\n",
      "Training loss: 0.008805825986415591\n",
      "Training loss: 0.008788684468947896\n",
      "Training loss: 0.008771594787816563\n",
      "Training loss: 0.00875455673107854\n",
      "Training loss: 0.0087375700878808\n",
      "Training loss: 0.008720634648453568\n",
      "Training loss: 0.008703750204103623\n",
      "Training loss: 0.008686916547207718\n",
      "Training loss: 0.008670133471205821\n",
      "Training loss: 0.008653400770594734\n",
      "Training loss: 0.008636718240921484\n",
      "Training loss: 0.008620085678776878\n",
      "Training loss: 0.00860350288178912\n",
      "Training loss: 0.008586969648617413\n",
      "Training loss: 0.00857048577894562\n",
      "Training loss: 0.008554051073476052\n",
      "Training loss: 0.008537665333923177\n",
      "Training loss: 0.00852132836300747\n",
      "Training loss: 0.00850503996444922\n",
      "Training loss: 0.008488799942962493\n",
      "Training loss: 0.00847260810424902\n",
      "Training loss: 0.00845646425499224\n",
      "Training loss: 0.008440368202851297\n",
      "Training loss: 0.008424319756455065\n",
      "Training loss: 0.008408318725396387\n",
      "Training loss: 0.008392364920226093\n",
      "Training loss: 0.008376458152447294\n",
      "Training loss: 0.008360598234509596\n",
      "Training loss: 0.008344784979803361\n",
      "Training loss: 0.008329018202654043\n",
      "Training loss: 0.008313297718316508\n",
      "Training loss: 0.008297623342969532\n",
      "Training loss: 0.008281994893710112\n",
      "Training loss: 0.008266412188548029\n",
      "Training loss: 0.00825087504640038\n",
      "Training loss: 0.008235383287086053\n",
      "Training loss: 0.008219936731320378\n",
      "Training loss: 0.008204535200709753\n",
      "Training loss: 0.008189178517746289\n",
      "Training loss: 0.008173866505802572\n",
      "Training loss: 0.008158598989126319\n",
      "Training loss: 0.00814337579283525\n",
      "Training loss: 0.008128196742911799\n",
      "Training loss: 0.008113061666198114\n",
      "Training loss: 0.008097970390390783\n",
      "Training loss: 0.008082922744035858\n",
      "Training loss: 0.008067918556523824\n",
      "Training loss: 0.00805295765808449\n",
      "Training loss: 0.008038039879782154\n",
      "Training loss: 0.008023165053510561\n",
      "Training loss: 0.008008333011988038\n",
      "Training loss: 0.007993543588752648\n",
      "Training loss: 0.007978796618157327\n",
      "Training loss: 0.00796409193536506\n",
      "Training loss: 0.00794942937634414\n",
      "Training loss: 0.007934808777863468\n",
      "Training loss: 0.00792022997748778\n",
      "Training loss: 0.00790569281357301\n",
      "Training loss: 0.007891197125261639\n",
      "Training loss: 0.007876742752478141\n",
      "Training loss: 0.007862329535924301\n",
      "Training loss: 0.007847957317074779\n",
      "Training loss: 0.007833625938172517\n",
      "Training loss: 0.00781933524222431\n",
      "Training loss: 0.00780508507299631\n",
      "Training loss: 0.007790875275009656\n",
      "Training loss: 0.0077767056935360125\n",
      "Training loss: 0.007762576174593286\n",
      "Training loss: 0.007748486564941225\n",
      "Training loss: 0.007734436712077164\n",
      "Training loss: 0.007720426464231706\n",
      "Training loss: 0.007706455670364531\n",
      "Training loss: 0.00769252418016015\n",
      "Training loss: 0.0076786318440237\n",
      "Training loss: 0.007664778513076814\n",
      "Training loss: 0.0076509640391534735\n",
      "Training loss: 0.00763718827479592\n",
      "Training loss: 0.007623451073250565\n",
      "Training loss: 0.007609752288463939\n",
      "Training loss: 0.007596091775078682\n",
      "Training loss: 0.007582469388429568\n",
      "Training loss: 0.007568884984539463\n",
      "Training loss: 0.007555338420115495\n",
      "Training loss: 0.007541829552545021\n",
      "Training loss: 0.0075283582398918344\n",
      "Training loss: 0.007514924340892256\n",
      "Training loss: 0.007501527714951311\n",
      "Training loss: 0.00748816822213892\n",
      "Training loss: 0.007474845723186077\n",
      "Training loss: 0.007461560079481161\n",
      "Training loss: 0.007448311153066117\n",
      "Training loss: 0.007435098806632818\n",
      "Training loss: 0.0074219229035193245\n",
      "Training loss: 0.007408783307706263\n",
      "Training loss: 0.007395679883813178\n",
      "Training loss: 0.007382612497094901\n",
      "Training loss: 0.00736958101343794\n",
      "Training loss: 0.007356585299357002\n",
      "Training loss: 0.007343625221991373\n",
      "Training loss: 0.007330700649101412\n",
      "Training loss: 0.0073178114490650694\n",
      "Training loss: 0.007304957490874409\n",
      "Training loss: 0.007292138644132169\n",
      "Training loss: 0.007279354779048307\n",
      "Training loss: 0.007266605766436623\n",
      "Training loss: 0.007253891477711371\n",
      "Training loss: 0.007241211784883885\n",
      "Training loss: 0.007228566560559245\n",
      "Training loss: 0.007215955677932992\n",
      "Training loss: 0.007203379010787754\n",
      "Training loss: 0.007190836433490075\n",
      "Training loss: 0.007178327820987085\n",
      "Training loss: 0.007165853048803312\n",
      "Training loss: 0.007153411993037453\n",
      "Training loss: 0.007141004530359172\n",
      "Training loss: 0.007128630538005977\n",
      "Training loss: 0.007116289893780029\n",
      "Training loss: 0.007103982476045032\n",
      "Training loss: 0.007091708163723115\n",
      "Training loss: 0.007079466836291757\n",
      "Training loss: 0.007067258373780732\n",
      "Training loss: 0.007055082656769034\n",
      "Training loss: 0.007042939566381855\n",
      "Training loss: 0.007030828984287569\n",
      "Training loss: 0.007018750792694821\n",
      "Training loss: 0.0070067048743494035\n",
      "Training loss: 0.00699469111253147\n",
      "Training loss: 0.006982709391052493\n",
      "Training loss: 0.0069707595942523955\n",
      "Training loss: 0.006958841606996663\n",
      "Training loss: 0.0069469553146734\n",
      "Training loss: 0.00693510060319062\n",
      "Training loss: 0.00692327735897323\n",
      "Training loss: 0.006911485468960339\n",
      "Training loss: 0.006899724820602388\n",
      "Training loss: 0.006887995301858402\n",
      "Training loss: 0.006876296801193186\n",
      "Training loss: 0.0068646292075746195\n",
      "Training loss: 0.0068529924104708964\n",
      "Training loss: 0.006841386299847798\n",
      "Training loss: 0.006829810766166052\n",
      "Training loss: 0.006818265700378555\n",
      "Training loss: 0.006806750993927812\n",
      "Training loss: 0.006795266538743226\n",
      "Training loss: 0.006783812227238506\n",
      "Training loss: 0.00677238795230899\n",
      "Training loss: 0.006760993607329151\n",
      "Training loss: 0.00674962908614992\n",
      "Training loss: 0.006738294283096148\n",
      "Training loss: 0.0067269890929641096\n",
      "Training loss: 0.006715713411018898\n",
      "Training loss: 0.006704467132991945\n",
      "Training loss: 0.00669325015507854\n",
      "Training loss: 0.0066820623739352915\n",
      "Training loss: 0.006670903686677693\n",
      "Training loss: 0.006659773990877717\n",
      "Training loss: 0.006648673184561233\n",
      "Training loss: 0.00663760116620574\n",
      "Training loss: 0.0066265578347378794\n",
      "Training loss: 0.006615543089531046\n",
      "Training loss: 0.0066045568304030485\n",
      "Training loss: 0.006593598957613684\n",
      "Training loss: 0.006582669371862431\n",
      "Training loss: 0.006571767974286113\n",
      "Training loss: 0.006560894666456581\n",
      "Training loss: 0.006550049350378373\n",
      "Training loss: 0.006539231928486494\n",
      "Training loss: 0.006528442303644045\n",
      "Training loss: 0.006517680379140051\n",
      "Training loss: 0.00650694605868716\n",
      "Training loss: 0.006496239246419419\n",
      "Training loss: 0.006485559846890039\n",
      "Training loss: 0.006474907765069224\n",
      "Training loss: 0.006464282906341958\n",
      "Training loss: 0.006453685176505809\n",
      "Training loss: 0.006443114481768772\n",
      "Training loss: 0.006432570728747155\n",
      "Training loss: 0.006422053824463344\n",
      "Training loss: 0.006411563676343768\n",
      "Training loss: 0.006401100192216771\n",
      "Training loss: 0.006390663280310461\n",
      "Training loss: 0.006380252849250645\n",
      "Training loss: 0.006369868808058785\n",
      "Training loss: 0.006359511066149907\n",
      "Training loss: 0.006349179533330538\n",
      "Training loss: 0.006338874119796683\n",
      "Training loss: 0.006328594736131816\n",
      "Training loss: 0.006318341293304823\n",
      "Training loss: 0.006308113702668071\n",
      "Training loss: 0.006297911875955338\n",
      "Training loss: 0.006287735725279893\n",
      "Training loss: 0.006277585163132485\n",
      "Training loss: 0.006267460102379461\n",
      "Training loss: 0.006257360456260761\n",
      "Training loss: 0.0062472861383879915\n",
      "Training loss: 0.006237237062742544\n",
      "Training loss: 0.00622721314367367\n",
      "Training loss: 0.0062172142958965846\n",
      "Training loss: 0.006207240434490562\n",
      "Training loss: 0.006197291474897124\n",
      "Training loss: 0.006187367332918118\n",
      "Training loss: 0.006177467924713881\n",
      "Training loss: 0.006167593166801438\n",
      "Training loss: 0.006157742976052647\n",
      "Training loss: 0.0061479172696923354\n",
      "Training loss: 0.006138115965296607\n",
      "Training loss: 0.0061283389807909395\n",
      "Training loss: 0.006118586234448437\n",
      "Training loss: 0.006108857644888091\n",
      "Training loss: 0.006099153131072983\n",
      "Training loss: 0.006089472612308523\n",
      "Training loss: 0.006079816008240724\n",
      "Training loss: 0.006070183238854493\n",
      "Training loss: 0.006060574224471843\n",
      "Training loss: 0.00605098888575027\n",
      "Training loss: 0.0060414271436809545\n",
      "Training loss: 0.00603188891958718\n",
      "Training loss: 0.006022374135122549\n",
      "Training loss: 0.006012882712269379\n",
      "Training loss: 0.006003414573337022\n",
      "Training loss: 0.00599396964096019\n",
      "Training loss: 0.0059845478380973595\n",
      "Training loss: 0.0059751490880291\n",
      "Training loss: 0.005965773314356477\n",
      "Training loss: 0.005956420440999427\n",
      "Training loss: 0.005947090392195137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.005937783092496507\n",
      "Training loss: 0.0059284984667704954\n",
      "Training loss: 0.005919236440196591\n",
      "Training loss: 0.00590999693826524\n",
      "Training loss: 0.005900779886776269\n",
      "Training loss: 0.005891585211837367\n",
      "Training loss: 0.005882412839862522\n",
      "Training loss: 0.005873262697570491\n",
      "Training loss: 0.005864134711983303\n",
      "Training loss: 0.005855028810424746\n",
      "Training loss: 0.005845944920518831\n",
      "Training loss: 0.005836882970188304\n",
      "Training loss: 0.005827842887653215\n",
      "Training loss: 0.005818824601429396\n",
      "Training loss: 0.005809828040326946\n",
      "Training loss: 0.0058008531334488985\n",
      "Training loss: 0.005791899810189623\n",
      "Training loss: 0.005782968000233506\n",
      "Training loss: 0.005774057633553436\n",
      "Training loss: 0.005765168640409424\n",
      "Training loss: 0.005756300951347157\n",
      "Training loss: 0.005747454497196617\n",
      "Training loss: 0.005738629209070622\n",
      "Training loss: 0.005729825018363499\n",
      "Training loss: 0.005721041856749688\n",
      "Training loss: 0.005712279656182297\n",
      "Training loss: 0.00570353834889182\n",
      "Training loss: 0.005694817867384743\n",
      "Training loss: 0.0056861181444421505\n",
      "Training loss: 0.005677439113118453\n",
      "Training loss: 0.005668780706739977\n",
      "Training loss: 0.005660142858903694\n",
      "Training loss: 0.005651525503475839\n",
      "Training loss: 0.005642928574590655\n",
      "Training loss: 0.0056343520066490325\n",
      "Training loss: 0.0056257957343172574\n",
      "Training loss: 0.00561725969252566\n",
      "Training loss: 0.005608743816467377\n",
      "Training loss: 0.005600248041597078\n",
      "Training loss: 0.005591772303629629\n",
      "Training loss: 0.0055833165385389075\n",
      "Training loss: 0.005574880682556511\n",
      "Training loss: 0.005566464672170472\n",
      "Training loss: 0.0055580684441241006\n",
      "Training loss: 0.0055496919354146535\n",
      "Training loss: 0.005541335083292176\n",
      "Training loss: 0.005532997825258243\n",
      "Training loss: 0.005524680099064777\n",
      "Training loss: 0.005516381842712793\n",
      "Training loss: 0.005508102994451235\n",
      "Training loss: 0.005499843492775778\n",
      "Training loss: 0.005491603276427602\n",
      "Training loss: 0.005483382284392289\n",
      "Training loss: 0.005475180455898564\n",
      "Training loss: 0.0054669977304171846\n",
      "Training loss: 0.005458834047659749\n",
      "Training loss: 0.005450689347577564\n",
      "Training loss: 0.005442563570360477\n",
      "Training loss: 0.0054344566564357225\n",
      "Training loss: 0.0054263685464668535\n",
      "Training loss: 0.005418299181352531\n",
      "Training loss: 0.005410248502225457\n",
      "Training loss: 0.005402216450451205\n",
      "Training loss: 0.005394202967627167\n",
      "Training loss: 0.005386207995581439\n",
      "Training loss: 0.005378231476371673\n",
      "Training loss: 0.005370273352284049\n",
      "Training loss: 0.00536233356583214\n",
      "Training loss: 0.005354412059755883\n",
      "Training loss: 0.005346508777020444\n",
      "Training loss: 0.005338623660815194\n",
      "Training loss: 0.005330756654552644\n",
      "Training loss: 0.005322907701867366\n",
      "Training loss: 0.005315076746614981\n",
      "Training loss: 0.005307263732871064\n",
      "Training loss: 0.005299468604930148\n",
      "Training loss: 0.005291691307304676\n",
      "Training loss: 0.005283931784723977\n",
      "Training loss: 0.005276189982133239\n",
      "Training loss: 0.005268465844692478\n",
      "Training loss: 0.00526075931777558\n",
      "Training loss: 0.005253070346969241\n",
      "Training loss: 0.005245398878071974\n",
      "Training loss: 0.0052377448570931654\n",
      "Training loss: 0.00523010823025204\n",
      "Training loss: 0.005222488943976682\n",
      "Training loss: 0.00521488694490306\n",
      "Training loss: 0.005207302179874076\n",
      "Training loss: 0.005199734595938569\n",
      "Training loss: 0.005192184140350406\n",
      "Training loss: 0.005184650760567424\n",
      "Training loss: 0.005177134404250592\n",
      "Training loss: 0.005169635019263003\n",
      "Training loss: 0.00516215255366894\n",
      "Training loss: 0.005154686955732948\n",
      "Training loss: 0.0051472381739189185\n",
      "Training loss: 0.00513980615688913\n",
      "Training loss: 0.00513239085350335\n",
      "Training loss: 0.005124992212817921\n",
      "Training loss: 0.005117610184084848\n",
      "Training loss: 0.005110244716750862\n",
      "Training loss: 0.005102895760456583\n",
      "Training loss: 0.005095563265035571\n",
      "Training loss: 0.005088247180513444\n",
      "Training loss: 0.005080947457107024\n",
      "Training loss: 0.005073664045223415\n",
      "Training loss: 0.005066396895459137\n",
      "Training loss: 0.005059145958599271\n",
      "Training loss: 0.005051911185616608\n",
      "Training loss: 0.005044692527670698\n",
      "Training loss: 0.005037489936107107\n",
      "Training loss: 0.005030303362456496\n",
      "Training loss: 0.005023132758433773\n",
      "Training loss: 0.005015978075937296\n",
      "Training loss: 0.005008839267047959\n",
      "Training loss: 0.0050017162840284264\n",
      "Training loss: 0.004994609079322282\n",
      "Training loss: 0.0049875176055531635\n",
      "Training loss: 0.004980441815524018\n",
      "Training loss: 0.004973381662216183\n",
      "Training loss: 0.004966337098788674\n",
      "Training loss: 0.004959308078577317\n",
      "Training loss: 0.0049522945550939685\n",
      "Training loss: 0.004945296482025692\n",
      "Training loss: 0.004938313813233981\n",
      "Training loss: 0.004931346502753955\n",
      "Training loss: 0.004924394504793597\n",
      "Training loss: 0.004917457773732941\n",
      "Training loss: 0.0049105362641233075\n",
      "Training loss: 0.004903629930686506\n",
      "Training loss: 0.004896738728314132\n",
      "Training loss: 0.004889862612066702\n",
      "Training loss: 0.004883001537172978\n",
      "Training loss: 0.004876155459029126\n",
      "Training loss: 0.004869324333198062\n",
      "Training loss: 0.004862508115408606\n",
      "Training loss: 0.004855706761554788\n",
      "Training loss: 0.004848920227695095\n",
      "Training loss: 0.004842148470051713\n",
      "Training loss: 0.0048353914450098455\n",
      "Training loss: 0.004828649109116919\n",
      "Training loss: 0.004821921419081878\n",
      "Training loss: 0.004815208331774471\n",
      "Training loss: 0.004808509804224534\n",
      "Training loss: 0.004801825793621258\n",
      "Training loss: 0.0047951562573124796\n",
      "Training loss: 0.004788501152803982\n",
      "Training loss: 0.004781860437758792\n",
      "Training loss: 0.00477523406999645\n",
      "Training loss: 0.004768622007492365\n",
      "Training loss: 0.004762024208377069\n",
      "Training loss: 0.004755440630935556\n",
      "Training loss: 0.004748871233606605\n",
      "Training loss: 0.004742315974982034\n",
      "Training loss: 0.004735774813806105\n",
      "Training loss: 0.004729247708974798\n",
      "Training loss: 0.004722734619535142\n",
      "Training loss: 0.004716235504684562\n",
      "Training loss: 0.004709750323770209\n",
      "Training loss: 0.004703279036288283\n",
      "Training loss: 0.004696821601883393\n",
      "Training loss: 0.0046903779803478905\n",
      "Training loss: 0.004683948131621218\n",
      "Training loss: 0.004677532015789268\n",
      "Training loss: 0.0046711295930837275\n",
      "Training loss: 0.004664740823881448\n",
      "Training loss: 0.004658365668703801\n",
      "Training loss: 0.004652004088216046\n",
      "Training loss: 0.004645656043226663\n",
      "Training loss: 0.004639321494686811\n",
      "Training loss: 0.004633000403689601\n",
      "Training loss: 0.004626692731469543\n",
      "Training loss: 0.0046203984394019085\n",
      "Training loss: 0.0046141174890020785\n",
      "Training loss: 0.004607849841924997\n",
      "Training loss: 0.0046015954599645005\n",
      "Training loss: 0.004595354305052756\n",
      "Training loss: 0.00458912633925963\n",
      "Training loss: 0.004582911524792091\n",
      "Training loss: 0.004576709823993633\n",
      "Training loss: 0.00457052119934366\n",
      "Training loss: 0.00456434561345689\n",
      "Training loss: 0.004558183029082791\n",
      "Training loss: 0.004552033409104986\n",
      "Training loss: 0.004545896716540648\n",
      "Training loss: 0.004539772914539964\n",
      "Training loss: 0.0045336619663855085\n",
      "Training loss: 0.004527563835491713\n",
      "Training loss: 0.004521478485404281\n",
      "Training loss: 0.004515405879799603\n",
      "Training loss: 0.004509345982484224\n",
      "Training loss: 0.004503298757394253\n",
      "Training loss: 0.004497264168594819\n",
      "Training loss: 0.004491242180279497\n",
      "Training loss: 0.004485232756769792\n",
      "Training loss: 0.0044792358625145474\n",
      "Training loss: 0.00447325146208942\n",
      "Training loss: 0.004467279520196325\n",
      "Training loss: 0.004461320001662902\n",
      "Training loss: 0.0044553728714419785\n",
      "Training loss: 0.004449438094611003\n",
      "Training loss: 0.004443515636371546\n",
      "Training loss: 0.0044376054620487515\n",
      "Training loss: 0.004431707537090833\n",
      "Training loss: 0.004425821827068502\n",
      "Training loss: 0.004419948297674456\n",
      "Training loss: 0.0044140869147229105\n",
      "Training loss: 0.004408237644149007\n",
      "Training loss: 0.004402400452008349\n",
      "Training loss: 0.0043965753044764525\n",
      "Training loss: 0.004390762167848269\n",
      "Training loss: 0.004384961008537659\n",
      "Training loss: 0.004379171793076862\n",
      "Training loss: 0.004373394488116049\n",
      "Training loss: 0.004367629060422788\n",
      "Training loss: 0.0043618754768815415\n",
      "Training loss: 0.004356133704493183\n",
      "Training loss: 0.0043504037103745\n",
      "Training loss: 0.004344685461757709\n",
      "Training loss: 0.004338978925989942\n",
      "Training loss: 0.0043332840705328154\n",
      "Training loss: 0.004327600862961866\n",
      "Training loss: 0.0043219292709661536\n",
      "Training loss: 0.004316269262347733\n",
      "Training loss: 0.0043106208050211715\n",
      "Training loss: 0.004304983867013119\n",
      "Training loss: 0.004299358416461778\n",
      "Training loss: 0.004293744421616504\n",
      "Training loss: 0.004288141850837247\n",
      "Training loss: 0.004282550672594193\n",
      "Training loss: 0.004276970855467219\n",
      "Training loss: 0.004271402368145466\n",
      "Training loss: 0.004265845179426868\n",
      "Training loss: 0.004260299258217721\n",
      "Training loss: 0.004254764573532207\n",
      "Training loss: 0.004249241094491939\n",
      "Training loss: 0.004243728790325538\n",
      "Training loss: 0.004238227630368168\n",
      "Training loss: 0.004232737584061072\n",
      "Training loss: 0.00422725862095118\n",
      "Training loss: 0.004221790710690623\n",
      "Training loss: 0.004216333823036306\n",
      "Training loss: 0.004210887927849494\n",
      "Training loss: 0.004205452995095354\n",
      "Training loss: 0.004200028994842516\n",
      "Training loss: 0.004194615897262678\n",
      "Training loss: 0.004189213672630144\n",
      "Training loss: 0.004183822291321425\n",
      "Training loss: 0.004178441723814785\n",
      "Training loss: 0.004173071940689857\n",
      "Training loss: 0.004167712912627197\n",
      "Training loss: 0.004162364610407866\n",
      "Training loss: 0.004157027004913031\n",
      "Training loss: 0.004151700067123534\n",
      "Training loss: 0.004146383768119495\n",
      "Training loss: 0.004141078079079902\n",
      "Training loss: 0.004135782971282181\n",
      "Training loss: 0.004130498416101828\n",
      "Training loss: 0.004125224385011959\n",
      "Training loss: 0.004119960849582983\n",
      "Training loss: 0.004114707781482103\n",
      "Training loss: 0.004109465152472979\n",
      "Training loss: 0.004104232934415365\n",
      "Training loss: 0.004099011099264623\n",
      "Training loss: 0.004093799619071404\n",
      "Training loss: 0.004088598465981232\n",
      "Training loss: 0.004083407612234124\n",
      "Training loss: 0.004078227030164178\n",
      "Training loss: 0.0040730566921992235\n",
      "Training loss: 0.004067896570860435\n",
      "Training loss: 0.004062746638761918\n",
      "Training loss: 0.0040576068686103564\n",
      "Training loss: 0.004052477233204652\n",
      "Training loss: 0.004047357705435499\n",
      "Training loss: 0.004042248258285047\n",
      "Training loss: 0.0040371488648265445\n",
      "Training loss: 0.004032059498223914\n",
      "Training loss: 0.004026980131731417\n",
      "Training loss: 0.004021910738693321\n",
      "Training loss: 0.004016851292543461\n",
      "Training loss: 0.00401180176680493\n",
      "Training loss: 0.004006762135089696\n",
      "Training loss: 0.004001732371098256\n",
      "Training loss: 0.003996712448619267\n",
      "Training loss: 0.003991702341529191\n",
      "Training loss: 0.00398670202379195\n",
      "Training loss: 0.00398171146945855\n",
      "Training loss: 0.003976730652666764\n",
      "Training loss: 0.003971759547640743\n",
      "Training loss: 0.00396679812869071\n",
      "Training loss: 0.003961846370212577\n",
      "Training loss: 0.003956904246687623\n",
      "Training loss: 0.003951971732682136\n",
      "Training loss: 0.003947048802847098\n",
      "Training loss: 0.003942135431917798\n",
      "Training loss: 0.00393723159471354\n",
      "Training loss: 0.003932337266137288\n",
      "Training loss: 0.00392745242117532\n",
      "Training loss: 0.003922577034896912\n",
      "Training loss: 0.003917711082454008\n",
      "Training loss: 0.003912854539080858\n",
      "Training loss: 0.003908007380093752\n",
      "Training loss: 0.0039031695808905953\n",
      "Training loss: 0.003898341116950702\n",
      "Training loss: 0.0038935219638343527\n",
      "Training loss: 0.0038887120971825696\n",
      "Training loss: 0.003883911492716741\n",
      "Training loss: 0.003879120126238309\n",
      "Training loss: 0.0038743379736284524\n",
      "Training loss: 0.003869565010847789\n",
      "Training loss: 0.0038648012139360245\n",
      "Training loss: 0.0038600465590116757\n",
      "Training loss: 0.0038553010222717376\n",
      "Training loss: 0.003850564579991379\n",
      "Training loss: 0.003845837208523627\n",
      "Training loss: 0.003841118884299049\n",
      "Training loss: 0.003836409583825492\n",
      "Training loss: 0.0038317092836877053\n",
      "Training loss: 0.0038270179605471035\n",
      "Training loss: 0.0038223355911414197\n",
      "Training loss: 0.0038176621522844466\n",
      "Training loss: 0.003812997620865678\n",
      "Training loss: 0.003808341973850071\n",
      "Training loss: 0.0038036951882777013\n",
      "Training loss: 0.0037990572412635133\n",
      "Training loss: 0.003794428109996976\n",
      "Training loss: 0.003789807771741829\n",
      "Training loss: 0.003785196203835771\n",
      "Training loss: 0.0037805933836901873\n",
      "Training loss: 0.003775999288789835\n",
      "Training loss: 0.0037714138966925726\n",
      "Training loss: 0.003766837185029088\n",
      "Training loss: 0.0037622691315025787\n",
      "Training loss: 0.003757709713888492\n",
      "Training loss: 0.0037531589100342367\n",
      "Training loss: 0.003748616697858912\n",
      "Training loss: 0.003744083055353005\n",
      "Training loss: 0.0037395579605781393\n",
      "Training loss: 0.003735041391666759\n",
      "Training loss: 0.003730533326821926\n",
      "Training loss: 0.0037260337443169446\n",
      "Training loss: 0.0037215426224951713\n",
      "Training loss: 0.0037170599397697186\n",
      "Training loss: 0.00371258567462315\n",
      "Training loss: 0.0037081198056072616\n",
      "Training loss: 0.0037036623113427835\n",
      "Training loss: 0.003699213170519128\n",
      "Training loss: 0.0036947723618940974\n",
      "Training loss: 0.003690339864293649\n",
      "Training loss: 0.0036859156566116276\n",
      "Training loss: 0.003681499717809472\n",
      "Training loss: 0.0036770920269159914\n",
      "Training loss: 0.003672692563027106\n",
      "Training loss: 0.003668301305305546\n",
      "Training loss: 0.0036639182329806317\n",
      "Training loss: 0.003659543325348005\n",
      "Training loss: 0.0036551765617693797\n",
      "Training loss: 0.003650817921672283\n",
      "Training loss: 0.0036464673845498017\n",
      "Training loss: 0.003642124929960327\n",
      "Training loss: 0.003637790537527308\n",
      "Training loss: 0.003633464186938999\n",
      "Training loss: 0.003629145857948223\n",
      "Training loss: 0.003624835530372103\n",
      "Training loss: 0.0036205331840918425\n",
      "Training loss: 0.0036162387990524595\n",
      "Training loss: 0.0036119523552625393\n",
      "Training loss: 0.0036076738327939996\n",
      "Training loss: 0.0036034032117818584\n",
      "Training loss: 0.003599140472423978\n",
      "Training loss: 0.0035948855949808497\n",
      "Training loss: 0.003590638559775311\n",
      "Training loss: 0.003586399347192348\n",
      "Training loss: 0.003582167937678832\n",
      "Training loss: 0.003577944311743308\n",
      "Training loss: 0.0035737284499557436\n",
      "Training loss: 0.0035695203329473087\n",
      "Training loss: 0.00356531994141012\n",
      "Training loss: 0.003561127256097034\n",
      "Training loss: 0.003556942257821406\n",
      "Training loss: 0.003552764927456857\n",
      "Training loss: 0.0035485952459370584\n",
      "Training loss: 0.00354443319425549\n",
      "Training loss: 0.003540278753465226\n",
      "Training loss: 0.0035361319046786975\n",
      "Training loss: 0.003531992629067482\n",
      "Training loss: 0.003527860907862063\n",
      "Training loss: 0.0035237367223516203\n",
      "Training loss: 0.0035196200538838065\n",
      "Training loss: 0.0035155108838645353\n",
      "Training loss: 0.0035114091937577504\n",
      "Training loss: 0.0035073149650851824\n",
      "Training loss: 0.003503228179426193\n",
      "Training loss: 0.0034991488184175083\n",
      "Training loss: 0.00349507686375302\n",
      "Training loss: 0.003491012297183579\n",
      "Training loss: 0.0034869551005167455\n",
      "Training loss: 0.0034829052556166386\n",
      "Training loss: 0.0034788627444036603\n",
      "Training loss: 0.003474827548854334\n",
      "Training loss: 0.003470799651001057\n",
      "Training loss: 0.003466779032931909\n",
      "Training loss: 0.0034627656767904735\n",
      "Training loss: 0.003458759564775552\n",
      "Training loss: 0.0034547606791410553\n",
      "Training loss: 0.0034507690021957018\n",
      "Training loss: 0.0034467845163028937\n",
      "Training loss: 0.0034428072038804637\n",
      "Training loss: 0.003438837047400508\n",
      "Training loss: 0.003434874029389142\n",
      "Training loss: 0.0034309181324263267\n",
      "Training loss: 0.0034269693391456796\n",
      "Training loss: 0.0034230276322342423\n",
      "Training loss: 0.003419092994432309\n",
      "Training loss: 0.0034151654085332155\n",
      "Training loss: 0.003411244857383153\n",
      "Training loss: 0.003407331323880958\n",
      "Training loss: 0.003403424790977936\n",
      "Training loss: 0.003399525241677658\n",
      "Training loss: 0.0033956326590357526\n",
      "Training loss: 0.0033917470261597303\n",
      "Training loss: 0.003387868326208797\n",
      "Training loss: 0.0033839965423936537\n",
      "Training loss: 0.0033801316579763112\n",
      "Training loss: 0.003376273656269889\n",
      "Training loss: 0.0033724225206384385\n",
      "Training loss: 0.003368578234496759\n",
      "Training loss: 0.003364740781310206\n",
      "Training loss: 0.003360910144594485\n",
      "Training loss: 0.003357086307915522\n",
      "Training loss: 0.0033532692548891973\n",
      "Training loss: 0.0033494589691812427\n",
      "Training loss: 0.003345655434507024\n",
      "Training loss: 0.003341858634631334\n",
      "Training loss: 0.003338068553368254\n",
      "Training loss: 0.003334285174580961\n",
      "Training loss: 0.0033305084821815285\n",
      "Training loss: 0.003326738460130767\n",
      "Training loss: 0.0033229750924380387\n",
      "Training loss: 0.0033192183631610968\n",
      "Training loss: 0.003315468256405871\n",
      "Training loss: 0.0033117247563263447\n",
      "Training loss: 0.003307987847124327\n",
      "Training loss: 0.003304257513049301\n",
      "Training loss: 0.0033005337383982826\n",
      "Training loss: 0.003296816507515575\n",
      "Training loss: 0.0032931058047926697\n",
      "Training loss: 0.0032894016146680295\n",
      "Training loss: 0.003285703921626935\n",
      "Training loss: 0.0032820127102013074\n",
      "Training loss: 0.003278327964969552\n",
      "Training loss: 0.0032746496705563805\n",
      "Training loss: 0.00327097781163264\n",
      "Training loss: 0.003267312372915153\n",
      "Training loss: 0.00326365333916655\n",
      "Training loss: 0.0032600006951950897\n",
      "Training loss: 0.003256354425854531\n",
      "Training loss: 0.003252714516043921\n",
      "Training loss: 0.0032490809507074725\n",
      "Training loss: 0.003245453714834371\n",
      "Training loss: 0.00324183279345863\n",
      "Training loss: 0.0032382181716589307\n",
      "Training loss: 0.0032346098345584534\n",
      "Training loss: 0.0032310077673247018\n",
      "Training loss: 0.003227411955169385\n",
      "Training loss: 0.003223822383348228\n",
      "Training loss: 0.003220239037160817\n",
      "Training loss: 0.0032166619019504427\n",
      "Training loss: 0.0032130909631039595\n",
      "Training loss: 0.003209526206051595\n",
      "Training loss: 0.003205967616266837\n",
      "Training loss: 0.0032024151792662495\n",
      "Training loss: 0.003198868880609325\n",
      "Training loss: 0.0031953287058983345\n",
      "Training loss: 0.003191794640778176\n",
      "Training loss: 0.0031882666709362162\n",
      "Training loss: 0.003184744782102135\n",
      "Training loss: 0.003181228960047794\n",
      "Training loss: 0.0031777191905870607\n",
      "Training loss: 0.0031742154595756876\n",
      "Training loss: 0.0031707177529111353\n",
      "Training loss: 0.003167226056532437\n",
      "Training loss: 0.003163740356420056\n",
      "Training loss: 0.003160260638595728\n",
      "Training loss: 0.003156786889122322\n",
      "Training loss: 0.0031533190941036736\n",
      "Training loss: 0.003149857239684486\n",
      "Training loss: 0.0031464013120501423\n",
      "Training loss: 0.0031429512974265605\n",
      "Training loss: 0.003139507182080098\n",
      "Training loss: 0.0031360689523173444\n",
      "Training loss: 0.0031326365944850205\n",
      "Training loss: 0.0031292100949698335\n",
      "Training loss: 0.003125789440198324\n",
      "Training loss: 0.0031223746166367324\n",
      "Training loss: 0.003118965610790836\n",
      "Training loss: 0.0031155624092058577\n",
      "Training loss: 0.003112164998466288\n",
      "Training loss: 0.0031087733651957543\n",
      "Training loss: 0.0031053874960568924\n",
      "Training loss: 0.0031020073777511935\n",
      "Training loss: 0.0030986329970188902\n",
      "Training loss: 0.003095264340638805\n",
      "Training loss: 0.0030919013954282095\n",
      "Training loss: 0.003088544148242702\n",
      "Training loss: 0.0030851925859760793\n",
      "Training loss: 0.003081846695560178\n",
      "Training loss: 0.003078506463964766\n",
      "Training loss: 0.0030751718781973796\n",
      "Training loss: 0.0030718429253032364\n",
      "Training loss: 0.0030685195923650594\n",
      "Training loss: 0.0030652018665029875\n",
      "Training loss: 0.003061889734874383\n",
      "Training loss: 0.0030585831846737885\n",
      "Training loss: 0.003055282203132711\n",
      "Training loss: 0.003051986777519553\n",
      "Training loss: 0.0030486968951394487\n",
      "Training loss: 0.003045412543334168\n",
      "Training loss: 0.003042133709481948\n",
      "Training loss: 0.0030388603809974007\n",
      "Training loss: 0.0030355925453313852\n",
      "Training loss: 0.003032330189970858\n",
      "Training loss: 0.003029073302438762\n",
      "Training loss: 0.0030258218702938957\n",
      "Training loss: 0.003022575881130819\n",
      "Training loss: 0.003019335322579678\n",
      "Training loss: 0.0030161001823061167\n",
      "Training loss: 0.0030128704480111464\n",
      "Training loss: 0.0030096461074310215\n",
      "Training loss: 0.0030064271483371237\n",
      "Training loss: 0.00300321355853583\n",
      "Training loss: 0.0030000053258683944\n",
      "Training loss: 0.0029968024382108355\n",
      "Training loss: 0.0029936048834738175\n",
      "Training loss: 0.0029904126496025185\n",
      "Training loss: 0.0029872257245765078\n",
      "Training loss: 0.002984044096409656\n",
      "Training loss: 0.0029808677531499945\n",
      "Training loss: 0.0029776966828795924\n",
      "Training loss: 0.0029745308737144575\n",
      "Training loss: 0.0029713703138044036\n",
      "Training loss: 0.002968214991332957\n",
      "Training loss: 0.002965064894517223\n",
      "Training loss: 0.002961920011607752\n",
      "Training loss: 0.0029587803308884736\n",
      "Training loss: 0.0029556458406765477\n",
      "Training loss: 0.0029525165293222593\n",
      "Training loss: 0.0029493923852089\n",
      "Training loss: 0.002946273396752662\n",
      "Training loss: 0.0029431595524025177\n",
      "Training loss: 0.0029400508406401387\n",
      "Training loss: 0.0029369472499797132\n",
      "Training loss: 0.002933848768967924\n",
      "Training loss: 0.0029307553861837744\n",
      "Training loss: 0.00292766709023851\n",
      "Training loss: 0.0029245838697754788\n",
      "Training loss: 0.0029215057134700574\n",
      "Training loss: 0.0029184326100295177\n",
      "Training loss: 0.0029153645481929296\n",
      "Training loss: 0.002912301516731045\n",
      "Training loss: 0.0029092435044461903\n",
      "Training loss: 0.002906190500172184\n",
      "Training loss: 0.00290314249277418\n",
      "Training loss: 0.0029000994711486146\n",
      "Training loss: 0.002897061424223064\n",
      "Training loss: 0.002894028340956153\n",
      "Training loss: 0.0028910002103374665\n",
      "Training loss: 0.002887977021387397\n",
      "Training loss: 0.002884958763157096\n",
      "Training loss: 0.002881945424728323\n",
      "Training loss: 0.0028789369952133825\n",
      "Training loss: 0.002875933463754992\n",
      "Training loss: 0.002872934819526191\n",
      "Training loss: 0.0028699410517302416\n",
      "Training loss: 0.0028669521496005124\n",
      "Training loss: 0.002863968102400402\n",
      "Training loss: 0.0028609888994232165\n",
      "Training loss: 0.0028580145299920744\n",
      "Training loss: 0.002855044983459808\n",
      "Training loss: 0.002852080249208866\n",
      "Training loss: 0.0028491203166512097\n",
      "Training loss: 0.002846165175228211\n",
      "Training loss: 0.00284321481441057\n",
      "Training loss: 0.0028402692236981974\n",
      "Training loss: 0.0028373283926201327\n",
      "Training loss: 0.0028343923107344237\n",
      "Training loss: 0.002831460967628063\n",
      "Training loss: 0.002828534352916862\n",
      "Training loss: 0.002825612456245361\n",
      "Training loss: 0.0028226952672867544\n",
      "Training loss: 0.0028197827757427568\n",
      "Training loss: 0.0028168749713435508\n",
      "Training loss: 0.002813971843847662\n",
      "Training loss: 0.0028110733830418637\n",
      "Training loss: 0.002808179578741107\n",
      "Training loss: 0.0028052904207884074\n",
      "Training loss: 0.0028024058990547625\n",
      "Training loss: 0.002799526003439036\n",
      "Training loss: 0.0027966507238678983\n",
      "Training loss: 0.002793780050295715\n",
      "Training loss: 0.0027909139727044564\n",
      "Training loss: 0.0027880524811036127\n",
      "Training loss: 0.0027851955655300836\n",
      "Training loss: 0.00278234321604812\n",
      "Training loss: 0.002779495422749212\n",
      "Training loss: 0.0027766521757519825\n",
      "Training loss: 0.002773813465202144\n",
      "Training loss: 0.0027709792812723676\n",
      "Training loss: 0.0027681496141622125\n",
      "Training loss: 0.0027653244540980326\n",
      "Training loss: 0.0027625037913328802\n",
      "Training loss: 0.0027596876161464466\n",
      "Training loss: 0.0027568759188449326\n",
      "Training loss: 0.0027540686897609924\n",
      "Training loss: 0.002751265919253646\n",
      "Training loss: 0.0027484675977081565\n",
      "Training loss: 0.0027456737155360073\n",
      "Training loss: 0.002742884263174758\n",
      "Training loss: 0.0027400992310879775\n",
      "Training loss: 0.002737318609765164\n",
      "Training loss: 0.002734542389721668\n",
      "Training loss: 0.002731770561498587\n",
      "Training loss: 0.002729003115662687\n",
      "Training loss: 0.0027262400428063292\n",
      "Training loss: 0.0027234813335473783\n",
      "Training loss: 0.002720726978529122\n",
      "Training loss: 0.002717976968420184\n",
      "Training loss: 0.0027152312939144312\n",
      "Training loss: 0.0027124899457309326\n",
      "Training loss: 0.0027097529146138154\n",
      "Training loss: 0.0027070201913322483\n",
      "Training loss: 0.00270429176668029\n",
      "Training loss: 0.0027015676314768714\n",
      "Training loss: 0.0026988477765656836\n",
      "Training loss: 0.002696132192815096\n",
      "Training loss: 0.002693420871118083\n",
      "Training loss: 0.0026907138023921496\n",
      "Training loss: 0.0026880109775792404\n",
      "Training loss: 0.0026853123876456665\n",
      "Training loss: 0.0026826180235820225\n",
      "Training loss: 0.002679927876403111\n",
      "Training loss: 0.0026772419371478657\n",
      "Training loss: 0.0026745601968792675\n",
      "Training loss: 0.0026718826466842704\n",
      "Training loss: 0.0026692092776737343\n",
      "Training loss: 0.0026665400809823235\n",
      "Training loss: 0.002663875047768434\n",
      "Training loss: 0.0026612141692141495\n",
      "Training loss: 0.002658557436525128\n",
      "Training loss: 0.00265590484093054\n",
      "Training loss: 0.002653256373682999\n",
      "Training loss: 0.002650612026058455\n",
      "Training loss: 0.002647971789356165\n",
      "Training loss: 0.002645335654898601\n",
      "Training loss: 0.002642703614031348\n",
      "Training loss: 0.002640075658123064\n",
      "Training loss: 0.002637451778565394\n",
      "Training loss: 0.0026348319667728907\n",
      "Training loss: 0.002632216214182946\n",
      "Training loss: 0.00262960451225572\n",
      "Training loss: 0.002626996852474065\n",
      "Training loss: 0.002624393226343457\n",
      "Training loss: 0.002621793625391914\n",
      "Training loss: 0.002619198041169929\n",
      "Training loss: 0.002616606465250393\n",
      "Training loss: 0.002614018889228541\n",
      "Training loss: 0.0026114353047218576\n",
      "Training loss: 0.0026088557033700235\n",
      "Training loss: 0.002606280076834833\n",
      "Training loss: 0.0026037084168001336\n",
      "Training loss: 0.0026011407149717385\n",
      "Training loss: 0.0025985769630773744\n",
      "Training loss: 0.002596017152866597\n",
      "Training loss: 0.0025934612761107523\n",
      "Training loss: 0.0025909093246028587\n",
      "Training loss: 0.0025883612901575754\n",
      "Training loss: 0.002585817164611119\n",
      "Training loss: 0.0025832769398212003\n",
      "Training loss: 0.002580740607666962\n",
      "Training loss: 0.0025782081600488854\n",
      "Training loss: 0.002575679588888748\n",
      "Training loss: 0.0025731548861295445\n",
      "Training loss: 0.002570634043735434\n",
      "Training loss: 0.0025681170536916453\n",
      "Training loss: 0.002565603908004425\n",
      "Training loss: 0.002563094598700992\n",
      "Training loss: 0.0025605891178294427\n",
      "Training loss: 0.002558087457458684\n",
      "Training loss: 0.002555589609678388\n",
      "Training loss: 0.0025530955665989045\n",
      "Training loss: 0.002550605320351223\n",
      "Training loss: 0.002548118863086878\n",
      "Training loss: 0.0025456361869779028\n",
      "Training loss: 0.0025431572842167633\n",
      "Training loss: 0.002540682147016285\n",
      "Training loss: 0.002538210767609598\n",
      "Training loss: 0.002535743138250067\n",
      "Training loss: 0.002533279251211219\n",
      "Training loss: 0.0025308190987867224\n",
      "Training loss: 0.0025283626732902553\n",
      "Training loss: 0.0025259099670554967\n",
      "Training loss: 0.0025234609724360483\n",
      "Training loss: 0.0025210156818053753\n",
      "Training loss: 0.0025185740875567214\n",
      "Training loss: 0.0025161361821030713\n",
      "Training loss: 0.0025137019578770976\n",
      "Training loss: 0.002511271407331057\n",
      "Training loss: 0.00250884452293678\n",
      "Training loss: 0.0025064212971855726\n",
      "Training loss: 0.0025040017225881742\n",
      "Training loss: 0.0025015857916746887\n",
      "Training loss: 0.002499173496994521\n",
      "Training loss: 0.0024967648311163323\n",
      "Training loss: 0.002494359786627961\n",
      "Training loss: 0.002491958356136378\n",
      "Training loss: 0.002489560532267611\n",
      "Training loss: 0.002487166307666721\n",
      "Training loss: 0.0024847756749976756\n",
      "Training loss: 0.00248238862694338\n",
      "Training loss: 0.0024800051562055237\n",
      "Training loss: 0.0024776252555046073\n",
      "Training loss: 0.0024752489175798175\n",
      "Training loss: 0.002472876135189016\n",
      "Training loss: 0.002470506901108644\n",
      "Training loss: 0.0024681412081337005\n",
      "Training loss: 0.00246577904907766\n",
      "Training loss: 0.0024634204167724174\n",
      "Training loss: 0.002461065304068247\n",
      "Training loss: 0.0024587137038337347\n",
      "Training loss: 0.002456365608955705\n",
      "Training loss: 0.0024540210123392037\n",
      "Training loss: 0.002451679906907402\n",
      "Training loss: 0.002449342285601569\n",
      "Training loss: 0.002447008141380999\n",
      "Training loss: 0.0024446774672229575\n",
      "Training loss: 0.0024423502561226433\n",
      "Training loss: 0.0024400265010931033\n",
      "Training loss: 0.002437706195165203\n",
      "Training loss: 0.002435389331387573\n",
      "Training loss: 0.002433075902826521\n",
      "Training loss: 0.002430765902566023\n",
      "Training loss: 0.0024284593237076297\n",
      "Training loss: 0.0024261561593704388\n",
      "Training loss: 0.0024238564026910266\n",
      "Training loss: 0.0024215600468234116\n",
      "Training loss: 0.002419267084938974\n",
      "Training loss: 0.0024169775102264165\n",
      "Training loss: 0.002414691315891734\n",
      "Training loss: 0.002412408495158114\n",
      "Training loss: 0.002410129041265923\n",
      "Training loss: 0.0024078529474726366\n",
      "Training loss: 0.0024055802070527895\n",
      "Training loss: 0.002403310813297935\n",
      "Training loss: 0.002401044759516563\n",
      "Training loss: 0.0023987820390340793\n",
      "Training loss: 0.0023965226451927446\n",
      "Training loss: 0.0023942665713516226\n",
      "Training loss: 0.0023920138108865126\n",
      "Training loss: 0.002389764357189929\n",
      "Training loss: 0.002387518203671027\n",
      "Training loss: 0.0023852753437555623\n",
      "Training loss: 0.002383035770885848\n",
      "Training loss: 0.00238079947852067\n",
      "Training loss: 0.002378566460135277\n",
      "Training loss: 0.0023763367092213255\n",
      "Training loss: 0.0023741102192868\n",
      "Training loss: 0.0023718869838559877\n",
      "Training loss: 0.0023696669964694337\n",
      "Training loss: 0.002367450250683869\n",
      "Training loss: 0.002365236740072185\n",
      "Training loss: 0.002363026458223367\n",
      "Training loss: 0.0023608193987424684\n",
      "Training loss: 0.002358615555250519\n",
      "Training loss: 0.0023564149213845366\n",
      "Training loss: 0.0023542174907974158\n",
      "Training loss: 0.002352023257157935\n",
      "Training loss: 0.002349832214150672\n",
      "Training loss: 0.0023476443554759775\n",
      "Training loss: 0.0023454596748499023\n",
      "Training loss: 0.0023432781660041903\n",
      "Training loss: 0.00234109982268618\n",
      "Training loss: 0.002338924638658816\n",
      "Training loss: 0.0023367526077005367\n",
      "Training loss: 0.0023345837236052967\n",
      "Training loss: 0.002332417980182465\n",
      "Training loss: 0.002330255371256794\n",
      "Training loss: 0.002328095890668398\n",
      "Training loss: 0.002325939532272675\n",
      "Training loss: 0.0023237862899402797\n",
      "Training loss: 0.002321636157557065\n",
      "Training loss: 0.0023194891290240468\n",
      "Training loss: 0.002317345198257364\n",
      "Training loss: 0.002315204359188208\n",
      "Training loss: 0.0023130666057628022\n",
      "Training loss: 0.0023109319319423514\n",
      "Training loss: 0.0023088003317029973\n",
      "Training loss: 0.002306671799035758\n",
      "Training loss: 0.0023045463279465035\n",
      "Training loss: 0.002302423912455917\n",
      "Training loss: 0.002300304546599415\n",
      "Training loss: 0.0022981882244271475\n",
      "Training loss: 0.002296074940003923\n",
      "Training loss: 0.002293964687409177\n",
      "Training loss: 0.0022918574607369295\n",
      "Training loss: 0.002289753254095734\n",
      "Training loss: 0.0022876520616086486\n",
      "Training loss: 0.0022855538774131687\n",
      "Training loss: 0.0022834586956612165\n",
      "Training loss: 0.0022813665105190714\n",
      "Training loss: 0.00227927731616733\n",
      "Training loss: 0.0022771911068008867\n",
      "Training loss: 0.0022751078766288626\n",
      "Training loss: 0.0022730276198745783\n",
      "Training loss: 0.002270950330775511\n",
      "Training loss: 0.002268876003583252\n",
      "Training loss: 0.002266804632563469\n",
      "Training loss: 0.002264736211995843\n",
      "Training loss: 0.002262670736174055\n",
      "Training loss: 0.0022606081994057344\n",
      "Training loss: 0.0022585485960124187\n",
      "Training loss: 0.002256491920329497\n",
      "Training loss: 0.0022544381667062015\n",
      "Training loss: 0.0022523873295055273\n",
      "Training loss: 0.002250339403104228\n",
      "Training loss: 0.002248294381892755\n",
      "Training loss: 0.002246252260275211\n",
      "Training loss: 0.0022442130326693404\n",
      "Training loss: 0.002242176693506454\n",
      "Training loss: 0.0022401432372314086\n",
      "Training loss: 0.00223811265830257\n",
      "Training loss: 0.0022360849511917583\n",
      "Training loss: 0.002234060110384219\n",
      "Training loss: 0.002232038130378576\n",
      "Training loss: 0.002230019005686803\n",
      "Training loss: 0.002228002730834188\n",
      "Training loss: 0.002225989300359272\n",
      "Training loss: 0.0022239787088138216\n",
      "Training loss: 0.0022219709507628116\n",
      "Training loss: 0.0022199660207843394\n",
      "Training loss: 0.0022179639134696414\n",
      "Training loss: 0.0022159646234230102\n",
      "Training loss: 0.002213968145261781\n",
      "Training loss: 0.0022119744736162814\n",
      "Training loss: 0.002209983603129807\n",
      "Training loss: 0.0022079955284585694\n",
      "Training loss: 0.002206010244271666\n",
      "Training loss: 0.002204027745251039\n",
      "Training loss: 0.002202048026091437\n",
      "Training loss: 0.0022000710815003946\n",
      "Training loss: 0.0021980969061981635\n",
      "Training loss: 0.0021961254949177107\n",
      "Training loss: 0.00219415684240465\n",
      "Training loss: 0.0021921909434172216\n",
      "Training loss: 0.0021902277927262634\n",
      "Training loss: 0.0021882673851151505\n",
      "Training loss: 0.002186309715379792\n",
      "Training loss: 0.002184354778328559\n",
      "Training loss: 0.0021824025687822613\n",
      "Training loss: 0.002180453081574139\n",
      "Training loss: 0.0021785063115497823\n",
      "Training loss: 0.0021765622535671134\n",
      "Training loss: 0.0021746209024963714\n",
      "Training loss: 0.0021726822532200456\n",
      "Training loss: 0.002170746300632865\n",
      "Training loss: 0.002168813039641736\n",
      "Training loss: 0.002166882465165743\n",
      "Training loss: 0.0021649545721360635\n",
      "Training loss: 0.002163029355495997\n",
      "Training loss: 0.0021611068102008746\n",
      "Training loss: 0.0021591869312180493\n",
      "Training loss: 0.002157269713526866\n",
      "Training loss: 0.0021553551521186027\n",
      "Training loss: 0.0021534432419964725\n",
      "Training loss: 0.002151533978175562\n",
      "Training loss: 0.0021496273556827887\n",
      "Training loss: 0.0021477233695569096\n",
      "Training loss: 0.002145822014848441\n",
      "Training loss: 0.0021439232866196493\n",
      "Training loss: 0.0021420271799445106\n",
      "Training loss: 0.0021401336899086933\n",
      "Training loss: 0.0021382428116094887\n",
      "Training loss: 0.0021363545401558138\n",
      "Training loss: 0.0021344688706681565\n",
      "Training loss: 0.002132585798278557\n",
      "Training loss: 0.0021307053181305537\n",
      "Training loss: 0.0021288274253791774\n",
      "Training loss: 0.002126952115190901\n",
      "Training loss: 0.0021250793827436157\n",
      "Training loss: 0.0021232092232265735\n",
      "Training loss: 0.0021213416318404026\n",
      "Training loss: 0.0021194766037970344\n",
      "Training loss: 0.0021176141343196802\n",
      "Training loss: 0.002115754218642813\n",
      "Training loss: 0.00211389685201211\n",
      "Training loss: 0.0021120420296844586\n",
      "Training loss: 0.0021101897469278886\n",
      "Training loss: 0.0021083399990215524\n",
      "Training loss: 0.002106492781255702\n",
      "Training loss: 0.0021046480889316548\n",
      "Training loss: 0.002102805917361742\n",
      "Training loss: 0.0021009662618693106\n",
      "Training loss: 0.0020991291177886655\n",
      "Training loss: 0.002097294480465055\n",
      "Training loss: 0.0020954623452546293\n",
      "Training loss: 0.002093632707524408\n",
      "Training loss: 0.0020918055626522707\n",
      "Training loss: 0.0020899809060268937\n",
      "Training loss: 0.0020881587330477415\n",
      "Training loss: 0.0020863390391250365\n",
      "Training loss: 0.0020845218196797145\n",
      "Training loss: 0.0020827070701434176\n",
      "Training loss: 0.002080894785958423\n",
      "Training loss: 0.002079084962577669\n",
      "Training loss: 0.002077277595464681\n",
      "Training loss: 0.002075472680093561\n",
      "Training loss: 0.0020736702119489503\n",
      "Training loss: 0.0020718701865259943\n",
      "Training loss: 0.002070072599330337\n",
      "Training loss: 0.002068277445878067\n",
      "Training loss: 0.0020664847216957\n",
      "Training loss: 0.0020646944223201512\n",
      "Training loss: 0.002062906543298679\n",
      "Training loss: 0.002061121080188916\n",
      "Training loss: 0.0020593380285587664\n",
      "Training loss: 0.002057557383986432\n",
      "Training loss: 0.0020557791420603676\n",
      "Training loss: 0.0020540032983792362\n",
      "Training loss: 0.0020522298485519033\n",
      "Training loss: 0.0020504587881973956\n",
      "Training loss: 0.0020486901129448697\n",
      "Training loss: 0.002046923818433601\n",
      "Training loss: 0.0020451599003129383\n",
      "Training loss: 0.0020433983542422736\n",
      "Training loss: 0.0020416391758910444\n",
      "Training loss: 0.0020398823609386564\n",
      "Training loss: 0.0020381279050744987\n",
      "Training loss: 0.002036375803997894\n",
      "Training loss: 0.002034626053418086\n",
      "Training loss: 0.0020328786490541894\n",
      "Training loss: 0.002031133586635185\n",
      "Training loss: 0.0020293908618998773\n",
      "Training loss: 0.002027650470596875\n",
      "Training loss: 0.0020259124084845617\n",
      "Training loss: 0.002024176671331074\n",
      "Training loss: 0.002022443254914259\n",
      "Training loss: 0.0020207121550216657\n",
      "Training loss: 0.002018983367450504\n",
      "Training loss: 0.002017256888007626\n",
      "Training loss: 0.0020155327125094998\n",
      "Training loss: 0.0020138108367821726\n",
      "Training loss: 0.002012091256661265\n",
      "Training loss: 0.002010373967991913\n",
      "Training loss: 0.00200865896662878\n",
      "Training loss: 0.002006946248435993\n",
      "Training loss: 0.0020052358092871483\n",
      "Training loss: 0.0020035276450652596\n",
      "Training loss: 0.002001821751662742\n",
      "Training loss: 0.002000118124981406\n",
      "Training loss: 0.0019984167609323864\n",
      "Training loss: 0.001996717655436166\n",
      "Training loss: 0.0019950208044225166\n",
      "Training loss: 0.0019933262038304873\n",
      "Training loss: 0.0019916338496083668\n",
      "Training loss: 0.0019899437377136797\n",
      "Training loss: 0.0019882558641131347\n",
      "Training loss: 0.0019865702247826365\n",
      "Training loss: 0.0019848868157072022\n",
      "Training loss: 0.001983205632881\n",
      "Training loss: 0.00198152667230728\n",
      "Training loss: 0.001979849929998371\n",
      "Training loss: 0.0019781754019756433\n",
      "Training loss: 0.001976503084269498\n",
      "Training loss: 0.001974832972919327\n",
      "Training loss: 0.001973165063973494\n",
      "Training loss: 0.0019714993534893244\n",
      "Training loss: 0.001969835837533043\n",
      "Training loss: 0.0019681745121798\n",
      "Training loss: 0.0019665153735136053\n",
      "Training loss: 0.0019648584176273232\n",
      "Training loss: 0.001963203640622649\n",
      "Training loss: 0.0019615510386100815\n",
      "Training loss: 0.0019599006077088855\n",
      "Training loss: 0.001958252344047098\n",
      "Training loss: 0.001956606243761474\n",
      "Training loss: 0.0019549623029974823\n",
      "Training loss: 0.001953320517909276\n",
      "Training loss: 0.00195168088465966\n",
      "Training loss: 0.0019500433994200927\n",
      "Training loss: 0.0019484080583706232\n",
      "Training loss: 0.0019467748576999057\n",
      "Training loss: 0.001945143793605156\n",
      "Training loss: 0.0019435148622921295\n",
      "Training loss: 0.0019418880599751072\n",
      "Training loss: 0.0019402633828768685\n",
      "Training loss: 0.0019386408272286542\n",
      "Training loss: 0.00193702038927017\n",
      "Training loss: 0.0019354020652495403\n",
      "Training loss: 0.0019337858514232962\n",
      "Training loss: 0.001932171744056354\n",
      "Training loss: 0.0019305597394219842\n",
      "Training loss: 0.0019289498338018029\n",
      "Training loss: 0.001927342023485733\n",
      "Training loss: 0.0019257363047719906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.001924132673967064\n",
      "Training loss: 0.0019225311273856863\n",
      "Training loss: 0.001920931661350822\n",
      "Training loss: 0.0019193342721936261\n",
      "Training loss: 0.0019177389562534464\n",
      "Training loss: 0.0019161457098777826\n",
      "Training loss: 0.001914554529422272\n",
      "Training loss: 0.0019129654112506722\n",
      "Training loss: 0.0019113783517348258\n",
      "Training loss: 0.0019097933472546507\n",
      "Training loss: 0.0019082103941981197\n",
      "Training loss: 0.0019066294889612204\n",
      "Training loss: 0.001905050627947953\n",
      "Training loss: 0.0019034738075703137\n",
      "Training loss: 0.0019018990242482435\n",
      "Training loss: 0.0019003262744096434\n",
      "Training loss: 0.0018987555544903217\n",
      "Training loss: 0.0018971868609339929\n",
      "Training loss: 0.0018956201901922476\n",
      "Training loss: 0.001894055538724537\n",
      "Training loss: 0.0018924929029981443\n",
      "Training loss: 0.0018909322794881777\n",
      "Training loss: 0.0018893736646775205\n",
      "Training loss: 0.0018878170550568547\n",
      "Training loss: 0.0018862624471245903\n",
      "Training loss: 0.0018847098373868975\n",
      "Training loss: 0.001883159222357629\n",
      "Training loss: 0.0018816105985583547\n",
      "Training loss: 0.0018800639625182944\n",
      "Training loss: 0.00187851931077433\n",
      "Training loss: 0.0018769766398709724\n",
      "Training loss: 0.0018754359463603399\n",
      "Training loss: 0.0018738972268021407\n",
      "Training loss: 0.0018723604777636474\n",
      "Training loss: 0.001870825695819692\n",
      "Training loss: 0.0018692928775526274\n",
      "Training loss: 0.0018677620195523203\n",
      "Training loss: 0.001866233118416115\n",
      "Training loss: 0.001864706170748846\n",
      "Training loss: 0.001863181173162772\n",
      "Training loss: 0.0018616581222776066\n",
      "Training loss: 0.0018601370147204555\n",
      "Training loss: 0.0018586178471258217\n",
      "Training loss: 0.0018571006161355796\n",
      "Training loss: 0.0018555853183989527\n",
      "Training loss: 0.0018540719505724947\n",
      "Training loss: 0.0018525605093200792\n",
      "Training loss: 0.0018510509913128614\n",
      "Training loss: 0.0018495433932292798\n",
      "Training loss: 0.0018480377117550254\n",
      "Training loss: 0.0018465339435830216\n",
      "Training loss: 0.001845032085413421\n",
      "Training loss: 0.0018435321339535483\n",
      "Training loss: 0.0018420340859179316\n",
      "Training loss: 0.0018405379380282457\n",
      "Training loss: 0.001839043687013313\n",
      "Training loss: 0.0018375513296090737\n",
      "Training loss: 0.0018360608625585726\n",
      "Training loss: 0.001834572282611936\n",
      "Training loss: 0.001833085586526354\n",
      "Training loss: 0.0018316007710660765\n",
      "Training loss: 0.0018301178330023754\n",
      "Training loss: 0.0018286367691135294\n",
      "Training loss: 0.00182715757618481\n",
      "Training loss: 0.0018256802510084671\n",
      "Training loss: 0.0018242047903837032\n",
      "Training loss: 0.0018227311911166585\n",
      "Training loss: 0.001821259450020391\n",
      "Training loss: 0.0018197895639148675\n",
      "Training loss: 0.0018183215296269234\n",
      "Training loss: 0.0018168553439902677\n",
      "Training loss: 0.0018153910038454608\n",
      "Training loss: 0.0018139285060398804\n",
      "Training loss: 0.0018124678474277284\n",
      "Training loss: 0.0018110090248699912\n",
      "Training loss: 0.0018095520352344316\n",
      "Training loss: 0.0018080968753955778\n",
      "Training loss: 0.0018066435422346891\n",
      "Training loss: 0.0018051920326397522\n",
      "Training loss: 0.0018037423435054655\n",
      "Training loss: 0.0018022944717332047\n",
      "Training loss: 0.0018008484142310141\n",
      "Training loss: 0.001799404167913607\n",
      "Training loss: 0.0017979617297023181\n",
      "Training loss: 0.001796521096525109\n",
      "Training loss: 0.0017950822653165423\n",
      "Training loss: 0.0017936452330177544\n",
      "Training loss: 0.0017922099965764664\n",
      "Training loss: 0.001790776552946935\n",
      "Training loss: 0.0017893448990899555\n",
      "Training loss: 0.0017879150319728393\n",
      "Training loss: 0.0017864869485693963\n",
      "Training loss: 0.0017850606458599254\n",
      "Training loss: 0.0017836361208311829\n",
      "Training loss: 0.0017822133704763785\n",
      "Training loss: 0.001780792391795157\n",
      "Training loss: 0.0017793731817935714\n",
      "Training loss: 0.0017779557374840804\n",
      "Training loss: 0.0017765400558855226\n",
      "Training loss: 0.0017751261340231048\n",
      "Training loss: 0.0017737139689283832\n",
      "Training loss: 0.0017723035576392432\n",
      "Training loss: 0.001770894897199893\n",
      "Training loss: 0.001769487984660842\n",
      "Training loss: 0.0017680828170788853\n",
      "Training loss: 0.0017666793915170687\n",
      "Training loss: 0.0017652777050447207\n",
      "Training loss: 0.0017638777547373796\n",
      "Training loss: 0.001762479537676823\n",
      "Training loss: 0.00176108305095102\n",
      "Training loss: 0.0017596882916541296\n",
      "Training loss: 0.001758295256886495\n",
      "Training loss: 0.0017569039437545964\n",
      "Training loss: 0.0017555143493710717\n",
      "Training loss: 0.0017541264708546764\n",
      "Training loss: 0.0017527403053302804\n",
      "Training loss: 0.0017513558499288357\n",
      "Training loss: 0.0017499731017873888\n",
      "Training loss: 0.0017485920580490389\n",
      "Training loss: 0.0017472127158629292\n",
      "Training loss: 0.0017458350723842419\n",
      "Training loss: 0.0017444591247741709\n",
      "Training loss: 0.001743084870199919\n",
      "Training loss: 0.0017417123058346595\n",
      "Training loss: 0.001740341428857545\n",
      "Training loss: 0.0017389722364536833\n",
      "Training loss: 0.001737604725814128\n",
      "Training loss: 0.0017362388941358487\n",
      "Training loss: 0.001734874738621717\n",
      "Training loss: 0.0017335122564805176\n",
      "Training loss: 0.0017321514449269038\n",
      "Training loss: 0.0017307923011813955\n",
      "Training loss: 0.0017294348224703604\n",
      "Training loss: 0.0017280790060260026\n",
      "Training loss: 0.0017267248490863507\n",
      "Training loss: 0.001725372348895227\n",
      "Training loss: 0.0017240215027022527\n",
      "Training loss: 0.001722672307762823\n",
      "Training loss: 0.0017213247613380974\n",
      "Training loss: 0.001719978860694975\n",
      "Training loss: 0.0017186346031060878\n",
      "Training loss: 0.0017172919858497982\n",
      "Training loss: 0.0017159510062101494\n",
      "Training loss: 0.0017146116614768852\n",
      "Training loss: 0.0017132739489454317\n",
      "Training loss: 0.0017119378659168577\n",
      "Training loss: 0.0017106034096978926\n",
      "Training loss: 0.0017092705776008835\n",
      "Training loss: 0.0017079393669438034\n",
      "Training loss: 0.0017066097750502195\n",
      "Training loss: 0.0017052817992492975\n",
      "Training loss: 0.0017039554368757789\n",
      "Training loss: 0.0017026306852699537\n",
      "Training loss: 0.0017013075417776642\n",
      "Training loss: 0.0016999860037502875\n",
      "Training loss: 0.001698666068544722\n",
      "Training loss: 0.0016973477335233543\n",
      "Training loss: 0.0016960309960540848\n",
      "Training loss: 0.0016947158535102714\n",
      "Training loss: 0.0016934023032707487\n",
      "Training loss: 0.0016920903427197923\n",
      "Training loss: 0.0016907799692471187\n",
      "Training loss: 0.001689471180247861\n",
      "Training loss: 0.0016881639731225649\n",
      "Training loss: 0.0016868583452771752\n",
      "Training loss: 0.0016855542941230136\n",
      "Training loss: 0.001684251817076758\n",
      "Training loss: 0.0016829509115604613\n",
      "Training loss: 0.0016816515750015066\n",
      "Training loss: 0.0016803538048326076\n",
      "Training loss: 0.0016790575984917883\n",
      "Training loss: 0.0016777629534223762\n",
      "Training loss: 0.0016764698670729916\n",
      "Training loss: 0.0016751783368975225\n",
      "Training loss: 0.0016738883603551148\n",
      "Training loss: 0.0016725999349101728\n",
      "Training loss: 0.0016713130580323387\n",
      "Training loss: 0.0016700277271964605\n",
      "Training loss: 0.0016687439398826026\n",
      "Training loss: 0.0016674616935760357\n",
      "Training loss: 0.0016661809857672022\n",
      "Training loss: 0.0016649018139517162\n",
      "Training loss: 0.001663624175630355\n",
      "Training loss: 0.0016623480683090348\n",
      "Training loss: 0.0016610734894988042\n",
      "Training loss: 0.0016598004367158372\n",
      "Training loss: 0.0016585289074813982\n",
      "Training loss: 0.0016572588993218646\n",
      "Training loss: 0.001655990409768686\n",
      "Training loss: 0.0016547234363583727\n",
      "Training loss: 0.0016534579766325054\n",
      "Training loss: 0.0016521940281377012\n",
      "Training loss: 0.0016509315884256032\n",
      "Training loss: 0.001649670655052886\n",
      "Training loss: 0.001648411225581211\n",
      "Training loss: 0.0016471532975772497\n",
      "Training loss: 0.001645896868612648\n",
      "Training loss: 0.0016446419362640221\n",
      "Training loss: 0.0016433884981129413\n",
      "Training loss: 0.0016421365517459254\n",
      "Training loss: 0.0016408860947544157\n",
      "Training loss: 0.0016396371247347885\n",
      "Training loss: 0.0016383896392883155\n",
      "Training loss: 0.0016371436360211643\n",
      "Training loss: 0.0016358991125443962\n",
      "Training loss: 0.0016346560664739368\n",
      "Training loss: 0.0016334144954305682\n",
      "Training loss: 0.0016321743970399238\n",
      "Training loss: 0.001630935768932477\n",
      "Training loss: 0.0016296986087435189\n",
      "Training loss: 0.0016284629141131584\n",
      "Training loss: 0.001627228682686291\n",
      "Training loss: 0.0016259959121126138\n",
      "Training loss: 0.0016247646000465904\n",
      "Training loss: 0.0016235347441474644\n",
      "Training loss: 0.00162230634207922\n",
      "Training loss: 0.0016210793915105834\n",
      "Training loss: 0.0016198538901150107\n",
      "Training loss: 0.0016186298355706803\n",
      "Training loss: 0.0016174072255604776\n",
      "Training loss: 0.0016161860577719764\n",
      "Training loss: 0.0016149663298974453\n",
      "Training loss: 0.0016137480396338083\n",
      "Training loss: 0.001612531184682666\n",
      "Training loss: 0.0016113157627502582\n",
      "Training loss: 0.0016101017715474696\n",
      "Training loss: 0.0016088892087898093\n",
      "Training loss: 0.0016076780721973895\n",
      "Training loss: 0.0016064683594949477\n",
      "Training loss: 0.0016052600684118023\n",
      "Training loss: 0.0016040531966818536\n",
      "Training loss: 0.0016028477420435697\n",
      "Training loss: 0.0016016437022399919\n",
      "Training loss: 0.0016004410750186878\n",
      "Training loss: 0.001599239858131784\n",
      "Training loss: 0.0015980400493359167\n",
      "Training loss: 0.0015968416463922437\n",
      "Training loss: 0.0015956446470664287\n",
      "Training loss: 0.0015944490491286228\n",
      "Training loss: 0.0015932548503534705\n",
      "Training loss: 0.0015920620485200695\n",
      "Training loss: 0.001590870641411992\n",
      "Training loss: 0.0015896806268172602\n",
      "Training loss: 0.0015884920025283284\n",
      "Training loss: 0.001587304766342083\n",
      "Training loss: 0.0015861189160598225\n",
      "Training loss: 0.001584934449487255\n",
      "Training loss: 0.0015837513644344921\n",
      "Training loss: 0.0015825696587160193\n",
      "Training loss: 0.0015813893301507057\n",
      "Training loss: 0.0015802103765617712\n",
      "Training loss: 0.0015790327957768092\n",
      "Training loss: 0.001577856585627739\n",
      "Training loss: 0.001576681743950819\n",
      "Training loss: 0.0015755082685866304\n",
      "Training loss: 0.0015743361573800603\n",
      "Training loss: 0.0015731654081803078\n",
      "Training loss: 0.001571996018840858\n",
      "Training loss: 0.0015708279872194675\n",
      "Training loss: 0.0015696613111781802\n",
      "Training loss: 0.0015684959885832858\n",
      "Training loss: 0.0015673320173053285\n",
      "Training loss: 0.0015661693952190954\n",
      "Training loss: 0.0015650081202035918\n",
      "Training loss: 0.0015638481901420578\n",
      "Training loss: 0.0015626896029219295\n",
      "Training loss: 0.0015615323564348524\n",
      "Training loss: 0.001560376448576647\n",
      "Training loss: 0.0015592218772473273\n",
      "Training loss: 0.001558068640351066\n",
      "Training loss: 0.0015569167357961915\n",
      "Training loss: 0.0015557661614952034\n",
      "Training loss: 0.0015546169153647091\n",
      "Training loss: 0.0015534689953254645\n",
      "Training loss: 0.0015523223993023341\n",
      "Training loss: 0.0015511771252243046\n",
      "Training loss: 0.0015500331710244515\n",
      "Training loss: 0.0015488905346399387\n",
      "Training loss: 0.0015477492140120168\n",
      "Training loss: 0.0015466092070860026\n",
      "Training loss: 0.0015454705118112693\n",
      "Training loss: 0.0015443331261412463\n",
      "Training loss: 0.0015431970480334054\n",
      "Training loss: 0.0015420622754492444\n",
      "Training loss: 0.0015409288063542809\n",
      "Training loss: 0.0015397966387180506\n",
      "Training loss: 0.0015386657705140868\n",
      "Training loss: 0.0015375361997199235\n",
      "Training loss: 0.0015364079243170715\n",
      "Training loss: 0.0015352809422910113\n",
      "Training loss: 0.0015341552516311995\n",
      "Training loss: 0.0015330308503310331\n",
      "Training loss: 0.00153190773638787\n",
      "Training loss: 0.001530785907802994\n",
      "Training loss: 0.001529665362581619\n",
      "Training loss: 0.0015285460987328778\n",
      "Training loss: 0.0015274281142698075\n",
      "Training loss: 0.0015263114072093448\n",
      "Training loss: 0.0015251959755723235\n",
      "Training loss: 0.0015240818173834524\n",
      "Training loss: 0.0015229689306713037\n",
      "Training loss: 0.00152185731346833\n",
      "Training loss: 0.001520746963810821\n",
      "Training loss: 0.0015196378797389153\n",
      "Training loss: 0.0015185300592965884\n",
      "Training loss: 0.0015174235005316445\n",
      "Training loss: 0.0015163182014956908\n",
      "Training loss: 0.0015152141602441584\n",
      "Training loss: 0.001514111374836268\n",
      "Training loss: 0.0015130098433350301\n",
      "Training loss: 0.0015119095638072401\n",
      "Training loss: 0.0015108105343234666\n",
      "Training loss: 0.0015097127529580298\n",
      "Training loss: 0.00150861621778902\n",
      "Training loss: 0.0015075209268982552\n",
      "Training loss: 0.0015064268783713064\n",
      "Training loss: 0.001505334070297466\n",
      "Training loss: 0.001504242500769738\n",
      "Training loss: 0.001503152167884845\n",
      "Training loss: 0.0015020630697432117\n",
      "Training loss: 0.0015009752044489485\n",
      "Training loss: 0.0014998885701098578\n",
      "Training loss: 0.0014988031648374106\n",
      "Training loss: 0.00149771898674675\n",
      "Training loss: 0.0014966360339566697\n",
      "Training loss: 0.0014955543045896246\n",
      "Training loss: 0.0014944737967716982\n",
      "Training loss: 0.0014933945086326136\n",
      "Training loss: 0.0014923164383057192\n",
      "Training loss: 0.0014912395839279695\n",
      "Training loss: 0.001490163943639939\n",
      "Training loss: 0.0014890895155857942\n",
      "Training loss: 0.001488016297913285\n",
      "Training loss: 0.0014869442887737552\n",
      "Training loss: 0.001485873486322115\n",
      "Training loss: 0.001484803888716839\n",
      "Training loss: 0.0014837354941199658\n",
      "Training loss: 0.0014826683006970759\n",
      "Training loss: 0.0014816023066172896\n",
      "Training loss: 0.0014805375100532573\n",
      "Training loss: 0.001479473909181163\n",
      "Training loss: 0.0014784115021806966\n",
      "Training loss: 0.0014773502872350572\n",
      "Training loss: 0.0014762902625309459\n",
      "Training loss: 0.0014752314262585526\n",
      "Training loss: 0.001474173776611549\n",
      "Training loss: 0.0014731173117870826\n",
      "Training loss: 0.001472062029985769\n",
      "Training loss: 0.0014710079294116788\n",
      "Training loss: 0.0014699550082723403\n",
      "Training loss: 0.0014689032647787117\n",
      "Training loss: 0.001467852697145198\n",
      "Training loss: 0.0014668033035896211\n",
      "Training loss: 0.0014657550823332303\n",
      "Training loss: 0.0014647080316006833\n",
      "Training loss: 0.0014636621496200363\n",
      "Training loss: 0.0014626174346227432\n",
      "Training loss: 0.0014615738848436474\n",
      "Training loss: 0.0014605314985209647\n",
      "Training loss: 0.0014594902738962884\n",
      "Training loss: 0.0014584502092145702\n",
      "Training loss: 0.0014574113027241225\n",
      "Training loss: 0.0014563735526766056\n",
      "Training loss: 0.00145533695732702\n",
      "Training loss: 0.0014543015149336923\n",
      "Training loss: 0.00145326722375828\n",
      "Training loss: 0.0014522340820657593\n",
      "Training loss: 0.0014512020881244135\n",
      "Training loss: 0.0014501712402058236\n",
      "Training loss: 0.0014491415365848728\n",
      "Training loss: 0.0014481129755397266\n",
      "Training loss: 0.0014470855553518314\n",
      "Training loss: 0.0014460592743059022\n",
      "Training loss: 0.0014450341306899194\n",
      "Training loss: 0.0014440101227951236\n",
      "Training loss: 0.0014429872489159987\n",
      "Training loss: 0.0014419655073502722\n",
      "Training loss: 0.001440944896398913\n",
      "Training loss: 0.0014399254143661023\n",
      "Training loss: 0.0014389070595592576\n",
      "Training loss: 0.001437889830288991\n",
      "Training loss: 0.001436873724869136\n",
      "Training loss: 0.0014358587416167066\n",
      "Training loss: 0.0014348448788519177\n",
      "Training loss: 0.0014338321348981691\n",
      "Training loss: 0.0014328205080820255\n",
      "Training loss: 0.0014318099967332337\n",
      "Training loss: 0.0014308005991846857\n",
      "Training loss: 0.001429792313772432\n",
      "Training loss: 0.0014287851388356772\n",
      "Training loss: 0.0014277790727167572\n",
      "Training loss: 0.0014267741137611448\n",
      "Training loss: 0.001425770260317431\n",
      "Training loss: 0.0014247675107373354\n",
      "Training loss: 0.0014237658633756728\n",
      "Training loss: 0.0014227653165903786\n",
      "Training loss: 0.0014217658687424729\n",
      "Training loss: 0.001420767518196067\n",
      "Training loss: 0.0014197702633183568\n",
      "Training loss: 0.0014187741024796186\n",
      "Training loss: 0.0014177790340531816\n",
      "Training loss: 0.0014167850564154511\n",
      "Training loss: 0.0014157921679458845\n",
      "Training loss: 0.001414800367026977\n",
      "Training loss: 0.001413809652044278\n",
      "Training loss: 0.001412820021386357\n",
      "Training loss: 0.001411831473444823\n",
      "Training loss: 0.0014108440066142908\n",
      "Training loss: 0.0014098576192924026\n",
      "Training loss: 0.0014088723098797953\n",
      "Training loss: 0.0014078880767801139\n",
      "Training loss: 0.0014069049183999874\n",
      "Training loss: 0.0014059228331490417\n",
      "Training loss: 0.0014049418194398678\n",
      "Training loss: 0.0014039618756880408\n",
      "Training loss: 0.001402983000312097\n",
      "Training loss: 0.0014020051917335275\n",
      "Training loss: 0.0014010284483767814\n",
      "Training loss: 0.0014000527686692493\n",
      "Training loss: 0.001399078151041267\n",
      "Training loss: 0.0013981045939260928\n",
      "Training loss: 0.0013971320957599156\n",
      "Training loss: 0.0013961606549818468\n",
      "Training loss: 0.0013951902700339008\n",
      "Training loss: 0.001394220939361006\n",
      "Training loss: 0.0013932526614109893\n",
      "Training loss: 0.0013922854346345684\n",
      "Training loss: 0.0013913192574853434\n",
      "Training loss: 0.001390354128419796\n",
      "Training loss: 0.0013893900458972865\n",
      "Training loss: 0.0013884270083800352\n",
      "Training loss: 0.0013874650143331307\n",
      "Training loss: 0.0013865040622245068\n",
      "Training loss: 0.0013855441505249474\n",
      "Training loss: 0.0013845852777080803\n",
      "Training loss: 0.0013836274422503673\n",
      "Training loss: 0.0013826706426310947\n",
      "Training loss: 0.0013817148773323724\n",
      "Training loss: 0.0013807601448391272\n",
      "Training loss: 0.0013798064436390933\n",
      "Training loss: 0.001378853772222813\n",
      "Training loss: 0.001377902129083618\n",
      "Training loss: 0.0013769515127176352\n",
      "Training loss: 0.001376001921623775\n",
      "Training loss: 0.001375053354303725\n",
      "Training loss: 0.0013741058092619413\n",
      "Training loss: 0.0013731592850056516\n",
      "Training loss: 0.0013722137800448425\n",
      "Training loss: 0.0013712692928922444\n",
      "Training loss: 0.0013703258220633518\n",
      "Training loss: 0.0013693833660763835\n",
      "Training loss: 0.0013684419234523005\n",
      "Training loss: 0.0013675014927147966\n",
      "Training loss: 0.0013665620723902797\n",
      "Training loss: 0.0013656236610078748\n",
      "Training loss: 0.0013646862570994287\n",
      "Training loss: 0.0013637498591994792\n",
      "Training loss: 0.0013628144658452649\n",
      "Training loss: 0.001361880075576728\n",
      "Training loss: 0.0013609466869364797\n",
      "Training loss: 0.0013600142984698316\n",
      "Training loss: 0.0013590829087247495\n",
      "Training loss: 0.0013581525162518835\n",
      "Training loss: 0.0013572231196045351\n",
      "Training loss: 0.0013562947173386718\n",
      "Training loss: 0.001355367308012904\n",
      "Training loss: 0.0013544408901884947\n",
      "Training loss: 0.001353515462429338\n",
      "Training loss: 0.0013525910233019728\n",
      "Training loss: 0.0013516675713755512\n",
      "Training loss: 0.0013507451052218516\n",
      "Training loss: 0.001349823623415277\n",
      "Training loss: 0.0013489031245328257\n",
      "Training loss: 0.0013479836071541124\n",
      "Training loss: 0.0013470650698613452\n",
      "Training loss: 0.0013461475112393224\n",
      "Training loss: 0.0013452309298754377\n",
      "Training loss: 0.001344315324359655\n",
      "Training loss: 0.0013434006932845205\n",
      "Training loss: 0.001342487035245154\n",
      "Training loss: 0.0013415743488392243\n",
      "Training loss: 0.0013406626326669796\n",
      "Training loss: 0.0013397518853312017\n",
      "Training loss: 0.001338842105437232\n",
      "Training loss: 0.0013379332915929496\n",
      "Training loss: 0.0013370254424087678\n",
      "Training loss: 0.0013361185564976343\n",
      "Training loss: 0.0013352126324750178\n",
      "Training loss: 0.001334307668958908\n",
      "Training loss: 0.0013334036645698127\n",
      "Training loss: 0.0013325006179307334\n",
      "Training loss: 0.0013315985276671917\n",
      "Training loss: 0.0013306973924072005\n",
      "Training loss: 0.001329797210781261\n",
      "Training loss: 0.0013288979814223576\n",
      "Training loss: 0.0013279997029659685\n",
      "Training loss: 0.0013271023740500365\n",
      "Training loss: 0.001326205993314977\n",
      "Training loss: 0.0013253105594036724\n",
      "Training loss: 0.00132441607096146\n",
      "Training loss: 0.001323522526636129\n",
      "Training loss: 0.0013226299250779288\n",
      "Training loss: 0.0013217382649395365\n",
      "Training loss: 0.0013208475448760804\n",
      "Training loss: 0.0013199577635451098\n",
      "Training loss: 0.001319068919606608\n",
      "Training loss: 0.0013181810117229784\n",
      "Training loss: 0.0013172940385590406\n",
      "Training loss: 0.0013164079987820273\n",
      "Training loss: 0.0013155228910615663\n",
      "Training loss: 0.0013146387140697067\n",
      "Training loss: 0.0013137554664808735\n",
      "Training loss: 0.0013128731469718928\n",
      "Training loss: 0.0013119917542219683\n",
      "Training loss: 0.0013111112869126907\n",
      "Training loss: 0.0013102317437280215\n",
      "Training loss: 0.0013093531233542882\n",
      "Training loss: 0.0013084754244801896\n",
      "Training loss: 0.001307598645796784\n",
      "Training loss: 0.0013067227859974728\n",
      "Training loss: 0.0013058478437780155\n",
      "Training loss: 0.0013049738178365145\n",
      "Training loss: 0.0013041007068734095\n",
      "Training loss: 0.0013032285095914758\n",
      "Training loss: 0.0013023572246958087\n",
      "Training loss: 0.0013014868508938419\n",
      "Training loss: 0.001300617386895312\n",
      "Training loss: 0.0012997488314122816\n",
      "Training loss: 0.0012988811831591096\n",
      "Training loss: 0.0012980144408524744\n",
      "Training loss: 0.0012971486032113332\n",
      "Training loss: 0.0012962836689569507\n",
      "Training loss: 0.001295419636812878\n",
      "Training loss: 0.0012945565055049427\n",
      "Training loss: 0.0012936942737612606\n",
      "Training loss: 0.001292832940312214\n",
      "Training loss: 0.0012919725038904547\n",
      "Training loss: 0.001291112963230903\n",
      "Training loss: 0.0012902543170707314\n",
      "Training loss: 0.001289396564149372\n",
      "Training loss: 0.0012885397032085038\n",
      "Training loss: 0.001287683732992052\n",
      "Training loss: 0.0012868286522461757\n",
      "Training loss: 0.0012859744597192768\n",
      "Training loss: 0.0012851211541619792\n",
      "Training loss: 0.0012842687343271359\n",
      "Training loss: 0.0012834171989698218\n",
      "Training loss: 0.001282566546847324\n",
      "Training loss: 0.001281716776719139\n",
      "Training loss: 0.0012808678873469756\n",
      "Training loss: 0.0012800198774947367\n",
      "Training loss: 0.0012791727459285243\n",
      "Training loss: 0.0012783264914166346\n",
      "Training loss: 0.0012774811127295482\n",
      "Training loss: 0.001276636608639929\n",
      "Training loss: 0.001275792977922619\n",
      "Training loss: 0.0012749502193546277\n",
      "Training loss: 0.0012741083317151431\n",
      "Training loss: 0.0012732673137855093\n",
      "Training loss: 0.0012724271643492352\n",
      "Training loss: 0.0012715878821919749\n",
      "Training loss: 0.0012707494661015396\n",
      "Training loss: 0.0012699119148678823\n",
      "Training loss: 0.0012690752272831005\n",
      "Training loss: 0.001268239402141426\n",
      "Training loss: 0.0012674044382392214\n",
      "Training loss: 0.0012665703343749763\n",
      "Training loss: 0.001265737089349302\n",
      "Training loss: 0.0012649047019649285\n",
      "Training loss: 0.001264073171026698\n",
      "Training loss: 0.0012632424953415613\n",
      "Training loss: 0.0012624126737185806\n",
      "Training loss: 0.0012615837049689091\n",
      "Training loss: 0.001260755587905796\n",
      "Training loss: 0.0012599283213445825\n",
      "Training loss: 0.0012591019041027019\n",
      "Training loss: 0.0012582763349996632\n",
      "Training loss: 0.0012574516128570524\n",
      "Training loss: 0.0012566277364985288\n",
      "Training loss: 0.0012558047047498328\n",
      "Training loss: 0.001254982516438751\n",
      "Training loss: 0.001254161170395135\n",
      "Training loss: 0.0012533406654508987\n",
      "Training loss: 0.0012525210004400015\n",
      "Training loss: 0.0012517021741984546\n",
      "Training loss: 0.0012508841855643034\n",
      "Training loss: 0.0012500670333776386\n",
      "Training loss: 0.0012492507164805838\n",
      "Training loss: 0.0012484352337172905\n",
      "Training loss: 0.001247620583933936\n",
      "Training loss: 0.0012468067659787171\n",
      "Training loss: 0.001245993778701845\n",
      "Training loss: 0.0012451816209555504\n",
      "Training loss: 0.0012443702915940682\n",
      "Training loss: 0.0012435597894736381\n",
      "Training loss: 0.0012427501134524956\n",
      "Training loss: 0.001241941262390878\n",
      "Training loss: 0.0012411332351510073\n",
      "Training loss: 0.001240326030597096\n",
      "Training loss: 0.00123951964759534\n",
      "Training loss: 0.0012387140850139162\n",
      "Training loss: 0.0012379093417229626\n",
      "Training loss: 0.0012371054165946026\n",
      "Training loss: 0.0012363023085029262\n",
      "Training loss: 0.0012355000163239702\n",
      "Training loss: 0.0012346985389357454\n",
      "Training loss: 0.0012338978752182038\n",
      "Training loss: 0.0012330980240532557\n",
      "Training loss: 0.0012322989843247525\n",
      "Training loss: 0.0012315007549184936\n",
      "Training loss: 0.001230703334722205\n",
      "Training loss: 0.0012299067226255516\n",
      "Training loss: 0.001229110917520128\n",
      "Training loss: 0.0012283159182994517\n",
      "Training loss: 0.0012275217238589642\n",
      "Training loss: 0.001226728333096023\n",
      "Training loss: 0.001225935744909897\n",
      "Training loss: 0.001225143958201766\n",
      "Training loss: 0.0012243529718747104\n",
      "Training loss: 0.00122356278483372\n",
      "Training loss: 0.0012227733959856721\n",
      "Training loss: 0.0012219848042393463\n",
      "Training loss: 0.0012211970085054052\n",
      "Training loss: 0.0012204100076963946\n",
      "Training loss: 0.001219623800726752\n",
      "Training loss: 0.0012188383865127798\n",
      "Training loss: 0.0012180537639726611\n",
      "Training loss: 0.001217269932026444\n",
      "Training loss: 0.0012164868895960476\n",
      "Training loss: 0.001215704635605245\n",
      "Training loss: 0.0012149231689796774\n",
      "Training loss: 0.0012141424886468264\n",
      "Training loss: 0.0012133625935360367\n",
      "Training loss: 0.0012125834825784903\n",
      "Training loss: 0.001211805154707217\n",
      "Training loss: 0.0012110276088570823\n",
      "Training loss: 0.0012102508439647865\n",
      "Training loss: 0.0012094748589688603\n",
      "Training loss: 0.001208699652809657\n",
      "Training loss: 0.0012079252244293663\n",
      "Training loss: 0.0012071515727719848\n",
      "Training loss: 0.001206378696783324\n",
      "Training loss: 0.0012056065954110192\n",
      "Training loss: 0.001204835267604506\n",
      "Training loss: 0.0012040647123150174\n",
      "Training loss: 0.001203294928495603\n",
      "Training loss: 0.001202525915101091\n",
      "Training loss: 0.0012017576710881225\n",
      "Training loss: 0.0012009901954151063\n",
      "Training loss: 0.001200223487042257\n",
      "Training loss: 0.001199457544931558\n",
      "Training loss: 0.0011986923680467768\n",
      "Training loss: 0.0011979279553534542\n",
      "Training loss: 0.0011971643058189\n",
      "Training loss: 0.0011964014184121913\n",
      "Training loss: 0.0011956392921041712\n",
      "Training loss: 0.0011948779258674459\n",
      "Training loss: 0.0011941173186763665\n",
      "Training loss: 0.0011933574695070482\n",
      "Training loss: 0.0011925983773373493\n",
      "Training loss: 0.001191840041146874\n",
      "Training loss: 0.0011910824599169708\n",
      "Training loss: 0.0011903256326307233\n",
      "Training loss: 0.0011895695582729543\n",
      "Training loss: 0.0011888142358302109\n",
      "Training loss: 0.0011880596642907765\n",
      "Training loss: 0.0011873058426446483\n",
      "Training loss: 0.0011865527698835523\n",
      "Training loss: 0.001185800445000921\n",
      "Training loss: 0.0011850488669919138\n",
      "Training loss: 0.001184298034853393\n",
      "Training loss: 0.0011835479475839252\n",
      "Training loss: 0.001182798604183784\n",
      "Training loss: 0.0011820500036549358\n",
      "Training loss: 0.0011813021450010472\n",
      "Training loss: 0.001180555027227481\n",
      "Training loss: 0.0011798086493412822\n",
      "Training loss: 0.0011790630103511834\n",
      "Training loss: 0.0011783181092675985\n",
      "Training loss: 0.0011775739451026195\n",
      "Training loss: 0.0011768305168700184\n",
      "Training loss: 0.0011760878235852286\n",
      "Training loss: 0.0011753458642653589\n",
      "Training loss: 0.0011746046379291807\n",
      "Training loss: 0.0011738641435971328\n",
      "Training loss: 0.0011731243802913017\n",
      "Training loss: 0.0011723853470354323\n",
      "Training loss: 0.0011716470428549221\n",
      "Training loss: 0.0011709094667768129\n",
      "Training loss: 0.0011701726178297994\n",
      "Training loss: 0.00116943649504421\n",
      "Training loss: 0.0011687010974520073\n",
      "Training loss: 0.001167966424086795\n",
      "Training loss: 0.0011672324739838073\n",
      "Training loss: 0.0011664992461799012\n",
      "Training loss: 0.0011657667397135627\n",
      "Training loss: 0.0011650349536248977\n",
      "Training loss: 0.0011643038869556272\n",
      "Training loss: 0.0011635735387490924\n",
      "Training loss: 0.0011628439080502376\n",
      "Training loss: 0.0011621149939056205\n",
      "Training loss: 0.0011613867953634014\n",
      "Training loss: 0.0011606593114733413\n",
      "Training loss: 0.0011599325412868042\n",
      "Training loss: 0.0011592064838567423\n",
      "Training loss: 0.0011584811382376993\n",
      "Training loss: 0.0011577565034858104\n",
      "Training loss: 0.0011570325786587996\n",
      "Training loss: 0.001156309362815963\n",
      "Training loss: 0.0011555868550181857\n",
      "Training loss: 0.0011548650543279188\n",
      "Training loss: 0.001154143959809191\n",
      "Training loss: 0.0011534235705275974\n",
      "Training loss: 0.001152703885550304\n",
      "Training loss: 0.0011519849039460376\n",
      "Training loss: 0.001151266624785078\n",
      "Training loss: 0.0011505490471392648\n",
      "Training loss: 0.0011498321700819964\n",
      "Training loss: 0.001149115992688219\n",
      "Training loss: 0.0011484005140344202\n",
      "Training loss: 0.0011476857331986306\n",
      "Training loss: 0.001146971649260437\n",
      "Training loss: 0.0011462582613009408\n",
      "Training loss: 0.0011455455684027999\n",
      "Training loss: 0.0011448335696501869\n",
      "Training loss: 0.0011441222641288102\n",
      "Training loss: 0.0011434116509259034\n",
      "Training loss: 0.0011427017291302187\n",
      "Training loss: 0.0011419924978320313\n",
      "Training loss: 0.0011412839561231291\n",
      "Training loss: 0.0011405761030968164\n",
      "Training loss: 0.0011398689378479048\n",
      "Training loss: 0.001139162459472714\n",
      "Training loss: 0.0011384566670690668\n",
      "Training loss: 0.0011377515597362847\n",
      "Training loss: 0.001137047136575191\n",
      "Training loss: 0.001136343396688098\n",
      "Training loss: 0.0011356403391788207\n",
      "Training loss: 0.0011349379631526483\n",
      "Training loss: 0.0011342362677163687\n",
      "Training loss: 0.0011335352519782418\n",
      "Training loss: 0.001132834915048015\n",
      "Training loss: 0.001132135256036912\n",
      "Training loss: 0.001131436274057625\n",
      "Training loss: 0.0011307379682243192\n",
      "Training loss: 0.0011300403376526325\n",
      "Training loss: 0.00112934338145966\n",
      "Training loss: 0.001128647098763967\n",
      "Training loss: 0.0011279514886855734\n",
      "Training loss: 0.0011272565503459512\n",
      "Training loss: 0.0011265622828680346\n",
      "Training loss: 0.0011258686853762015\n",
      "Training loss: 0.00112517575699628\n",
      "Training loss: 0.0011244834968555436\n",
      "Training loss: 0.0011237919040827033\n",
      "Training loss: 0.0011231009778079125\n",
      "Training loss: 0.0011224107171627657\n",
      "Training loss: 0.001121721121280276\n",
      "Training loss: 0.0011210321892949015\n",
      "Training loss: 0.0011203439203425194\n",
      "Training loss: 0.0011196563135604364\n",
      "Training loss: 0.001118969368087372\n",
      "Training loss: 0.001118283083063474\n",
      "Training loss: 0.0011175974576303025\n",
      "Training loss: 0.001116912490930834\n",
      "Training loss: 0.0011162281821094462\n",
      "Training loss: 0.0011155445303119376\n",
      "Training loss: 0.0011148615346854981\n",
      "Training loss: 0.0011141791943787293\n",
      "Training loss: 0.001113497508541631\n",
      "Training loss: 0.0011128164763255918\n",
      "Training loss: 0.0011121360968834035\n",
      "Training loss: 0.001111456369369241\n",
      "Training loss: 0.0011107772929386714\n",
      "Training loss: 0.0011100988667486489\n",
      "Training loss: 0.0011094210899575052\n",
      "Training loss: 0.0011087439617249548\n",
      "Training loss: 0.001108067481212091\n",
      "Training loss: 0.0011073916475813784\n",
      "Training loss: 0.0011067164599966547\n",
      "Training loss: 0.0011060419176231236\n",
      "Training loss: 0.0011053680196273653\n",
      "Training loss: 0.0011046947651773076\n",
      "Training loss: 0.0011040221534422523\n",
      "Training loss: 0.001103350183592854\n",
      "Training loss: 0.0011026788548011202\n",
      "Training loss: 0.001102008166240415\n",
      "Training loss: 0.0011013381170854538\n",
      "Training loss: 0.0011006687065122974\n",
      "Training loss: 0.001099999933698346\n",
      "Training loss: 0.0010993317978223542\n",
      "Training loss: 0.0010986642980644105\n",
      "Training loss: 0.0010979974336059334\n",
      "Training loss: 0.0010973312036296808\n",
      "Training loss: 0.0010966656073197441\n",
      "Training loss: 0.0010960006438615446\n",
      "Training loss: 0.001095336312441826\n",
      "Training loss: 0.0010946726122486533\n",
      "Training loss: 0.0010940095424714215\n",
      "Training loss: 0.0010933471023008383\n",
      "Training loss: 0.0010926852909289228\n",
      "Training loss: 0.001092024107549017\n",
      "Training loss: 0.0010913635513557667\n",
      "Training loss: 0.001090703621545133\n",
      "Training loss: 0.0010900443173143734\n",
      "Training loss: 0.0010893856378620544\n",
      "Training loss: 0.0010887275823880408\n",
      "Training loss: 0.0010880701500934969\n",
      "Training loss: 0.001087413340180881\n",
      "Training loss: 0.0010867571518539446\n",
      "Training loss: 0.0010861015843177271\n",
      "Training loss: 0.001085446636778564\n",
      "Training loss: 0.001084792308444064\n",
      "Training loss: 0.0010841385985231294\n",
      "Training loss: 0.0010834855062259341\n",
      "Training loss: 0.0010828330307639388\n",
      "Training loss: 0.0010821811713498694\n",
      "Training loss: 0.0010815299271977295\n",
      "Training loss: 0.001080879297522795\n",
      "Training loss: 0.0010802292815416073\n",
      "Training loss: 0.0010795798784719731\n",
      "Training loss: 0.00107893108753296\n",
      "Training loss: 0.0010782829079448997\n",
      "Training loss: 0.0010776353389293743\n",
      "Training loss: 0.0010769883797092322\n",
      "Training loss: 0.0010763420295085693\n",
      "Training loss: 0.0010756962875527268\n",
      "Training loss: 0.0010750511530683053\n",
      "Training loss: 0.0010744066252831458\n",
      "Training loss: 0.0010737627034263229\n",
      "Training loss: 0.00107311938672817\n",
      "Training loss: 0.0010724766744202436\n",
      "Training loss: 0.001071834565735346\n",
      "Training loss: 0.0010711930599075065\n",
      "Training loss: 0.0010705521561719896\n",
      "Training loss: 0.0010699118537652904\n",
      "Training loss: 0.0010692721519251181\n",
      "Training loss: 0.001068633049890423\n",
      "Training loss: 0.0010679945469013663\n",
      "Training loss: 0.0010673566421993334\n",
      "Training loss: 0.0010667193350269225\n",
      "Training loss: 0.0010660826246279445\n",
      "Training loss: 0.0010654465102474314\n",
      "Training loss: 0.0010648109911316165\n",
      "Training loss: 0.001064176066527949\n",
      "Training loss: 0.0010635417356850711\n",
      "Training loss: 0.0010629079978528376\n",
      "Training loss: 0.0010622748522823022\n",
      "Training loss: 0.0010616422982257152\n",
      "Training loss: 0.0010610103349365193\n",
      "Training loss: 0.0010603789616693555\n",
      "Training loss: 0.0010597481776800587\n",
      "Training loss: 0.001059117982225648\n",
      "Training loss: 0.0010584883745643275\n",
      "Training loss: 0.0010578593539554899\n",
      "Training loss: 0.001057230919659704\n",
      "Training loss: 0.001056603070938726\n",
      "Training loss: 0.001055975807055487\n",
      "Training loss: 0.0010553491272740879\n",
      "Training loss: 0.0010547230308598097\n",
      "Training loss: 0.0010540975170791\n",
      "Training loss: 0.0010534725851995771\n",
      "Training loss: 0.0010528482344900243\n",
      "Training loss: 0.00105222446422039\n",
      "Training loss: 0.0010516012736617784\n",
      "Training loss: 0.0010509786620864624\n",
      "Training loss: 0.0010503566287678632\n",
      "Training loss: 0.0010497351729805648\n",
      "Training loss: 0.0010491142940002992\n",
      "Training loss: 0.0010484939911039486\n",
      "Training loss: 0.0010478742635695471\n",
      "Training loss: 0.001047255110676274\n",
      "Training loss: 0.0010466365317044516\n",
      "Training loss: 0.0010460185259355394\n",
      "Training loss: 0.0010454010926521417\n",
      "Training loss: 0.0010447842311380022\n",
      "Training loss: 0.0010441679406779938\n",
      "Training loss: 0.0010435522205581263\n",
      "Training loss: 0.0010429370700655372\n",
      "Training loss: 0.0010423224884884984\n",
      "Training loss: 0.0010417084751164032\n",
      "Training loss: 0.001041095029239769\n",
      "Training loss: 0.00104048215015024\n",
      "Training loss: 0.001039869837140575\n",
      "Training loss: 0.001039258089504653\n",
      "Training loss: 0.0010386469065374706\n",
      "Training loss: 0.001038036287535135\n",
      "Training loss: 0.0010374262317948653\n",
      "Training loss: 0.0010368167386149898\n",
      "Training loss: 0.0010362078072949478\n",
      "Training loss: 0.0010355994371352819\n",
      "Training loss: 0.0010349916274376319\n",
      "Training loss: 0.0010343843775047475\n",
      "Training loss: 0.0010337776866404676\n",
      "Training loss: 0.0010331715541497422\n",
      "Training loss: 0.0010325659793385998\n",
      "Training loss: 0.0010319609615141757\n",
      "Training loss: 0.001031356499984679\n",
      "Training loss: 0.0010307525940594204\n",
      "Training loss: 0.0010301492430487996\n",
      "Training loss: 0.0010295464462642845\n",
      "Training loss: 0.0010289442030184414\n",
      "Training loss: 0.0010283425126249085\n",
      "Training loss: 0.0010277413743984007\n",
      "Training loss: 0.001027140787654717\n",
      "Training loss: 0.0010265407517107202\n",
      "Training loss: 0.0010259412658843543\n",
      "Training loss: 0.0010253423294946287\n",
      "Training loss: 0.0010247439418616228\n",
      "Training loss: 0.0010241461023064765\n",
      "Training loss: 0.0010235488101513978\n",
      "Training loss: 0.0010229520647196551\n",
      "Training loss: 0.0010223558653355788\n",
      "Training loss: 0.0010217602113245599\n",
      "Training loss: 0.0010211651020130324\n",
      "Training loss: 0.0010205705367285006\n",
      "Training loss: 0.0010199765147995072\n",
      "Training loss: 0.0010193830355556517\n",
      "Training loss: 0.0010187900983275808\n",
      "Training loss: 0.0010181977024469853\n",
      "Training loss: 0.0010176058472466\n",
      "Training loss: 0.0010170145320602018\n",
      "Training loss: 0.0010164237562226132\n",
      "Training loss: 0.0010158335190696827\n",
      "Training loss: 0.0010152438199383025\n",
      "Training loss: 0.001014654658166399\n",
      "Training loss: 0.001014066033092932\n",
      "Training loss: 0.001013477944057885\n",
      "Training loss: 0.0010128903904022765\n",
      "Training loss: 0.0010123033714681498\n",
      "Training loss: 0.001011716886598565\n",
      "Training loss: 0.0010111309351376148\n",
      "Training loss: 0.0010105455164304072\n",
      "Training loss: 0.00100996062982307\n",
      "Training loss: 0.0010093762746627483\n",
      "Training loss: 0.0010087924502976\n",
      "Training loss: 0.0010082091560767964\n",
      "Training loss: 0.0010076263913505199\n",
      "Training loss: 0.0010070441554699662\n",
      "Training loss: 0.0010064624477873303\n",
      "Training loss: 0.001005881267655816\n",
      "Training loss: 0.0010053006144296322\n",
      "Training loss: 0.0010047204874639873\n",
      "Training loss: 0.0010041408861150873\n",
      "Training loss: 0.0010035618097401443\n",
      "Training loss: 0.0010029832576973546\n",
      "Training loss: 0.001002405229345916\n",
      "Training loss: 0.0010018277240460163\n",
      "Training loss: 0.0010012507411588315\n",
      "Training loss: 0.001000674280046533\n",
      "Training loss: 0.0010000983400722668\n",
      "Training loss: 0.000999522920600179\n",
      "Training loss: 0.0009989480209953857\n",
      "Training loss: 0.000998373640623988\n",
      "Training loss: 0.00099779977885307\n",
      "Training loss: 0.0009972264350506842\n",
      "Training loss: 0.000996653608585871\n",
      "Training loss: 0.000996081298828631\n",
      "Training loss: 0.0009955095051499476\n",
      "Training loss: 0.0009949382269217695\n",
      "Training loss: 0.0009943674635170144\n",
      "Training loss: 0.000993797214309564\n",
      "Training loss: 0.0009932274786742694\n",
      "Training loss: 0.0009926582559869426\n",
      "Training loss: 0.000992089545624353\n",
      "Training loss: 0.0009915213469642327\n",
      "Training loss: 0.000990953659385277\n",
      "Training loss: 0.0009903864822671242\n",
      "Training loss: 0.000989819814990373\n",
      "Training loss: 0.0009892536569365798\n",
      "Training loss: 0.00098868800748824\n",
      "Training loss: 0.0009881228660288093\n",
      "Training loss: 0.0009875582319426816\n",
      "Training loss: 0.000986994104615197\n",
      "Training loss: 0.0009864304834326413\n",
      "Training loss: 0.0009858673677822407\n",
      "Training loss: 0.000985304757052163\n",
      "Training loss: 0.000984742650631515\n",
      "Training loss: 0.0009841810479103352\n",
      "Training loss: 0.0009836199482795975\n",
      "Training loss: 0.0009830593511312136\n",
      "Training loss: 0.0009824992558580218\n",
      "Training loss: 0.000981939661853791\n",
      "Training loss: 0.0009813805685132192\n",
      "Training loss: 0.0009808219752319269\n",
      "Training loss: 0.0009802638814064627\n",
      "Training loss: 0.000979706286434296\n",
      "Training loss: 0.0009791491897138147\n",
      "Training loss: 0.0009785925906443276\n",
      "Training loss: 0.0009780364886260654\n",
      "Training loss: 0.0009774808830601681\n",
      "Training loss: 0.0009769257733486956\n",
      "Training loss: 0.0009763711588946124\n",
      "Training loss: 0.0009758170391018019\n",
      "Training loss: 0.0009752634133750505\n",
      "Training loss: 0.0009747102811200544\n",
      "Training loss: 0.0009741576417434187\n",
      "Training loss: 0.0009736054946526448\n",
      "Training loss: 0.0009730538392561403\n",
      "Training loss: 0.0009725026749632187\n",
      "Training loss: 0.0009719520011840819\n",
      "Training loss: 0.0009714018173298398\n",
      "Training loss: 0.0009708521228124909\n",
      "Training loss: 0.0009703029170449279\n",
      "Training loss: 0.0009697541994409388\n",
      "Training loss: 0.0009692059694152038\n",
      "Training loss: 0.0009686582263832853\n",
      "Training loss: 0.0009681109697616433\n",
      "Training loss: 0.0009675641989676143\n",
      "Training loss: 0.0009670179134194218\n",
      "Training loss: 0.0009664721125361762\n",
      "Training loss: 0.0009659267957378638\n",
      "Training loss: 0.0009653819624453489\n",
      "Training loss: 0.000964837612080385\n",
      "Training loss: 0.0009642937440655839\n",
      "Training loss: 0.000963750357824447\n",
      "Training loss: 0.0009632074527813413\n",
      "Training loss: 0.0009626650283615061\n",
      "Training loss: 0.0009621230839910494\n",
      "Training loss: 0.0009615816190969563\n",
      "Training loss: 0.0009610406331070627\n",
      "Training loss: 0.0009605001254500858\n",
      "Training loss: 0.0009599600955555902\n",
      "Training loss: 0.0009594205428540143\n",
      "Training loss: 0.0009588814667766518\n",
      "Training loss: 0.0009583428667556626\n",
      "Training loss: 0.0009578047422240477\n",
      "Training loss: 0.0009572670926156768\n",
      "Training loss: 0.0009567299173652729\n",
      "Training loss: 0.0009561932159084028\n",
      "Training loss: 0.0009556569876814964\n",
      "Training loss: 0.0009551212321218197\n",
      "Training loss: 0.0009545859486674956\n",
      "Training loss: 0.0009540511367574913\n",
      "Training loss: 0.0009535167958316192\n",
      "Training loss: 0.0009529829253305291\n",
      "Training loss: 0.000952449524695718\n",
      "Training loss: 0.0009519165933695217\n",
      "Training loss: 0.0009513841307951177\n",
      "Training loss: 0.0009508521364165154\n",
      "Training loss: 0.00095032060967856\n",
      "Training loss: 0.0009497895500269327\n",
      "Training loss: 0.0009492589569081493\n",
      "Training loss: 0.0009487288297695526\n",
      "Training loss: 0.0009481991680593158\n",
      "Training loss: 0.0009476699712264363\n",
      "Training loss: 0.0009471412387207484\n",
      "Training loss: 0.0009466129699928999\n",
      "Training loss: 0.0009460851644943697\n",
      "Training loss: 0.0009455578216774565\n",
      "Training loss: 0.0009450309409952743\n",
      "Training loss: 0.0009445045219017676\n",
      "Training loss: 0.0009439785638516862\n",
      "Training loss: 0.0009434530663005997\n",
      "Training loss: 0.0009429280287048963\n",
      "Training loss: 0.0009424034505217731\n",
      "Training loss: 0.0009418793312092394\n",
      "Training loss: 0.0009413556702261184\n",
      "Training loss: 0.0009408324670320341\n",
      "Training loss: 0.0009403097210874231\n",
      "Training loss: 0.0009397874318535305\n",
      "Training loss: 0.0009392655987923982\n",
      "Training loss: 0.0009387442213668737\n",
      "Training loss: 0.0009382232990406121\n",
      "Training loss: 0.000937702831278057\n",
      "Training loss: 0.0009371828175444585\n",
      "Training loss: 0.0009366632573058634\n",
      "Training loss: 0.0009361441500291076\n",
      "Training loss: 0.00093562549518183\n",
      "Training loss: 0.0009351072922324594\n",
      "Training loss: 0.0009345895406502091\n",
      "Training loss: 0.0009340722399050908\n",
      "Training loss: 0.000933555389467901\n",
      "Training loss: 0.0009330389888102244\n",
      "Training loss: 0.0009325230374044309\n",
      "Training loss: 0.0009320075347236726\n",
      "Training loss: 0.0009314924802418859\n",
      "Training loss: 0.000930977873433787\n",
      "Training loss: 0.0009304637137748788\n",
      "Training loss: 0.0009299500007414311\n",
      "Training loss: 0.0009294367338105036\n",
      "Training loss: 0.0009289239124599252\n",
      "Training loss: 0.0009284115361682925\n",
      "Training loss: 0.0009278996044149884\n",
      "Training loss: 0.0009273881166801577\n",
      "Training loss: 0.0009268770724447216\n",
      "Training loss: 0.0009263664711903633\n",
      "Training loss: 0.0009258563123995398\n",
      "Training loss: 0.000925346595555472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0009248373201421431\n",
      "Training loss: 0.0009243284856443051\n",
      "Training loss: 0.0009238200915474649\n",
      "Training loss: 0.000923312137337896\n",
      "Training loss: 0.0009228046225026304\n",
      "Training loss: 0.000922297546529454\n",
      "Training loss: 0.0009217909089069122\n",
      "Training loss: 0.0009212847091243047\n",
      "Training loss: 0.000920778946671681\n",
      "Training loss: 0.0009202736210398556\n",
      "Training loss: 0.0009197687317203816\n",
      "Training loss: 0.0009192642782055651\n",
      "Training loss: 0.0009187602599884609\n",
      "Training loss: 0.0009182566765628772\n",
      "Training loss: 0.0009177535274233582\n",
      "Training loss: 0.0009172508120651991\n",
      "Training loss: 0.0009167485299844314\n",
      "Training loss: 0.0009162466806778386\n",
      "Training loss: 0.000915745263642934\n",
      "Training loss: 0.0009152442783779794\n",
      "Training loss: 0.0009147437243819681\n",
      "Training loss: 0.0009142436011546325\n",
      "Training loss: 0.0009137439081964368\n",
      "Training loss: 0.0009132446450085877\n",
      "Training loss: 0.0009127458110930152\n",
      "Training loss: 0.0009122474059523834\n",
      "Training loss: 0.0009117494290900887\n",
      "Training loss: 0.0009112518800102544\n",
      "Training loss: 0.0009107547582177334\n",
      "Training loss: 0.0009102580632180995\n",
      "Training loss: 0.0009097617945176579\n",
      "Training loss: 0.0009092659516234322\n",
      "Training loss: 0.0009087705340431679\n",
      "Training loss: 0.0009082755412853377\n",
      "Training loss: 0.0009077809728591303\n",
      "Training loss: 0.0009072868282744479\n",
      "Training loss: 0.000906793107041921\n",
      "Training loss: 0.0009062998086728845\n",
      "Training loss: 0.0009058069326793952\n",
      "Training loss: 0.0009053144785742202\n",
      "Training loss: 0.0009048224458708387\n",
      "Training loss: 0.0009043308340834434\n",
      "Training loss: 0.0009038396427269337\n",
      "Training loss: 0.0009033488713169191\n",
      "Training loss: 0.0009028585193697132\n",
      "Training loss: 0.0009023685864023402\n",
      "Training loss: 0.0009018790719325228\n",
      "Training loss: 0.0009013899754786939\n",
      "Training loss: 0.0009009012965599837\n",
      "Training loss: 0.000900413034696223\n",
      "Training loss: 0.0008999251894079494\n",
      "Training loss: 0.0008994377602163883\n",
      "Training loss: 0.000898950746643468\n",
      "Training loss: 0.0008984641482118146\n",
      "Training loss: 0.0008979779644447425\n",
      "Training loss: 0.0008974921948662662\n",
      "Training loss: 0.0008970068390010909\n",
      "Training loss: 0.000896521896374606\n",
      "Training loss: 0.0008960373665129018\n",
      "Training loss: 0.0008955532489427472\n",
      "Training loss: 0.0008950695431916049\n",
      "Training loss: 0.0008945862487876227\n",
      "Training loss: 0.00089410336525963\n",
      "Training loss: 0.0008936208921371479\n",
      "Training loss: 0.0008931388289503683\n",
      "Training loss: 0.0008926571752301744\n",
      "Training loss: 0.0008921759305081231\n",
      "Training loss: 0.0008916950943164562\n",
      "Training loss: 0.0008912146661880906\n",
      "Training loss: 0.0008907346456566163\n",
      "Training loss: 0.0008902550322563029\n",
      "Training loss: 0.0008897758255220929\n",
      "Training loss: 0.0008892970249896008\n",
      "Training loss: 0.0008888186301951195\n",
      "Training loss: 0.0008883406406756027\n",
      "Training loss: 0.0008878630559686812\n",
      "Training loss: 0.0008873858756126468\n",
      "Training loss: 0.0008869090991464665\n",
      "Training loss: 0.0008864327261097711\n",
      "Training loss: 0.0008859567560428491\n",
      "Training loss: 0.0008854811884866659\n",
      "Training loss: 0.0008850060229828367\n",
      "Training loss: 0.0008845312590736441\n",
      "Training loss: 0.0008840568963020321\n",
      "Training loss: 0.0008835829342116009\n",
      "Training loss: 0.0008831093723466093\n",
      "Training loss: 0.0008826362102519718\n",
      "Training loss: 0.0008821634474732626\n",
      "Training loss: 0.0008816910835567046\n",
      "Training loss: 0.0008812191180491768\n",
      "Training loss: 0.0008807475504982125\n",
      "Training loss: 0.0008802763804519956\n",
      "Training loss: 0.0008798056074593566\n",
      "Training loss: 0.0008793352310697743\n",
      "Training loss: 0.000878865250833381\n",
      "Training loss: 0.0008783956663009518\n",
      "Training loss: 0.0008779264770239079\n",
      "Training loss: 0.0008774576825543122\n",
      "Training loss: 0.0008769892824448733\n",
      "Training loss: 0.0008765212762489425\n",
      "Training loss: 0.0008760536635205101\n",
      "Training loss: 0.0008755864438142076\n",
      "Training loss: 0.0008751196166853025\n",
      "Training loss: 0.0008746531816897051\n",
      "Training loss: 0.0008741871383839612\n",
      "Training loss: 0.0008737214863252467\n",
      "Training loss: 0.0008732562250713747\n",
      "Training loss: 0.0008727913541807927\n",
      "Training loss: 0.0008723268732125782\n",
      "Training loss: 0.0008718627817264451\n",
      "Training loss: 0.0008713990792827311\n",
      "Training loss: 0.0008709357654424013\n",
      "Training loss: 0.0008704728397670566\n",
      "Training loss: 0.0008700103018189156\n",
      "Training loss: 0.0008695481511608271\n",
      "Training loss: 0.0008690863873562701\n",
      "Training loss: 0.0008686250099693324\n",
      "Training loss: 0.0008681640185647367\n",
      "Training loss: 0.0008677034127078238\n",
      "Training loss: 0.0008672431919645536\n",
      "Training loss: 0.0008667833559015027\n",
      "Training loss: 0.0008663239040858698\n",
      "Training loss: 0.0008658648360854702\n",
      "Training loss: 0.00086540615146873\n",
      "Training loss: 0.000864947849804696\n",
      "Training loss: 0.000864489930663028\n",
      "Training loss: 0.0008640323936139978\n",
      "Training loss: 0.0008635752382284838\n",
      "Training loss: 0.0008631184640779829\n",
      "Training loss: 0.000862662070734599\n",
      "Training loss: 0.0008622060577710436\n",
      "Training loss: 0.0008617504247606335\n",
      "Training loss: 0.000861295171277293\n",
      "Training loss: 0.000860840296895555\n",
      "Training loss: 0.0008603858011905567\n",
      "Training loss: 0.0008599316837380334\n",
      "Training loss: 0.0008594779441143286\n",
      "Training loss: 0.0008590245818963797\n",
      "Training loss: 0.0008585715966617307\n",
      "Training loss: 0.0008581189879885228\n",
      "Training loss: 0.0008576667554554981\n",
      "Training loss: 0.0008572148986419885\n",
      "Training loss: 0.0008567634171279304\n",
      "Training loss: 0.0008563123104938475\n",
      "Training loss: 0.0008558615783208623\n",
      "Training loss: 0.0008554112201906898\n",
      "Training loss: 0.0008549612356856371\n",
      "Training loss: 0.0008545116243886036\n",
      "Training loss: 0.0008540623858830741\n",
      "Training loss: 0.0008536135197531237\n",
      "Training loss: 0.0008531650255834207\n",
      "Training loss: 0.0008527169029592137\n",
      "Training loss: 0.0008522691514663381\n",
      "Training loss: 0.0008518217706912219\n",
      "Training loss: 0.0008513747602208645\n",
      "Training loss: 0.0008509281196428618\n",
      "Training loss: 0.0008504818485453793\n",
      "Training loss: 0.0008500359465171695\n",
      "Training loss: 0.0008495904131475656\n",
      "Training loss: 0.0008491452480264784\n",
      "Training loss: 0.0008487004507443976\n",
      "Training loss: 0.0008482560208923881\n",
      "Training loss: 0.0008478119580620913\n",
      "Training loss: 0.0008473682618457232\n",
      "Training loss: 0.000846924931836078\n",
      "Training loss: 0.0008464819676265138\n",
      "Training loss: 0.0008460393688109741\n",
      "Training loss: 0.0008455971349839616\n",
      "Training loss: 0.000845155265740551\n",
      "Training loss: 0.0008447137606763942\n",
      "Training loss: 0.0008442726193877019\n",
      "Training loss: 0.0008438318414712595\n",
      "Training loss: 0.0008433914265244133\n",
      "Training loss: 0.000842951374145075\n",
      "Training loss: 0.0008425116839317229\n",
      "Training loss: 0.0008420723554834024\n",
      "Training loss: 0.000841633388399712\n",
      "Training loss: 0.0008411947822808193\n",
      "Training loss: 0.0008407565367274516\n",
      "Training loss: 0.00084031865134089\n",
      "Training loss: 0.0008398811257229822\n",
      "Training loss: 0.0008394439594761241\n",
      "Training loss: 0.0008390071522032804\n",
      "Training loss: 0.0008385707035079641\n",
      "Training loss: 0.000838134612994241\n",
      "Training loss: 0.0008376988802667359\n",
      "Training loss: 0.0008372635049306243\n",
      "Training loss: 0.0008368284865916325\n",
      "Training loss: 0.0008363938248560385\n",
      "Training loss: 0.0008359595193306763\n",
      "Training loss: 0.0008355255696229185\n",
      "Training loss: 0.0008350919753406954\n",
      "Training loss: 0.0008346587360924776\n",
      "Training loss: 0.0008342258514872892\n",
      "Training loss: 0.0008337933211346939\n",
      "Training loss: 0.0008333611446448059\n",
      "Training loss: 0.0008329293216282742\n",
      "Training loss: 0.0008324978516962977\n",
      "Training loss: 0.0008320667344606146\n",
      "Training loss: 0.0008316359695335054\n",
      "Training loss: 0.0008312055565277898\n",
      "Training loss: 0.0008307754950568301\n",
      "Training loss: 0.0008303457847345168\n",
      "Training loss: 0.000829916425175289\n",
      "Training loss: 0.0008294874159941158\n",
      "Training loss: 0.0008290587568065038\n",
      "Training loss: 0.0008286304472284923\n",
      "Training loss: 0.0008282024868766596\n",
      "Training loss: 0.0008277748753681091\n",
      "Training loss: 0.0008273476123204822\n",
      "Training loss: 0.0008269206973519495\n",
      "Training loss: 0.0008264941300812098\n",
      "Training loss: 0.0008260679101274927\n",
      "Training loss: 0.0008256420371105606\n",
      "Training loss: 0.0008252165106506911\n",
      "Training loss: 0.0008247913303687015\n",
      "Training loss: 0.0008243664958859271\n",
      "Training loss: 0.0008239420068242344\n",
      "Training loss: 0.0008235178628060035\n",
      "Training loss: 0.0008230940634541502\n",
      "Training loss: 0.0008226706083920987\n",
      "Training loss: 0.0008222474972438063\n",
      "Training loss: 0.0008218247296337441\n",
      "Training loss: 0.0008214023051869074\n",
      "Training loss: 0.0008209802235288055\n",
      "Training loss: 0.000820558484285466\n",
      "Training loss: 0.0008201370870834371\n",
      "Training loss: 0.0008197160315497836\n",
      "Training loss: 0.000819295317312078\n",
      "Training loss: 0.0008188749439984152\n",
      "Training loss: 0.0008184549112373967\n",
      "Training loss: 0.0008180352186581434\n",
      "Training loss: 0.0008176158658902828\n",
      "Training loss: 0.0008171968525639581\n",
      "Training loss: 0.000816778178309816\n",
      "Training loss: 0.0008163598427590198\n",
      "Training loss: 0.0008159418455432313\n",
      "Training loss: 0.0008155241862946335\n",
      "Training loss: 0.0008151068646459034\n",
      "Training loss: 0.0008146898802302324\n",
      "Training loss: 0.0008142732326813125\n",
      "Training loss: 0.0008138569216333393\n",
      "Training loss: 0.0008134409467210121\n",
      "Training loss: 0.0008130253075795393\n",
      "Training loss: 0.0008126100038446175\n",
      "Training loss: 0.0008121950351524565\n",
      "Training loss: 0.0008117804011397614\n",
      "Training loss: 0.0008113661014437336\n",
      "Training loss: 0.0008109521357020803\n",
      "Training loss: 0.0008105385035529955\n",
      "Training loss: 0.0008101252046351823\n",
      "Training loss: 0.0008097122385878263\n",
      "Training loss: 0.0008092996050506215\n",
      "Training loss: 0.0008088873036637448\n",
      "Training loss: 0.0008084753340678731\n",
      "Training loss: 0.000808063695904172\n",
      "Training loss: 0.0008076523888143016\n",
      "Training loss: 0.0008072414124404128\n",
      "Training loss: 0.0008068307664251441\n",
      "Training loss: 0.0008064204504116247\n",
      "Training loss: 0.0008060104640434728\n",
      "Training loss: 0.0008056008069647938\n",
      "Training loss: 0.0008051914788201757\n",
      "Training loss: 0.0008047824792547039\n",
      "Training loss: 0.0008043738079139378\n",
      "Training loss: 0.0008039654644439264\n",
      "Training loss: 0.0008035574484912\n",
      "Training loss: 0.0008031497597027718\n",
      "Training loss: 0.0008027423977261383\n",
      "Training loss: 0.0008023353622092811\n",
      "Training loss: 0.000801928652800652\n",
      "Training loss: 0.0008015222691491928\n",
      "Training loss: 0.0008011162109043172\n",
      "Training loss: 0.0008007104777159219\n",
      "Training loss: 0.0008003050692343759\n",
      "Training loss: 0.0007998999851105283\n",
      "Training loss: 0.0007994952249957052\n",
      "Training loss: 0.0007990907885417021\n",
      "Training loss: 0.0007986866754007917\n",
      "Training loss: 0.0007982828852257204\n",
      "Training loss: 0.0007978794176697082\n",
      "Training loss: 0.0007974762723864451\n",
      "Training loss: 0.0007970734490300927\n",
      "Training loss: 0.0007966709472552798\n",
      "Training loss: 0.0007962687667171099\n",
      "Training loss: 0.0007958669070711529\n",
      "Training loss: 0.0007954653679734436\n",
      "Training loss: 0.0007950641490804912\n",
      "Training loss: 0.0007946632500492625\n",
      "Training loss: 0.0007942626705372002\n",
      "Training loss: 0.0007938624102021975\n",
      "Training loss: 0.0007934624687026254\n",
      "Training loss: 0.000793062845697312\n",
      "Training loss: 0.0007926635408455495\n",
      "Training loss: 0.000792264553807087\n",
      "Training loss: 0.0007918658842421435\n",
      "Training loss: 0.0007914675318113922\n",
      "Training loss: 0.0007910694961759634\n",
      "Training loss: 0.0007906717769974558\n",
      "Training loss: 0.0007902743739379177\n",
      "Training loss: 0.0007898772866598565\n",
      "Training loss: 0.000789480514826235\n",
      "Training loss: 0.0007890840581004768\n",
      "Training loss: 0.0007886879161464583\n",
      "Training loss: 0.0007882920886285046\n",
      "Training loss: 0.0007878965752114001\n",
      "Training loss: 0.0007875013755603838\n",
      "Training loss: 0.0007871064893411431\n",
      "Training loss: 0.0007867119162198102\n",
      "Training loss: 0.0007863176558629852\n",
      "Training loss: 0.0007859237079376995\n",
      "Training loss: 0.0007855300721114472\n",
      "Training loss: 0.0007851367480521627\n",
      "Training loss: 0.0007847437354282318\n",
      "Training loss: 0.0007843510339084881\n",
      "Training loss: 0.0007839586431622047\n",
      "Training loss: 0.0007835665628591099\n",
      "Training loss: 0.0007831747926693696\n",
      "Training loss: 0.0007827833322635944\n",
      "Training loss: 0.0007823921813128391\n",
      "Training loss: 0.0007820013394886061\n",
      "Training loss: 0.0007816108064628288\n",
      "Training loss: 0.0007812205819078906\n",
      "Training loss: 0.0007808306654966148\n",
      "Training loss: 0.0007804410569022579\n",
      "Training loss: 0.0007800517557985233\n",
      "Training loss: 0.0007796627618595497\n",
      "Training loss: 0.0007792740747599089\n",
      "Training loss: 0.000778885694174615\n",
      "Training loss: 0.0007784976197791135\n",
      "Training loss: 0.0007781098512492953\n",
      "Training loss: 0.0007777223882614744\n",
      "Training loss: 0.0007773352304924048\n",
      "Training loss: 0.0007769483776192686\n",
      "Training loss: 0.000776561829319686\n",
      "Training loss: 0.0007761755852717095\n",
      "Training loss: 0.0007757896451538195\n",
      "Training loss: 0.0007754040086449274\n",
      "Training loss: 0.0007750186754243726\n",
      "Training loss: 0.0007746336451719321\n",
      "Training loss: 0.0007742489175677968\n",
      "Training loss: 0.0007738644922926012\n",
      "Training loss: 0.0007734803690273933\n",
      "Training loss: 0.0007730965474536542\n",
      "Training loss: 0.0007727130272532931\n",
      "Training loss: 0.000772329808108638\n",
      "Training loss: 0.0007719468897024442\n",
      "Training loss: 0.000771564271717891\n",
      "Training loss: 0.0007711819538385793\n",
      "Training loss: 0.0007707999357485283\n",
      "Training loss: 0.0007704182171321898\n",
      "Training loss: 0.0007700367976744286\n",
      "Training loss: 0.0007696556770605272\n",
      "Training loss: 0.0007692748549761924\n",
      "Training loss: 0.0007688943311075506\n",
      "Training loss: 0.0007685141051411408\n",
      "Training loss: 0.0007681341767639236\n",
      "Training loss: 0.0007677545456632765\n",
      "Training loss: 0.000767375211526995\n",
      "Training loss: 0.0007669961740432794\n",
      "Training loss: 0.0007666174329007601\n",
      "Training loss: 0.0007662389877884716\n",
      "Training loss: 0.0007658608383958623\n",
      "Training loss: 0.0007654829844127987\n",
      "Training loss: 0.0007651054255295541\n",
      "Training loss: 0.0007647281614368135\n",
      "Training loss: 0.0007643511918256757\n",
      "Training loss: 0.000763974516387649\n",
      "Training loss: 0.0007635981348146511\n",
      "Training loss: 0.000763222046799006\n",
      "Training loss: 0.0007628462520334464\n",
      "Training loss: 0.0007624707502111163\n",
      "Training loss: 0.0007620955410255614\n",
      "Training loss: 0.0007617206241707375\n",
      "Training loss: 0.0007613459993410035\n",
      "Training loss: 0.0007609716662311243\n",
      "Training loss: 0.0007605976245362711\n",
      "Training loss: 0.0007602238739520149\n",
      "Training loss: 0.000759850414174328\n",
      "Training loss: 0.0007594772448995923\n",
      "Training loss: 0.0007591043658245841\n",
      "Training loss: 0.0007587317766464844\n",
      "Training loss: 0.0007583594770628754\n",
      "Training loss: 0.0007579874667717356\n",
      "Training loss: 0.0007576157454714461\n",
      "Training loss: 0.0007572443128607847\n",
      "Training loss: 0.0007568731686389265\n",
      "Training loss: 0.0007565023125054417\n",
      "Training loss: 0.0007561317441603004\n",
      "Training loss: 0.0007557614633038694\n",
      "Training loss: 0.0007553914696369081\n",
      "Training loss: 0.0007550217628605715\n",
      "Training loss: 0.0007546523426764104\n",
      "Training loss: 0.0007542832087863651\n",
      "Training loss: 0.0007539143608927735\n",
      "Training loss: 0.0007535457986983579\n",
      "Training loss: 0.0007531775219062418\n",
      "Training loss: 0.0007528095302199309\n",
      "Training loss: 0.0007524418233433265\n",
      "Training loss: 0.0007520744009807167\n",
      "Training loss: 0.0007517072628367814\n",
      "Training loss: 0.0007513404086165888\n",
      "Training loss: 0.0007509738380255895\n",
      "Training loss: 0.0007506075507696238\n",
      "Training loss: 0.0007502415465549267\n",
      "Training loss: 0.0007498758250881059\n",
      "Training loss: 0.0007495103860761669\n",
      "Training loss: 0.0007491452292264871\n",
      "Training loss: 0.0007487803542468351\n",
      "Training loss: 0.0007484157608453687\n",
      "Training loss: 0.0007480514487306172\n",
      "Training loss: 0.0007476874176114968\n",
      "Training loss: 0.0007473236671973061\n",
      "Training loss: 0.0007469601971977269\n",
      "Training loss: 0.0007465970073228155\n",
      "Training loss: 0.0007462340972830137\n",
      "Training loss: 0.0007458714667891401\n",
      "Training loss: 0.0007455091155523889\n",
      "Training loss: 0.0007451470432843379\n",
      "Training loss: 0.0007447852496969409\n",
      "Training loss: 0.0007444237345025254\n",
      "Training loss: 0.0007440624974137972\n",
      "Training loss: 0.0007437015381438375\n",
      "Training loss: 0.0007433408564061015\n",
      "Training loss: 0.0007429804519144239\n",
      "Training loss: 0.0007426203243830065\n",
      "Training loss: 0.0007422604735264263\n",
      "Training loss: 0.0007419008990596351\n",
      "Training loss: 0.0007415416006979555\n",
      "Training loss: 0.0007411825781570818\n",
      "Training loss: 0.0007408238311530766\n",
      "Training loss: 0.0007404653594023789\n",
      "Training loss: 0.000740107162621791\n",
      "Training loss: 0.0007397492405284895\n",
      "Training loss: 0.0007393915928400122\n",
      "Training loss: 0.0007390342192742743\n",
      "Training loss: 0.0007386771195495525\n",
      "Training loss: 0.0007383202933844904\n",
      "Training loss: 0.0007379637404981028\n",
      "Training loss: 0.000737607460609762\n",
      "Training loss: 0.0007372514534392107\n",
      "Training loss: 0.0007368957187065564\n",
      "Training loss: 0.0007365402561322695\n",
      "Training loss: 0.0007361850654371824\n",
      "Training loss: 0.0007358301463424924\n",
      "Training loss: 0.0007354754985697576\n",
      "Training loss: 0.0007351211218408988\n",
      "Training loss: 0.0007347670158781955\n",
      "Training loss: 0.0007344131804042899\n",
      "Training loss: 0.0007340596151421866\n",
      "Training loss: 0.000733706319815244\n",
      "Training loss: 0.0007333532941471823\n",
      "Training loss: 0.0007330005378620804\n",
      "Training loss: 0.0007326480506843747\n",
      "Training loss: 0.0007322958323388573\n",
      "Training loss: 0.0007319438825506788\n",
      "Training loss: 0.0007315922010453437\n",
      "Training loss: 0.0007312407875487118\n",
      "Training loss: 0.0007308896417870029\n",
      "Training loss: 0.0007305387634867832\n",
      "Training loss: 0.0007301881523749793\n",
      "Training loss: 0.0007298378081788652\n",
      "Training loss: 0.0007294877306260741\n",
      "Training loss: 0.0007291379194445861\n",
      "Training loss: 0.0007287883743627373\n",
      "Training loss: 0.000728439095109208\n",
      "Training loss: 0.0007280900814130386\n",
      "Training loss: 0.0007277413330036102\n",
      "Training loss: 0.0007273928496106554\n",
      "Training loss: 0.0007270446309642621\n",
      "Training loss: 0.000726696676794861\n",
      "Training loss: 0.0007263489868332278\n",
      "Training loss: 0.0007260015608104921\n",
      "Training loss: 0.0007256543984581238\n",
      "Training loss: 0.000725307499507946\n",
      "Training loss: 0.0007249608636921216\n",
      "Training loss: 0.0007246144907431583\n",
      "Training loss: 0.0007242683803939126\n",
      "Training loss: 0.0007239225323775793\n",
      "Training loss: 0.0007235769464277046\n",
      "Training loss: 0.0007232316222781709\n",
      "Training loss: 0.0007228865596632046\n",
      "Training loss: 0.0007225417583173753\n",
      "Training loss: 0.0007221972179755901\n",
      "Training loss: 0.0007218529383731039\n",
      "Training loss: 0.0007215089192455048\n",
      "Training loss: 0.000721165160328725\n",
      "Training loss: 0.0007208216613590304\n",
      "Training loss: 0.0007204784220730318\n",
      "Training loss: 0.0007201354422076782\n",
      "Training loss: 0.0007197927215002478\n",
      "Training loss: 0.0007194502596883665\n",
      "Training loss: 0.0007191080565099908\n",
      "Training loss: 0.0007187661117034111\n",
      "Training loss: 0.0007184244250072601\n",
      "Training loss: 0.000718082996160501\n",
      "Training loss: 0.0007177418249024319\n",
      "Training loss: 0.0007174009109726846\n",
      "Training loss: 0.0007170602541112267\n",
      "Training loss: 0.0007167198540583553\n",
      "Training loss: 0.0007163797105547002\n",
      "Training loss: 0.0007160398233412289\n",
      "Training loss: 0.0007157001921592325\n",
      "Training loss: 0.0007153608167503358\n",
      "Training loss: 0.0007150216968564942\n",
      "Training loss: 0.0007146828322199931\n",
      "Training loss: 0.0007143442225834483\n",
      "Training loss: 0.0007140058676898017\n",
      "Training loss: 0.0007136677672823241\n",
      "Training loss: 0.0007133299211046175\n",
      "Training loss: 0.0007129923289006066\n",
      "Training loss: 0.0007126549904145434\n",
      "Training loss: 0.0007123179053910112\n",
      "Training loss: 0.0007119810735749126\n",
      "Training loss: 0.0007116444947114775\n",
      "Training loss: 0.0007113081685462658\n",
      "Training loss: 0.0007109720948251523\n",
      "Training loss: 0.0007106362732943436\n",
      "Training loss: 0.0007103007037003654\n",
      "Training loss: 0.0007099653857900644\n",
      "Training loss: 0.0007096303193106164\n",
      "Training loss: 0.000709295504009512\n",
      "Training loss: 0.000708960939634565\n",
      "Training loss: 0.0007086266259339127\n",
      "Training loss: 0.000708292562656011\n",
      "Training loss: 0.0007079587495496351\n",
      "Training loss: 0.0007076251863638757\n",
      "Training loss: 0.0007072918728481515\n",
      "Training loss: 0.0007069588087521904\n",
      "Training loss: 0.0007066259938260461\n",
      "Training loss: 0.0007062934278200788\n",
      "Training loss: 0.0007059611104849781\n",
      "Training loss: 0.0007056290415717433\n",
      "Training loss: 0.0007052972208316894\n",
      "Training loss: 0.0007049656480164469\n",
      "Training loss: 0.0007046343228779642\n",
      "Training loss: 0.0007043032451684996\n",
      "Training loss: 0.0007039724146406286\n",
      "Training loss: 0.0007036418310472385\n",
      "Training loss: 0.000703311494141528\n",
      "Training loss: 0.0007029814036770138\n",
      "Training loss: 0.0007026515594075207\n",
      "Training loss: 0.000702321961087183\n",
      "Training loss: 0.0007019926084704515\n",
      "Training loss: 0.0007016635013120834\n",
      "Training loss: 0.0007013346393671465\n",
      "Training loss: 0.0007010060223910188\n",
      "Training loss: 0.0007006776501393902\n",
      "Training loss: 0.0007003495223682557\n",
      "Training loss: 0.0007000216388339169\n",
      "Training loss: 0.0006996939992929886\n",
      "Training loss: 0.0006993666035023896\n",
      "Training loss: 0.0006990394512193432\n",
      "Training loss: 0.0006987125422013845\n",
      "Training loss: 0.0006983858762063501\n",
      "Training loss: 0.0006980594529923846\n",
      "Training loss: 0.000697733272317936\n",
      "Training loss: 0.0006974073339417555\n",
      "Training loss: 0.0006970816376229025\n",
      "Training loss: 0.0006967561831207342\n",
      "Training loss: 0.0006964309701949168\n",
      "Training loss: 0.000696105998605415\n",
      "Training loss: 0.0006957812681124966\n",
      "Training loss: 0.000695456778476732\n",
      "Training loss: 0.0006951325294589922\n",
      "Training loss: 0.0006948085208204487\n",
      "Training loss: 0.0006944847523225731\n",
      "Training loss: 0.0006941612237271377\n",
      "Training loss: 0.0006938379347962154\n",
      "Training loss: 0.0006935148852921721\n",
      "Training loss: 0.0006931920749776775\n",
      "Training loss: 0.0006928695036157015\n",
      "Training loss: 0.000692547170969505\n",
      "Training loss: 0.0006922250768026493\n",
      "Training loss: 0.0006919032208789945\n",
      "Training loss: 0.0006915816029626956\n",
      "Training loss: 0.0006912602228181992\n",
      "Training loss: 0.0006909390802102508\n",
      "Training loss: 0.0006906181749038928\n",
      "Training loss: 0.0006902975066644593\n",
      "Training loss: 0.0006899770752575763\n",
      "Training loss: 0.0006896568804491691\n",
      "Training loss: 0.0006893369220054485\n",
      "Training loss: 0.0006890171996929258\n",
      "Training loss: 0.0006886977132783986\n",
      "Training loss: 0.0006883784625289611\n",
      "Training loss: 0.0006880594472119928\n",
      "Training loss: 0.0006877406670951715\n",
      "Training loss: 0.000687422121946458\n",
      "Training loss: 0.0006871038115341072\n",
      "Training loss: 0.000686785735626661\n",
      "Training loss: 0.0006864678939929572\n",
      "Training loss: 0.0006861502864021135\n",
      "Training loss: 0.0006858329126235381\n",
      "Training loss: 0.0006855157724269321\n",
      "Training loss: 0.0006851988655822823\n",
      "Training loss: 0.0006848821918598535\n",
      "Training loss: 0.0006845657510302053\n",
      "Training loss: 0.0006842495428641849\n",
      "Training loss: 0.0006839335671329232\n",
      "Training loss: 0.0006836178236078336\n",
      "Training loss: 0.0006833023120606131\n",
      "Training loss: 0.0006829870322632501\n",
      "Training loss: 0.0006826719839880118\n",
      "Training loss: 0.0006823571670074485\n",
      "Training loss: 0.0006820425810943932\n",
      "Training loss: 0.0006817282260219662\n",
      "Training loss: 0.0006814141015635655\n",
      "Training loss: 0.0006811002074928707\n",
      "Training loss: 0.0006807865435838467\n",
      "Training loss: 0.0006804731096107367\n",
      "Training loss: 0.000680159905348064\n",
      "Training loss: 0.0006798469305706323\n",
      "Training loss: 0.0006795341850535271\n",
      "Training loss: 0.0006792216685721067\n",
      "Training loss: 0.0006789093809020205\n",
      "Training loss: 0.0006785973218191816\n",
      "Training loss: 0.0006782854910997918\n",
      "Training loss: 0.0006779738885203245\n",
      "Training loss: 0.0006776625138575334\n",
      "Training loss: 0.0006773513668884454\n",
      "Training loss: 0.0006770404473903692\n",
      "Training loss: 0.0006767297551408866\n",
      "Training loss: 0.0006764192899178553\n",
      "Training loss: 0.0006761090514994056\n",
      "Training loss: 0.0006757990396639442\n",
      "Training loss: 0.0006754892541901549\n",
      "Training loss: 0.0006751796948569894\n",
      "Training loss: 0.0006748703614436778\n",
      "Training loss: 0.0006745612537297198\n",
      "Training loss: 0.000674252371494894\n",
      "Training loss: 0.0006739437145192405\n",
      "Training loss: 0.0006736352825830811\n",
      "Training loss: 0.0006733270754670024\n",
      "Training loss: 0.0006730190929518693\n",
      "Training loss: 0.0006727113348188079\n",
      "Training loss: 0.0006724038008492229\n",
      "Training loss: 0.0006720964908247827\n",
      "Training loss: 0.000671789404527428\n",
      "Training loss: 0.0006714825417393671\n",
      "Training loss: 0.0006711759022430804\n",
      "Training loss: 0.0006708694858213094\n",
      "Training loss: 0.0006705632922570712\n",
      "Training loss: 0.0006702573213336441\n",
      "Training loss: 0.0006699515728345794\n",
      "Training loss: 0.0006696460465436878\n",
      "Training loss: 0.0006693407422450504\n",
      "Training loss: 0.000669035659723016\n",
      "Training loss: 0.0006687307987621942\n",
      "Training loss: 0.0006684261591474651\n",
      "Training loss: 0.0006681217406639644\n",
      "Training loss: 0.0006678175430970992\n",
      "Training loss: 0.0006675135662325424\n",
      "Training loss: 0.0006672098098562223\n",
      "Training loss: 0.0006669062737543371\n",
      "Training loss: 0.0006666029577133427\n",
      "Training loss: 0.00066629986151996\n",
      "Training loss: 0.0006659969849611717\n",
      "Training loss: 0.0006656943278242211\n",
      "Training loss: 0.0006653918898966132\n",
      "Training loss: 0.0006650896709661146\n",
      "Training loss: 0.0006647876708207488\n",
      "Training loss: 0.0006644858892488037\n",
      "Training loss: 0.0006641843260388208\n",
      "Training loss: 0.0006638829809796056\n",
      "Training loss: 0.0006635818538602234\n",
      "Training loss: 0.0006632809444699916\n",
      "Training loss: 0.0006629802525984906\n",
      "Training loss: 0.0006626797780355605\n",
      "Training loss: 0.0006623795205712896\n",
      "Training loss: 0.0006620794799960309\n",
      "Training loss: 0.0006617796561003939\n",
      "Training loss: 0.0006614800486752403\n",
      "Training loss: 0.0006611806575116888\n",
      "Training loss: 0.0006608814824011131\n",
      "Training loss: 0.0006605825231351401\n",
      "Training loss: 0.0006602837795056577\n",
      "Training loss: 0.0006599852513048046\n",
      "Training loss: 0.0006596869383249666\n",
      "Training loss: 0.0006593888403587905\n",
      "Training loss: 0.0006590909571991797\n",
      "Training loss: 0.000658793288639278\n",
      "Training loss: 0.0006584958344724942\n",
      "Training loss: 0.0006581985944924783\n",
      "Training loss: 0.0006579015684931393\n",
      "Training loss: 0.0006576047562686343\n",
      "Training loss: 0.0006573081576133722\n",
      "Training loss: 0.0006570117723220116\n",
      "Training loss: 0.0006567156001894613\n",
      "Training loss: 0.0006564196410108811\n",
      "Training loss: 0.0006561238945816747\n",
      "Training loss: 0.0006558283606975041\n",
      "Training loss: 0.0006555330391542737\n",
      "Training loss: 0.0006552379297481366\n",
      "Training loss: 0.0006549430322754928\n",
      "Training loss: 0.0006546483465329935\n",
      "Training loss: 0.0006543538723175371\n",
      "Training loss: 0.0006540596094262608\n",
      "Training loss: 0.0006537655576565579\n",
      "Training loss: 0.000653471716806062\n",
      "Training loss: 0.000653178086672655\n",
      "Training loss: 0.000652884667054467\n",
      "Training loss: 0.0006525914577498638\n",
      "Training loss: 0.0006522984585574626\n",
      "Training loss: 0.0006520056692761251\n",
      "Training loss: 0.000651713089704954\n",
      "Training loss: 0.0006514207196432983\n",
      "Training loss: 0.0006511285588907492\n",
      "Training loss: 0.000650836607247137\n",
      "Training loss: 0.0006505448645125428\n",
      "Training loss: 0.0006502533304872793\n",
      "Training loss: 0.0006499620049719101\n",
      "Training loss: 0.0006496708877672332\n",
      "Training loss: 0.000649379978674295\n",
      "Training loss: 0.0006490892774943776\n",
      "Training loss: 0.0006487987840290008\n",
      "Training loss: 0.0006485084980799316\n",
      "Training loss: 0.000648218419449171\n",
      "Training loss: 0.0006479285479389602\n",
      "Training loss: 0.0006476388833517835\n",
      "Training loss: 0.0006473494254903578\n",
      "Training loss: 0.0006470601741576395\n",
      "Training loss: 0.0006467711291568274\n",
      "Training loss: 0.0006464822902913528\n",
      "Training loss: 0.0006461936573648906\n",
      "Training loss: 0.0006459052301813419\n",
      "Training loss: 0.0006456170085448544\n",
      "Training loss: 0.0006453289922598056\n",
      "Training loss: 0.0006450411811308133\n",
      "Training loss: 0.0006447535749627261\n",
      "Training loss: 0.0006444661735606314\n",
      "Training loss: 0.0006441789767298519\n",
      "Training loss: 0.0006438919842759391\n",
      "Training loss: 0.000643605196004685\n",
      "Training loss: 0.0006433186117221139\n",
      "Training loss: 0.0006430322312344776\n",
      "Training loss: 0.0006427460543482707\n",
      "Training loss: 0.0006424600808702153\n",
      "Training loss: 0.0006421743106072613\n",
      "Training loss: 0.0006418887433666001\n",
      "Training loss: 0.0006416033789556502\n",
      "Training loss: 0.0006413182171820589\n",
      "Training loss: 0.0006410332578537073\n",
      "Training loss: 0.000640748500778708\n",
      "Training loss: 0.0006404639457654013\n",
      "Training loss: 0.0006401795926223619\n",
      "Training loss: 0.0006398954411583894\n",
      "Training loss: 0.0006396114911825146\n",
      "Training loss: 0.0006393277425040007\n",
      "Training loss: 0.0006390441949323324\n",
      "Training loss: 0.0006387608482772267\n",
      "Training loss: 0.000638477702348628\n",
      "Training loss: 0.0006381947569567108\n",
      "Training loss: 0.0006379120119118791\n",
      "Training loss: 0.0006376294670247522\n",
      "Training loss: 0.000637347122106188\n",
      "Training loss: 0.0006370649769672681\n",
      "Training loss: 0.0006367830314192927\n",
      "Training loss: 0.0006365012852738008\n",
      "Training loss: 0.0006362197383425446\n",
      "Training loss: 0.0006359383904375094\n",
      "Training loss: 0.0006356572413709006\n",
      "Training loss: 0.0006353762909551482\n",
      "Training loss: 0.0006350955390029099\n",
      "Training loss: 0.000634814985327068\n",
      "Training loss: 0.0006345346297407201\n",
      "Training loss: 0.0006342544720571942\n",
      "Training loss: 0.000633974512090037\n",
      "Training loss: 0.0006336947496530224\n",
      "Training loss: 0.0006334151845601426\n",
      "Training loss: 0.0006331358166256123\n",
      "Training loss: 0.0006328566456638685\n",
      "Training loss: 0.000632577671489567\n",
      "Training loss: 0.0006322988939175897\n",
      "Training loss: 0.0006320203127630326\n",
      "Training loss: 0.0006317419278412177\n",
      "Training loss: 0.000631463738967683\n",
      "Training loss: 0.0006311857459581878\n",
      "Training loss: 0.00063090794862871\n",
      "Training loss: 0.0006306303467954461\n",
      "Training loss: 0.0006303529402748137\n",
      "Training loss: 0.0006300757288834459\n",
      "Training loss: 0.000629798712438194\n",
      "Training loss: 0.0006295218907561284\n",
      "Training loss: 0.0006292452636545363\n",
      "Training loss: 0.0006289688309509217\n",
      "Training loss: 0.0006286925924630052\n",
      "Training loss: 0.0006284165480087246\n",
      "Training loss: 0.0006281406974062338\n",
      "Training loss: 0.0006278650404739037\n",
      "Training loss: 0.0006275895770303183\n",
      "Training loss: 0.0006273143068942761\n",
      "Training loss: 0.0006270392298847925\n",
      "Training loss: 0.0006267643458210977\n",
      "Training loss: 0.0006264896545226375\n",
      "Training loss: 0.0006262151558090674\n",
      "Training loss: 0.0006259408495002583\n",
      "Training loss: 0.0006256667354162963\n",
      "Training loss: 0.0006253928133774793\n",
      "Training loss: 0.0006251190832043203\n",
      "Training loss: 0.000624845544717538\n",
      "Training loss: 0.0006245721977380706\n",
      "Training loss: 0.0006242990420870638\n",
      "Training loss: 0.000624026077585876\n",
      "Training loss: 0.0006237533040560799\n",
      "Training loss: 0.0006234807213194527\n",
      "Training loss: 0.0006232083291979885\n",
      "Training loss: 0.0006229361275138868\n",
      "Training loss: 0.0006226641160895584\n",
      "Training loss: 0.0006223922947476286\n",
      "Training loss: 0.0006221206633109261\n",
      "Training loss: 0.0006218492216024901\n",
      "Training loss: 0.0006215779694455696\n",
      "Training loss: 0.0006213069066636258\n",
      "Training loss: 0.0006210360330803183\n",
      "Training loss: 0.0006207653485195247\n",
      "Training loss: 0.0006204948528053247\n",
      "Training loss: 0.0006202245457620051\n",
      "Training loss: 0.0006199544272140645\n",
      "Training loss: 0.0006196844969862031\n",
      "Training loss: 0.0006194147549033281\n",
      "Training loss: 0.0006191452007905572\n",
      "Training loss: 0.0006188758344732086\n",
      "Training loss: 0.0006186066557768108\n",
      "Training loss: 0.0006183376645270948\n",
      "Training loss: 0.0006180688605499938\n",
      "Training loss: 0.000617800243671653\n",
      "Training loss: 0.0006175318137184165\n",
      "Training loss: 0.0006172635705168316\n",
      "Training loss: 0.0006169955138936558\n",
      "Training loss: 0.0006167276436758415\n",
      "Training loss: 0.0006164599596905518\n",
      "Training loss: 0.0006161924617651498\n",
      "Training loss: 0.0006159251497272006\n",
      "Training loss: 0.0006156580234044726\n",
      "Training loss: 0.0006153910826249359\n",
      "Training loss: 0.0006151243272167601\n",
      "Training loss: 0.0006148577570083243\n",
      "Training loss: 0.0006145913718281983\n",
      "Training loss: 0.0006143251715051578\n",
      "Training loss: 0.0006140591558681819\n",
      "Training loss: 0.0006137933247464443\n",
      "Training loss: 0.000613527677969323\n",
      "Training loss: 0.0006132622153663946\n",
      "Training loss: 0.0006129969367674341\n",
      "Training loss: 0.0006127318420024169\n",
      "Training loss: 0.0006124669309015167\n",
      "Training loss: 0.0006122022032951093\n",
      "Training loss: 0.0006119376590137641\n",
      "Training loss: 0.0006116732978882481\n",
      "Training loss: 0.0006114091197495303\n",
      "Training loss: 0.0006111451244287753\n",
      "Training loss: 0.0006108813117573439\n",
      "Training loss: 0.0006106176815667973\n",
      "Training loss: 0.0006103542336888881\n",
      "Training loss: 0.0006100909679555708\n",
      "Training loss: 0.0006098278841989918\n",
      "Training loss: 0.0006095649822514974\n",
      "Training loss: 0.0006093022619456243\n",
      "Training loss: 0.0006090397231141096\n",
      "Training loss: 0.0006087773655898798\n",
      "Training loss: 0.0006085151892060629\n",
      "Training loss: 0.0006082531937959765\n",
      "Training loss: 0.000607991379193135\n",
      "Training loss: 0.0006077297452312442\n",
      "Training loss: 0.0006074682917442061\n",
      "Training loss: 0.0006072070185661134\n",
      "Training loss: 0.000606945925531259\n",
      "Training loss: 0.000606685012474115\n",
      "Training loss: 0.0006064242792293595\n",
      "Training loss: 0.0006061637256318575\n",
      "Training loss: 0.0006059033515166631\n",
      "Training loss: 0.0006056431567190261\n",
      "Training loss: 0.0006053831410743897\n",
      "Training loss: 0.0006051233044183843\n",
      "Training loss: 0.0006048636465868324\n",
      "Training loss: 0.000604604167415746\n",
      "Training loss: 0.0006043448667413286\n",
      "Training loss: 0.0006040857443999734\n",
      "Training loss: 0.0006038268002282684\n",
      "Training loss: 0.0006035680340629824\n",
      "Training loss: 0.0006033094457410794\n",
      "Training loss: 0.0006030510350997115\n",
      "Training loss: 0.0006027928019762188\n",
      "Training loss: 0.0006025347462081314\n",
      "Training loss: 0.0006022768676331649\n",
      "Training loss: 0.0006020191660892256\n",
      "Training loss: 0.0006017616414144059\n",
      "Training loss: 0.0006015042934469865\n",
      "Training loss: 0.0006012471220254363\n",
      "Training loss: 0.0006009901269884099\n",
      "Training loss: 0.0006007333081747481\n",
      "Training loss: 0.0006004766654234786\n",
      "Training loss: 0.0006002201985738152\n",
      "Training loss: 0.0005999639074651597\n",
      "Training loss: 0.0005997077919370963\n",
      "Training loss: 0.0005994518518293957\n",
      "Training loss: 0.0005991960869820163\n",
      "Training loss: 0.000598940497235095\n",
      "Training loss: 0.000598685082428962\n",
      "Training loss: 0.0005984298424041245\n",
      "Training loss: 0.000598174777001279\n",
      "Training loss: 0.0005979198860612993\n",
      "Training loss: 0.0005976651694252502\n",
      "Training loss: 0.0005974106269343761\n",
      "Training loss: 0.0005971562584301024\n",
      "Training loss: 0.000596902063754044\n",
      "Training loss: 0.000596648042747991\n",
      "Training loss: 0.0005963941952539216\n",
      "Training loss: 0.0005961405211139916\n",
      "Training loss: 0.0005958870201705425\n",
      "Training loss: 0.0005956336922660929\n",
      "Training loss: 0.0005953805372433464\n",
      "Training loss: 0.0005951275549451852\n",
      "Training loss: 0.000594874745214675\n",
      "Training loss: 0.0005946221078950602\n",
      "Training loss: 0.0005943696428297651\n",
      "Training loss: 0.000594117349862395\n",
      "Training loss: 0.0005938652288367353\n",
      "Training loss: 0.0005936132795967507\n",
      "Training loss: 0.0005933615019865811\n",
      "Training loss: 0.0005931098958505514\n",
      "Training loss: 0.0005928584610331649\n",
      "Training loss: 0.000592607197379098\n",
      "Training loss: 0.0005923561047332115\n",
      "Training loss: 0.0005921051829405415\n",
      "Training loss: 0.0005918544318463019\n",
      "Training loss: 0.0005916038512958823\n",
      "Training loss: 0.000591353441134853\n",
      "Training loss: 0.0005911032012089601\n",
      "Training loss: 0.0005908531313641246\n",
      "Training loss: 0.0005906032314464477\n",
      "Training loss: 0.0005903535013022046\n",
      "Training loss: 0.0005901039407778451\n",
      "Training loss: 0.0005898545497199986\n",
      "Training loss: 0.0005896053279754689\n",
      "Training loss: 0.000589356275391234\n",
      "Training loss: 0.0005891073918144429\n",
      "Training loss: 0.0005888586770924302\n",
      "Training loss: 0.0005886101310726947\n",
      "Training loss: 0.0005883617536029166\n",
      "Training loss: 0.0005881135445309437\n",
      "Training loss: 0.0005878655037048039\n",
      "Training loss: 0.0005876176309726937\n",
      "Training loss: 0.0005873699261829893\n",
      "Training loss: 0.0005871223891842321\n",
      "Training loss: 0.0005868750198251415\n",
      "Training loss: 0.0005866278179546111\n",
      "Training loss: 0.0005863807834216999\n",
      "Training loss: 0.0005861339160756462\n",
      "Training loss: 0.0005858872157658583\n",
      "Training loss: 0.000585640682341913\n",
      "Training loss: 0.000585394315653564\n",
      "Training loss: 0.000585148115550733\n",
      "Training loss: 0.0005849020818835112\n",
      "Training loss: 0.0005846562145021657\n",
      "Training loss: 0.0005844105132571272\n",
      "Training loss: 0.0005841649779990037\n",
      "Training loss: 0.0005839196085785667\n",
      "Training loss: 0.000583674404846765\n",
      "Training loss: 0.0005834293666547089\n",
      "Training loss: 0.0005831844938536822\n",
      "Training loss: 0.0005829397862951407\n",
      "Training loss: 0.0005826952438307036\n",
      "Training loss: 0.0005824508663121626\n",
      "Training loss: 0.0005822066535914769\n",
      "Training loss: 0.0005819626055207719\n",
      "Training loss: 0.0005817187219523433\n",
      "Training loss: 0.0005814750027386531\n",
      "Training loss: 0.000581231447732331\n",
      "Training loss: 0.0005809880567861778\n",
      "Training loss: 0.0005807448297531535\n",
      "Training loss: 0.0005805017664863919\n",
      "Training loss: 0.000580258866839192\n",
      "Training loss: 0.0005800161306650162\n",
      "Training loss: 0.0005797735578174957\n",
      "Training loss: 0.0005795311481504288\n",
      "Training loss: 0.0005792889015177742\n",
      "Training loss: 0.0005790468177736592\n",
      "Training loss: 0.0005788048967723794\n",
      "Training loss: 0.0005785631383683912\n",
      "Training loss: 0.0005783215424163161\n",
      "Training loss: 0.000578080108770943\n",
      "Training loss: 0.0005778388372872214\n",
      "Training loss: 0.0005775977278202671\n",
      "Training loss: 0.0005773567802253593\n",
      "Training loss: 0.0005771159943579417\n",
      "Training loss: 0.0005768753700736211\n",
      "Training loss: 0.000576634907228167\n",
      "Training loss: 0.0005763946056775121\n",
      "Training loss: 0.0005761544652777479\n",
      "Training loss: 0.0005759144858851372\n",
      "Training loss: 0.0005756746673560998\n",
      "Training loss: 0.0005754350095472123\n",
      "Training loss: 0.0005751955123152259\n",
      "Training loss: 0.0005749561755170416\n",
      "Training loss: 0.0005747169990097283\n",
      "Training loss: 0.0005744779826505139\n",
      "Training loss: 0.0005742391262967855\n",
      "Training loss: 0.0005740004298060967\n",
      "Training loss: 0.0005737618930361559\n",
      "Training loss: 0.0005735235158448374\n",
      "Training loss: 0.000573285298090166\n",
      "Training loss: 0.0005730472396303372\n",
      "Training loss: 0.0005728093403237007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0005725716000287655\n",
      "Training loss: 0.0005723340186042016\n",
      "Training loss: 0.0005720965959088391\n",
      "Training loss: 0.0005718593318016587\n",
      "Training loss: 0.000571622226141812\n",
      "Training loss: 0.0005713852787886006\n",
      "Training loss: 0.0005711484896014876\n",
      "Training loss: 0.000570911858440092\n",
      "Training loss: 0.0005706753851641918\n",
      "Training loss: 0.0005704390696337213\n",
      "Training loss: 0.0005702029117087752\n",
      "Training loss: 0.0005699669112496021\n",
      "Training loss: 0.0005697310681166056\n",
      "Training loss: 0.0005694953821703524\n",
      "Training loss: 0.0005692598532715614\n",
      "Training loss: 0.0005690244812811069\n",
      "Training loss: 0.0005687892660600209\n",
      "Training loss: 0.0005685542074694914\n",
      "Training loss: 0.000568319305370861\n",
      "Training loss: 0.0005680845596256304\n",
      "Training loss: 0.0005678499700954503\n",
      "Training loss: 0.0005676155366421293\n",
      "Training loss: 0.0005673812591276335\n",
      "Training loss: 0.0005671471374140765\n",
      "Training loss: 0.0005669131713637348\n",
      "Training loss: 0.0005666793608390306\n",
      "Training loss: 0.0005664457057025457\n",
      "Training loss: 0.0005662122058170156\n",
      "Training loss: 0.0005659788610453262\n",
      "Training loss: 0.0005657456712505169\n",
      "Training loss: 0.0005655126362957822\n",
      "Training loss: 0.0005652797560444705\n",
      "Training loss: 0.000565047030360078\n",
      "Training loss: 0.0005648144591062588\n",
      "Training loss: 0.0005645820421468144\n",
      "Training loss: 0.0005643497793456996\n",
      "Training loss: 0.0005641176705670269\n",
      "Training loss: 0.0005638857156750515\n",
      "Training loss: 0.0005636539145341862\n",
      "Training loss: 0.0005634222670089931\n",
      "Training loss: 0.0005631907729641853\n",
      "Training loss: 0.0005629594322646236\n",
      "Training loss: 0.0005627282447753249\n",
      "Training loss: 0.0005624972103614535\n",
      "Training loss: 0.0005622663288883232\n",
      "Training loss: 0.0005620356002214026\n",
      "Training loss: 0.0005618050242263007\n",
      "Training loss: 0.0005615746007687854\n",
      "Training loss: 0.0005613443297147709\n",
      "Training loss: 0.0005611142109303174\n",
      "Training loss: 0.0005608842442816394\n",
      "Training loss: 0.0005606544296350961\n",
      "Training loss: 0.0005604247668571983\n",
      "Training loss: 0.0005601952558146017\n",
      "Training loss: 0.0005599658963741115\n",
      "Training loss: 0.0005597366884026824\n",
      "Training loss: 0.000559507631767417\n",
      "Training loss: 0.0005592787263355625\n",
      "Training loss: 0.0005590499719745186\n",
      "Training loss: 0.0005588213685518257\n",
      "Training loss: 0.0005585929159351752\n",
      "Training loss: 0.0005583646139924041\n",
      "Training loss: 0.000558136462591497\n",
      "Training loss: 0.0005579084616005872\n",
      "Training loss: 0.0005576806108879468\n",
      "Training loss: 0.0005574529103220014\n",
      "Training loss: 0.000557225359771316\n",
      "Training loss: 0.0005569979591046093\n",
      "Training loss: 0.0005567707081907372\n",
      "Training loss: 0.0005565436068987045\n",
      "Training loss: 0.0005563166550976629\n",
      "Training loss: 0.0005560898526569063\n",
      "Training loss: 0.0005558631994458721\n",
      "Training loss: 0.0005556366953341458\n",
      "Training loss: 0.0005554103401914568\n",
      "Training loss: 0.0005551841338876708\n",
      "Training loss: 0.0005549580762928111\n",
      "Training loss: 0.0005547321672770326\n",
      "Training loss: 0.000554506406710639\n",
      "Training loss: 0.0005542807944640756\n",
      "Training loss: 0.000554055330407935\n",
      "Training loss: 0.0005538300144129449\n",
      "Training loss: 0.0005536048463499845\n",
      "Training loss: 0.0005533798260900673\n",
      "Training loss: 0.0005531549535043571\n",
      "Training loss: 0.0005529302284641512\n",
      "Training loss: 0.0005527056508408978\n",
      "Training loss: 0.0005524812205061804\n",
      "Training loss: 0.0005522569373317242\n",
      "Training loss: 0.0005520328011893994\n",
      "Training loss: 0.0005518088119512175\n",
      "Training loss: 0.000551584969489325\n",
      "Training loss: 0.0005513612736760179\n",
      "Training loss: 0.0005511377243837264\n",
      "Training loss: 0.000550914321485023\n",
      "Training loss: 0.0005506910648526214\n",
      "Training loss: 0.000550467954359372\n",
      "Training loss: 0.0005502449898782686\n",
      "Training loss: 0.0005500221712824433\n",
      "Training loss: 0.000549799498445172\n",
      "Training loss: 0.000549576971239861\n",
      "Training loss: 0.0005493545895400637\n",
      "Training loss: 0.0005491323532194682\n",
      "Training loss: 0.0005489102621518998\n",
      "Training loss: 0.0005486883162113281\n",
      "Training loss: 0.0005484665152718607\n",
      "Training loss: 0.000548244859207735\n",
      "Training loss: 0.0005480233478933377\n",
      "Training loss: 0.000547801981203183\n",
      "Training loss: 0.0005475807590119325\n",
      "Training loss: 0.0005473596811943766\n",
      "Training loss: 0.0005471387476254498\n",
      "Training loss: 0.0005469179581802187\n",
      "Training loss: 0.0005466973127338903\n",
      "Training loss: 0.0005464768111618053\n",
      "Training loss: 0.0005462564533394429\n",
      "Training loss: 0.0005460362391424195\n",
      "Training loss: 0.0005458161684464867\n",
      "Training loss: 0.0005455962411275294\n",
      "Training loss: 0.0005453764570615719\n",
      "Training loss: 0.0005451568161247738\n",
      "Training loss: 0.0005449373181934305\n",
      "Training loss: 0.0005447179631439679\n",
      "Training loss: 0.0005444987508529574\n",
      "Training loss: 0.0005442796811970921\n",
      "Training loss: 0.0005440607540532113\n",
      "Training loss: 0.0005438419692982818\n",
      "Training loss: 0.0005436233268094056\n",
      "Training loss: 0.0005434048264638252\n",
      "Training loss: 0.0005431864681389102\n",
      "Training loss: 0.0005429682517121624\n",
      "Training loss: 0.0005427501770612265\n",
      "Training loss: 0.000542532244063871\n",
      "Training loss: 0.0005423144525980068\n",
      "Training loss: 0.0005420968025416699\n",
      "Training loss: 0.0005418792937730341\n",
      "Training loss: 0.0005416619261704017\n",
      "Training loss: 0.0005414446996122143\n",
      "Training loss: 0.0005412276139770406\n",
      "Training loss: 0.0005410106691435852\n",
      "Training loss: 0.0005407938649906785\n",
      "Training loss: 0.0005405772013972895\n",
      "Training loss: 0.0005403606782425176\n",
      "Training loss: 0.0005401442954055913\n",
      "Training loss: 0.0005399280527658722\n",
      "Training loss: 0.0005397119502028529\n",
      "Training loss: 0.0005394959875961573\n",
      "Training loss: 0.0005392801648255406\n",
      "Training loss: 0.0005390644817708864\n",
      "Training loss: 0.0005388489383122118\n",
      "Training loss: 0.0005386335343296634\n",
      "Training loss: 0.0005384182697035166\n",
      "Training loss: 0.0005382031443141775\n",
      "Training loss: 0.0005379881580421846\n",
      "Training loss: 0.0005377733107682008\n",
      "Training loss: 0.0005375586023730224\n",
      "Training loss: 0.0005373440327375766\n",
      "Training loss: 0.0005371296017429163\n",
      "Training loss: 0.0005369153092702239\n",
      "Training loss: 0.0005367011552008112\n",
      "Training loss: 0.0005364871394161201\n",
      "Training loss: 0.000536273261797719\n",
      "Training loss: 0.0005360595222273049\n",
      "Training loss: 0.0005358459205867035\n",
      "Training loss: 0.0005356324567578693\n",
      "Training loss: 0.0005354191306228818\n",
      "Training loss: 0.000535205942063952\n",
      "Training loss: 0.0005349928909634177\n",
      "Training loss: 0.0005347799772037398\n",
      "Training loss: 0.0005345672006675096\n",
      "Training loss: 0.0005343545612374476\n",
      "Training loss: 0.0005341420587963945\n",
      "Training loss: 0.000533929693227328\n",
      "Training loss: 0.0005337174644133427\n",
      "Training loss: 0.0005335053722376618\n",
      "Training loss: 0.0005332934165836379\n",
      "Training loss: 0.0005330815973347454\n",
      "Training loss: 0.0005328699143745882\n",
      "Training loss: 0.0005326583675868959\n",
      "Training loss: 0.0005324469568555184\n",
      "Training loss: 0.0005322356820644368\n",
      "Training loss: 0.0005320245430977554\n",
      "Training loss: 0.0005318135398397008\n",
      "Training loss: 0.0005316026721746301\n",
      "Training loss: 0.00053139193998702\n",
      "Training loss: 0.0005311813431614752\n",
      "Training loss: 0.000530970881582722\n",
      "Training loss: 0.0005307605551356108\n",
      "Training loss: 0.0005305503637051194\n",
      "Training loss: 0.0005303403071763473\n",
      "Training loss: 0.0005301303854345173\n",
      "Training loss: 0.0005299205983649763\n",
      "Training loss: 0.0005297109458531951\n",
      "Training loss: 0.0005295014277847665\n",
      "Training loss: 0.0005292920440454059\n",
      "Training loss: 0.0005290827945209546\n",
      "Training loss: 0.000528873679097372\n",
      "Training loss: 0.0005286646976607476\n",
      "Training loss: 0.0005284558500972855\n",
      "Training loss: 0.0005282471362933144\n",
      "Training loss: 0.0005280385561352879\n",
      "Training loss: 0.0005278301095097788\n",
      "Training loss: 0.000527621796303481\n",
      "Training loss: 0.0005274136164032143\n",
      "Training loss: 0.0005272055696959143\n",
      "Training loss: 0.0005269976560686437\n",
      "Training loss: 0.0005267898754085803\n",
      "Training loss: 0.000526582227603028\n",
      "Training loss: 0.0005263747125394077\n",
      "Training loss: 0.0005261673301052651\n",
      "Training loss: 0.0005259600801882619\n",
      "Training loss: 0.0005257529626761825\n",
      "Training loss: 0.0005255459774569345\n",
      "Training loss: 0.0005253391244185389\n",
      "Training loss: 0.0005251324034491416\n",
      "Training loss: 0.0005249258144370063\n",
      "Training loss: 0.0005247193572705168\n",
      "Training loss: 0.0005245130318381771\n",
      "Training loss: 0.0005243068380286106\n",
      "Training loss: 0.0005241007757305575\n",
      "Training loss: 0.0005238948448328796\n",
      "Training loss: 0.0005236890452245555\n",
      "Training loss: 0.0005234833767946836\n",
      "Training loss: 0.0005232778394324803\n",
      "Training loss: 0.0005230724330272781\n",
      "Training loss: 0.0005228671574685344\n",
      "Training loss: 0.0005226620126458182\n",
      "Training loss: 0.0005224569984488197\n",
      "Training loss: 0.0005222521147673453\n",
      "Training loss: 0.0005220473614913192\n",
      "Training loss: 0.0005218427385107825\n",
      "Training loss: 0.0005216382457158959\n",
      "Training loss: 0.0005214338829969357\n",
      "Training loss: 0.000521229650244294\n",
      "Training loss: 0.0005210255473484817\n",
      "Training loss: 0.0005208215742001266\n",
      "Training loss: 0.0005206177306899705\n",
      "Training loss: 0.0005204140167088739\n",
      "Training loss: 0.0005202104321478129\n",
      "Training loss: 0.000520006976897878\n",
      "Training loss: 0.0005198036508502783\n",
      "Training loss: 0.0005196004538963383\n",
      "Training loss: 0.0005193973859274964\n",
      "Training loss: 0.0005191944468353094\n",
      "Training loss: 0.0005189916365114454\n",
      "Training loss: 0.000518788954847689\n",
      "Training loss: 0.0005185864017359442\n",
      "Training loss: 0.0005183839770682233\n",
      "Training loss: 0.0005181816807366567\n",
      "Training loss: 0.0005179795126334901\n",
      "Training loss: 0.0005177774726510812\n",
      "Training loss: 0.0005175755606819058\n",
      "Training loss: 0.00051737377661855\n",
      "Training loss: 0.0005171721203537145\n",
      "Training loss: 0.0005169705917802168\n",
      "Training loss: 0.0005167691907909848\n",
      "Training loss: 0.0005165679172790596\n",
      "Training loss: 0.0005163667711375993\n",
      "Training loss: 0.0005161657522598727\n",
      "Training loss: 0.0005159648605392636\n",
      "Training loss: 0.0005157640958692655\n",
      "Training loss: 0.0005155634581434887\n",
      "Training loss: 0.0005153629472556529\n",
      "Training loss: 0.0005151625630995916\n",
      "Training loss: 0.0005149623055692515\n",
      "Training loss: 0.0005147621745586898\n",
      "Training loss: 0.0005145621699620777\n",
      "Training loss: 0.0005143622916736984\n",
      "Training loss: 0.0005141625395879449\n",
      "Training loss: 0.0005139629135993228\n",
      "Training loss: 0.0005137634136024511\n",
      "Training loss: 0.0005135640394920552\n",
      "Training loss: 0.0005133647911629798\n",
      "Training loss: 0.0005131656685101704\n",
      "Training loss: 0.0005129666714286927\n",
      "Training loss: 0.0005127677998137218\n",
      "Training loss: 0.0005125690535605362\n",
      "Training loss: 0.0005123704325645319\n",
      "Training loss: 0.0005121719367212153\n",
      "Training loss: 0.0005119735659262007\n",
      "Training loss: 0.0005117753200752097\n",
      "Training loss: 0.0005115771990640812\n",
      "Training loss: 0.0005113792027887576\n",
      "Training loss: 0.0005111813311452936\n",
      "Training loss: 0.0005109835840298537\n",
      "Training loss: 0.0005107859613387129\n",
      "Training loss: 0.0005105884629682507\n",
      "Training loss: 0.0005103910888149642\n",
      "Training loss: 0.0005101938387754481\n",
      "Training loss: 0.000509996712746415\n",
      "Training loss: 0.0005097997106246835\n",
      "Training loss: 0.0005096028323071802\n",
      "Training loss: 0.0005094060776909393\n",
      "Training loss: 0.0005092094466731062\n",
      "Training loss: 0.0005090129391509324\n",
      "Training loss: 0.0005088165550217789\n",
      "Training loss: 0.0005086202941831108\n",
      "Training loss: 0.0005084241565325066\n",
      "Training loss: 0.000508228141967648\n",
      "Training loss: 0.0005080322503863238\n",
      "Training loss: 0.0005078364816864359\n",
      "Training loss: 0.0005076408357659874\n",
      "Training loss: 0.0005074453125230894\n",
      "Training loss: 0.0005072499118559639\n",
      "Training loss: 0.0005070546336629349\n",
      "Training loss: 0.0005068594778424364\n",
      "Training loss: 0.0005066644442930071\n",
      "Training loss: 0.0005064695329132917\n",
      "Training loss: 0.0005062747436020426\n",
      "Training loss: 0.0005060800762581184\n",
      "Training loss: 0.0005058855307804818\n",
      "Training loss: 0.0005056911070682044\n",
      "Training loss: 0.000505496805020462\n",
      "Training loss: 0.0005053026245365331\n",
      "Training loss: 0.0005051085655158059\n",
      "Training loss: 0.0005049146278577724\n",
      "Training loss: 0.000504720811462029\n",
      "Training loss: 0.0005045271162282766\n",
      "Training loss: 0.0005043335420563258\n",
      "Training loss: 0.0005041400888460842\n",
      "Training loss: 0.000503946756497571\n",
      "Training loss: 0.0005037535449109066\n",
      "Training loss: 0.0005035604539863157\n",
      "Training loss: 0.0005033674836241283\n",
      "Training loss: 0.0005031746337247791\n",
      "Training loss: 0.0005029819041888037\n",
      "Training loss: 0.0005027892949168438\n",
      "Training loss: 0.0005025968058096459\n",
      "Training loss: 0.00050240443676806\n",
      "Training loss: 0.0005022121876930348\n",
      "Training loss: 0.0005020200584856287\n",
      "Training loss: 0.0005018280490470009\n",
      "Training loss: 0.0005016361592784102\n",
      "Training loss: 0.0005014443890812257\n",
      "Training loss: 0.0005012527383569112\n",
      "Training loss: 0.0005010612070070377\n",
      "Training loss: 0.00050086979493328\n",
      "Training loss: 0.0005006785020374128\n",
      "Training loss: 0.0005004873282213122\n",
      "Training loss: 0.0005002962733869584\n",
      "Training loss: 0.0005001053374364348\n",
      "Training loss: 0.0004999145202719249\n",
      "Training loss: 0.00049972382179571\n",
      "Training loss: 0.0004995332419101814\n",
      "Training loss: 0.0004993427805178272\n",
      "Training loss: 0.0004991524375212371\n",
      "Training loss: 0.0004989622128231008\n",
      "Training loss: 0.0004987721063262131\n",
      "Training loss: 0.0004985821179334655\n",
      "Training loss: 0.0004983922475478536\n",
      "Training loss: 0.0004982024950724699\n",
      "Training loss: 0.0004980128604105125\n",
      "Training loss: 0.0004978233434652772\n",
      "Training loss: 0.0004976339441401591\n",
      "Training loss: 0.0004974446623386584\n",
      "Training loss: 0.0004972554979643676\n",
      "Training loss: 0.0004970664509209861\n",
      "Training loss: 0.0004968775211123094\n",
      "Training loss: 0.0004966887084422349\n",
      "Training loss: 0.0004965000128147588\n",
      "Training loss: 0.0004963114341339769\n",
      "Training loss: 0.0004961229723040829\n",
      "Training loss: 0.0004959346272293707\n",
      "Training loss: 0.0004957463988142357\n",
      "Training loss: 0.0004955582869631697\n",
      "Training loss: 0.0004953702915807608\n",
      "Training loss: 0.0004951824125717035\n",
      "Training loss: 0.0004949946498407823\n",
      "Training loss: 0.0004948070032928866\n",
      "Training loss: 0.000494619472833\n",
      "Training loss: 0.0004944320583662088\n",
      "Training loss: 0.0004942447597976923\n",
      "Training loss: 0.0004940575770327316\n",
      "Training loss: 0.0004938705099767032\n",
      "Training loss: 0.000493683558535084\n",
      "Training loss: 0.0004934967226134447\n",
      "Training loss: 0.0004933100021174588\n",
      "Training loss: 0.0004931233969528928\n",
      "Training loss: 0.000492936907025612\n",
      "Training loss: 0.0004927505322415775\n",
      "Training loss: 0.00049256427250685\n",
      "Training loss: 0.0004923781277275866\n",
      "Training loss: 0.000492192097810039\n",
      "Training loss: 0.0004920061826605576\n",
      "Training loss: 0.0004918203821855874\n",
      "Training loss: 0.0004916346962916721\n",
      "Training loss: 0.0004914491248854523\n",
      "Training loss: 0.0004912636678736588\n",
      "Training loss: 0.0004910783251631279\n",
      "Training loss: 0.0004908930966607833\n",
      "Training loss: 0.0004907079822736494\n",
      "Training loss: 0.0004905229819088441\n",
      "Training loss: 0.0004903380954735825\n",
      "Training loss: 0.000490153322875174\n",
      "Training loss: 0.000489968664021023\n",
      "Training loss: 0.0004897841188186287\n",
      "Training loss: 0.0004895996871755892\n",
      "Training loss: 0.0004894153689995935\n",
      "Training loss: 0.0004892311641984274\n",
      "Training loss: 0.0004890470726799695\n",
      "Training loss: 0.0004888630943521951\n",
      "Training loss: 0.0004886792291231745\n",
      "Training loss: 0.0004884954769010705\n",
      "Training loss: 0.0004883118375941417\n",
      "Training loss: 0.0004881283111107383\n",
      "Training loss: 0.0004879448973593073\n",
      "Training loss: 0.0004877615962483879\n",
      "Training loss: 0.0004875784076866131\n",
      "Training loss: 0.0004873953315827141\n",
      "Training loss: 0.00048721236784550877\n",
      "Training loss: 0.00048702951638391153\n",
      "Training loss: 0.00048684677710692973\n",
      "Training loss: 0.0004866641499236652\n",
      "Training loss: 0.0004864816347433128\n",
      "Training loss: 0.00048629923147515934\n",
      "Training loss: 0.00048611694002858544\n",
      "Training loss: 0.0004859347603130622\n",
      "Training loss: 0.0004857526922381564\n",
      "Training loss: 0.0004855707357135247\n",
      "Training loss: 0.0004853888906489204\n",
      "Training loss: 0.0004852071569541822\n",
      "Training loss: 0.00048502553453924846\n",
      "Training loss: 0.00048484402331414343\n",
      "Training loss: 0.0004846626231889888\n",
      "Training loss: 0.00048448133407399165\n",
      "Training loss: 0.0004843001558794585\n",
      "Training loss: 0.0004841190885157811\n",
      "Training loss: 0.00048393813189344587\n",
      "Training loss: 0.0004837572859230292\n",
      "Training loss: 0.00048357655051519934\n",
      "Training loss: 0.0004833959255807157\n",
      "Training loss: 0.0004832154110304304\n",
      "Training loss: 0.0004830350067752829\n",
      "Training loss: 0.0004828547127263054\n",
      "Training loss: 0.0004826745287946226\n",
      "Training loss: 0.00048249445489144806\n",
      "Training loss: 0.0004823144909280862\n",
      "Training loss: 0.0004821346368159283\n",
      "Training loss: 0.0004819548924664626\n",
      "Training loss: 0.00048177525779126207\n",
      "Training loss: 0.0004815957327019929\n",
      "Training loss: 0.0004814163171104082\n",
      "Training loss: 0.0004812370109283544\n",
      "Training loss: 0.0004810578140677678\n",
      "Training loss: 0.00048087872644066756\n",
      "Training loss: 0.0004806997479591709\n",
      "Training loss: 0.0004805208785354803\n",
      "Training loss: 0.0004803421180818859\n",
      "Training loss: 0.0004801634665107711\n",
      "Training loss: 0.0004799849237346056\n",
      "Training loss: 0.00047980648966594895\n",
      "Training loss: 0.0004796281642174489\n",
      "Training loss: 0.0004794499473018434\n",
      "Training loss: 0.00047927183883195694\n",
      "Training loss: 0.00047909383872070263\n",
      "Training loss: 0.00047891594688108624\n",
      "Training loss: 0.00047873816322619415\n",
      "Training loss: 0.000478560487669209\n",
      "Training loss: 0.00047838292012339524\n",
      "Training loss: 0.00047820546050211023\n",
      "Training loss: 0.0004780281087187937\n",
      "Training loss: 0.00047785086468697907\n",
      "Training loss: 0.000477673728320281\n",
      "Training loss: 0.00047749669953240834\n",
      "Training loss: 0.00047731977823715335\n",
      "Training loss: 0.0004771429643483977\n",
      "Training loss: 0.0004769662577801066\n",
      "Training loss: 0.00047678965844633427\n",
      "Training loss: 0.00047661316626122546\n",
      "Training loss: 0.0004764367811390063\n",
      "Training loss: 0.0004762605029939933\n",
      "Training loss: 0.0004760843317405888\n",
      "Training loss: 0.00047590826729328027\n",
      "Training loss: 0.0004757323095666437\n",
      "Training loss: 0.0004755564584753405\n",
      "Training loss: 0.00047538071393411654\n",
      "Training loss: 0.00047520507585780894\n",
      "Training loss: 0.00047502954416133667\n",
      "Training loss: 0.0004748541187597029\n",
      "Training loss: 0.0004746787995680006\n",
      "Training loss: 0.00047450358650140867\n",
      "Training loss: 0.0004743284794751891\n",
      "Training loss: 0.00047415347840469005\n",
      "Training loss: 0.0004739785832053457\n",
      "Training loss: 0.0004738037937926743\n",
      "Training loss: 0.000473629110082281\n",
      "Training loss: 0.00047345453198985614\n",
      "Training loss: 0.0004732800594311732\n",
      "Training loss: 0.00047310569232209216\n",
      "Training loss: 0.0004729314305785573\n",
      "Training loss: 0.00047275727411659626\n",
      "Training loss: 0.0004725832228523219\n",
      "Training loss: 0.0004724092767019347\n",
      "Training loss: 0.00047223543558171425\n",
      "Training loss: 0.00047206169940802983\n",
      "Training loss: 0.0004718880680973281\n",
      "Training loss: 0.0004717145415661462\n",
      "Training loss: 0.00047154111973110526\n",
      "Training loss: 0.00047136780250890296\n",
      "Training loss: 0.0004711945898163291\n",
      "Training loss: 0.00047102148157025195\n",
      "Training loss: 0.0004708484776876272\n",
      "Training loss: 0.0004706755780854873\n",
      "Training loss: 0.00047050278268095854\n",
      "Training loss: 0.00047033009139123955\n",
      "Training loss: 0.00047015750413361866\n",
      "Training loss: 0.0004699850208254658\n",
      "Training loss: 0.00046981264138423295\n",
      "Training loss: 0.00046964036572745625\n",
      "Training loss: 0.00046946819377275404\n",
      "Training loss: 0.0004692961254378264\n",
      "Training loss: 0.0004691241606404573\n",
      "Training loss: 0.00046895229929851205\n",
      "Training loss: 0.0004687805413299395\n",
      "Training loss: 0.0004686088866527719\n",
      "Training loss: 0.00046843733518511744\n",
      "Training loss: 0.00046826588684517416\n",
      "Training loss: 0.0004680945415512175\n",
      "Training loss: 0.0004679232992216048\n",
      "Training loss: 0.0004677521597747791\n",
      "Training loss: 0.00046758112312925913\n",
      "Training loss: 0.0004674101892036496\n",
      "Training loss: 0.00046723935791663436\n",
      "Training loss: 0.00046706862918698063\n",
      "Training loss: 0.0004668980029335346\n",
      "Training loss: 0.0004667274790752234\n",
      "Training loss: 0.00046655705753106047\n",
      "Training loss: 0.00046638673822013433\n",
      "Training loss: 0.000466216521061616\n",
      "Training loss: 0.00046604640597475676\n",
      "Training loss: 0.00046587639287889044\n",
      "Training loss: 0.00046570648169342997\n",
      "Training loss: 0.00046553667233786817\n",
      "Training loss: 0.00046536696473178143\n",
      "Training loss: 0.00046519735879482256\n",
      "Training loss: 0.00046502785444672623\n",
      "Training loss: 0.00046485845160730566\n",
      "Training loss: 0.0004646891501964572\n",
      "Training loss: 0.0004645199501341546\n",
      "Training loss: 0.00046435085134045195\n",
      "Training loss: 0.0004641818537354844\n",
      "Training loss: 0.0004640129572394646\n",
      "Training loss: 0.0004638441617726842\n",
      "Training loss: 0.0004636754672555174\n",
      "Training loss: 0.00046350687360841437\n",
      "Training loss: 0.00046333838075190764\n",
      "Training loss: 0.0004631699886066064\n",
      "Training loss: 0.00046300169709319826\n",
      "Training loss: 0.0004628335061324533\n",
      "Training loss: 0.0004626654156452169\n",
      "Training loss: 0.0004624974255524146\n",
      "Training loss: 0.0004623295357750505\n",
      "Training loss: 0.00046216174623420854\n",
      "Training loss: 0.0004619940568510484\n",
      "Training loss: 0.00046182646754681125\n",
      "Training loss: 0.0004616589782428123\n",
      "Training loss: 0.0004614915888604482\n",
      "Training loss: 0.00046132429932119174\n",
      "Training loss: 0.00046115710954659826\n",
      "Training loss: 0.00046099001945829407\n",
      "Training loss: 0.0004608230289779868\n",
      "Training loss: 0.00046065613802746374\n",
      "Training loss: 0.000460489346528587\n",
      "Training loss: 0.0004603226544032952\n",
      "Training loss: 0.00046015606157360876\n",
      "Training loss: 0.0004599895679616209\n",
      "Training loss: 0.0004598231734895046\n",
      "Training loss: 0.00045965687807950783\n",
      "Training loss: 0.0004594906816539596\n",
      "Training loss: 0.00045932458413525947\n",
      "Training loss: 0.00045915858544589064\n",
      "Training loss: 0.0004589926855084102\n",
      "Training loss: 0.0004588268842454504\n",
      "Training loss: 0.0004586611815797214\n",
      "Training loss: 0.0004584955774340115\n",
      "Training loss: 0.00045833007173118226\n",
      "Training loss: 0.00045816466439417276\n",
      "Training loss: 0.0004579993553459995\n",
      "Training loss: 0.0004578341445097536\n",
      "Training loss: 0.00045766903180860275\n",
      "Training loss: 0.0004575040171657914\n",
      "Training loss: 0.0004573391005046396\n",
      "Training loss: 0.000457174281748542\n",
      "Training loss: 0.0004570095608209669\n",
      "Training loss: 0.0004568449376454644\n",
      "Training loss: 0.0004566804121456559\n",
      "Training loss: 0.00045651598424523555\n",
      "Training loss: 0.00045635165386798105\n",
      "Training loss: 0.0004561874209377374\n",
      "Training loss: 0.0004560232853784281\n",
      "Training loss: 0.0004558592471140515\n",
      "Training loss: 0.0004556953060686788\n",
      "Training loss: 0.000455531462166459\n",
      "Training loss: 0.0004553677153316152\n",
      "Training loss: 0.0004552040654884437\n",
      "Training loss: 0.0004550405125613164\n",
      "Training loss: 0.0004548770564746807\n",
      "Training loss: 0.0004547136971530566\n",
      "Training loss: 0.00045455043452103847\n",
      "Training loss: 0.00045438726850329745\n",
      "Training loss: 0.0004542241990245755\n",
      "Training loss: 0.0004540612260096906\n",
      "Training loss: 0.0004538983493835324\n",
      "Training loss: 0.0004537355690710695\n",
      "Training loss: 0.0004535728849973383\n",
      "Training loss: 0.0004534102970874546\n",
      "Training loss: 0.0004532478052666011\n",
      "Training loss: 0.00045308540946003927\n",
      "Training loss: 0.00045292310959310286\n",
      "Training loss: 0.00045276090559119895\n",
      "Training loss: 0.0004525987973798052\n",
      "Training loss: 0.0004524367848844758\n",
      "Training loss: 0.00045227486803084015\n",
      "Training loss: 0.00045211304674459244\n",
      "Training loss: 0.00045195132095150845\n",
      "Training loss: 0.0004517896905774307\n",
      "Training loss: 0.00045162815554827866\n",
      "Training loss: 0.00045146671579004066\n",
      "Training loss: 0.00045130537122878165\n",
      "Training loss: 0.00045114412179063606\n",
      "Training loss: 0.00045098296740181106\n",
      "Training loss: 0.0004508219079885896\n",
      "Training loss: 0.00045066094347731946\n",
      "Training loss: 0.0004505000737944292\n",
      "Training loss: 0.0004503392988664122\n",
      "Training loss: 0.00045017861861983794\n",
      "Training loss: 0.00045001803298134785\n",
      "Training loss: 0.00044985754187765127\n",
      "Training loss: 0.0004496971452355355\n",
      "Training loss: 0.0004495368429818531\n",
      "Training loss: 0.0004493766350435309\n",
      "Training loss: 0.0004492165213475709\n",
      "Training loss: 0.00044905650182104054\n",
      "Training loss: 0.00044889657639107995\n",
      "Training loss: 0.0004487367449849013\n",
      "Training loss: 0.0004485770075297907\n",
      "Training loss: 0.0004484173639531001\n",
      "Training loss: 0.0004482578141822549\n",
      "Training loss: 0.00044809835814475297\n",
      "Training loss: 0.0004479389957681593\n",
      "Training loss: 0.00044777972698011314\n",
      "Training loss: 0.00044762055170832223\n",
      "Training loss: 0.0004474614698805666\n",
      "Training loss: 0.0004473024814246921\n",
      "Training loss: 0.0004471435862686223\n",
      "Training loss: 0.00044698478434034407\n",
      "Training loss: 0.00044682607556792\n",
      "Training loss: 0.0004466674598794785\n",
      "Training loss: 0.0004465089372032178\n",
      "Training loss: 0.0004463505074674128\n",
      "Training loss: 0.0004461921706004006\n",
      "Training loss: 0.00044603392653059045\n",
      "Training loss: 0.00044587577518646394\n",
      "Training loss: 0.00044571771649656893\n",
      "Training loss: 0.00044555975038952465\n",
      "Training loss: 0.0004454018767940183\n",
      "Training loss: 0.00044524409563880847\n",
      "Training loss: 0.0004450864068527224\n",
      "Training loss: 0.0004449288103646543\n",
      "Training loss: 0.00044477130610357033\n",
      "Training loss: 0.0004446138939985036\n",
      "Training loss: 0.00044445657397855737\n",
      "Training loss: 0.000444299345972904\n",
      "Training loss: 0.00044414220991078457\n",
      "Training loss: 0.0004439851657215091\n",
      "Training loss: 0.00044382821333445357\n",
      "Training loss: 0.00044367135267906664\n",
      "Training loss: 0.0004435145836848629\n",
      "Training loss: 0.00044335790628142606\n",
      "Training loss: 0.0004432013203984064\n",
      "Training loss: 0.0004430448259655254\n",
      "Training loss: 0.0004428884229125724\n",
      "Training loss: 0.0004427321111694011\n",
      "Training loss: 0.0004425758906659374\n",
      "Training loss: 0.0004424197613321733\n",
      "Training loss: 0.0004422637230981682\n",
      "Training loss: 0.00044210777589405095\n",
      "Training loss: 0.0004419519196500149\n",
      "Training loss: 0.0004417961542963265\n",
      "Training loss: 0.0004416404797633135\n",
      "Training loss: 0.0004414848959813744\n",
      "Training loss: 0.0004413294028809753\n",
      "Training loss: 0.0004411740003926471\n",
      "Training loss: 0.0004410186884469923\n",
      "Training loss: 0.00044086346697467534\n",
      "Training loss: 0.00044070833590643195\n",
      "Training loss: 0.00044055329517306383\n",
      "Training loss: 0.0004403983447054347\n",
      "Training loss: 0.0004402434844344844\n",
      "Training loss: 0.0004400887142912116\n",
      "Training loss: 0.0004399340342066828\n",
      "Training loss: 0.00043977944411203627\n",
      "Training loss: 0.0004396249439384696\n",
      "Training loss: 0.000439470533617253\n",
      "Training loss: 0.0004393162130797191\n",
      "Training loss: 0.0004391619822572675\n",
      "Training loss: 0.00043900784108136514\n",
      "Training loss: 0.000438853789483544\n",
      "Training loss: 0.00043869982739540417\n",
      "Training loss: 0.0004385459547486059\n",
      "Training loss: 0.00043839217147488183\n",
      "Training loss: 0.00043823847750602975\n",
      "Training loss: 0.0004380848727739082\n",
      "Training loss: 0.00043793135721044635\n",
      "Training loss: 0.0004377779307476377\n",
      "Training loss: 0.0004376245933175385\n",
      "Training loss: 0.0004374713448522745\n",
      "Training loss: 0.00043731818528403385\n",
      "Training loss: 0.0004371651145450711\n",
      "Training loss: 0.00043701213256770515\n",
      "Training loss: 0.00043685923928432293\n",
      "Training loss: 0.00043670643462737156\n",
      "Training loss: 0.00043655371852936665\n",
      "Training loss: 0.00043640109092288944\n",
      "Training loss: 0.0004362485517405827\n",
      "Training loss: 0.000436096100915157\n",
      "Training loss: 0.0004359437383793834\n",
      "Training loss: 0.0004357914640661003\n",
      "Training loss: 0.00043563927790821286\n",
      "Training loss: 0.00043548717983868585\n",
      "Training loss: 0.0004353351697905537\n",
      "Training loss: 0.00043518324769691036\n",
      "Training loss: 0.0004350314134909188\n",
      "Training loss: 0.0004348796671057992\n",
      "Training loss: 0.0004347280084748392\n",
      "Training loss: 0.0004345764375313951\n",
      "Training loss: 0.00043442495420888106\n",
      "Training loss: 0.00043427355844077663\n",
      "Training loss: 0.0004341222501606264\n",
      "Training loss: 0.0004339710293020367\n",
      "Training loss: 0.00043381989579868225\n"
     ]
    }
   ],
   "source": [
    "params = calculate_gradient()\n",
    "y_pred = params[0]*X**2 + params[1]*X + params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea63e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final loss for this model: 0.00043381989579868225\n",
      "The final parameters are:\n",
      "weights: w1=1.755643048563951 w2=-0.5011458016995667 , bias:0.029560099499798414\n"
     ]
    }
   ],
   "source": [
    "#printing the params and the losses for the regression model.\n",
    "print(\"The final loss for this model: {}\".format(losses[-1]))\n",
    "print(\"The final parameters are:\\nweights: w1={} w2={} , bias:{}\".format(params[0],params[1],params[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ddef75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAHgCAYAAAC1uFRDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3y0lEQVR4nO3df5zcVX3o/9fZzYYsBLMhkUB+WHLvF6kxhF/BHwVpYpTEKlBFAW310orU9qJe1FS87TdQ+njUKFdp47VKLvjV9l6JETENBYotsJeqpQUaGhBEqCDZDRoIbEpgQza75/vH7G4mn53dnZn9fGbmM/N6Ph557M5nPjNz9rDhnXPO+31OiDEiSZLyp63eDZAkSdUxiEuSlFMGcUmScsogLklSThnEJUnKKYO4JEk5Na3eDajU3Llz43HHHVfRa1566SWOOOKIbBrUYuzLdNiP6bEv02NfpiOLfnzggQeeizG+Onk9d0H8uOOO4/7776/oNd3d3axYsSKbBrUY+zId9mN67Mv02JfpyKIfQwg/L3Xd6XRJknLKIC5JUk4ZxCVJyqncrYmXMjAwQE9PD/v27Sv5/KxZs3j00Udr3Kr8mTFjBgsXLqSjo6PeTZEklaEpgnhPTw9HHnkkxx13HCGEMc+/+OKLHHnkkXVoWX7EGNm9ezc9PT0sXry43s2RJJWhKabT9+3bx5w5c0oGcJUnhMCcOXPGnc2QJDWepgjigAE8BfahJOVL0wTxRrBlyxZCCPzkJz+Z8L4///M/5+WXX676c77xjW9w2WWXVf16SVJzMIin6MYbb+TMM8/kxhtvnPC+qQZxSZKgRYP4lm29nLH+LhZfcStnrL+LLdt6p/yee/fu5Qc/+AE33HADmzZtAmBwcJBPf/rTLF26lGXLlvHlL3+ZDRs2sHPnTlauXMnKlSsBmDlz5uj73HTTTVx88cUA3HLLLbzxjW/klFNO4W1vexu//OUvp9xOSVLzaIrs9Eps2dbLZ29+iP6BQQB6+/r57M0PAfCbpyyo+n3/5m/+hjVr1vDa176WOXPm8MADD/Av//IvPPXUUzz44INMmzaN559/nqOOOoovfelL3H333cydO3fC9zzzzDO59957CSFw/fXX84UvfIEvfvGLVbdRktRcWi6IX3PHY6MBfET/wCDX3PHYlIL4jTfeyCc+8QkALrroIm688UaefPJJPvrRjzJtWqGbjzrqqIres6enhwsvvJBnnnmG/fv3W/olSTpEywXxnX39FV0vx/PPP89dd93FQw89RAiBwcFBQgicfvrpZb2+OCu8uMTrYx/7GJ/85Cc599xz6e7u5qqrrqq6jZKk5tNya+Lzuzorul6Om266iQ9+8IP8/Oc/56mnnmLHjh0sXryYk046ieuuu44DBw4AhWAPcOSRR/Liiy+Ovn7evHk8+uijDA0N8b3vfW/0+p49e1iwoDA78M1vfrPq9kmSmlPLBfG1q0+gs6P9kGudHe2sXX1C1e9544038u53v/uQa+effz7PPPMMr3nNa1i2bBknnXQS3/rWtwC49NJLWbNmzWhi2/r163nXu97Fr/3ar3HssceOvsdVV13F+973Pk477bRJ188lSa0nxBjr3YaKLF++PCbPE3/00Ud53eteN+5rktuubtnWyzV3PMbOvn7md3WydvUJU1oPbyaT9aXnDafDfkyPfZke+3KKtm+GO6+m+5hLWPGL62HVOlh2QSpvHUJ4IMa4PHm95dbEoZCFbtCWJKVm+2a45eMw0A/HAHt2FB5DaoG8lJabTpckKXV3Xl0I4MUG+gvXM2QQlyRpqvb0VHY9JQZxSZKmatbCyq6nxCAuSdJUrVoHHYlS5Y7OwvUMtWRimyRJqRpJXhtZA5+1KNXs9PE4Ek9Je3s7J598MkuXLuV973vflE4pu/jii7npppsAuOSSS3jkkUfGvbe7u5sf/ehHFX/Gcccdx3PPPVd1GyVJCcsugMsfhmNPLnzNOICDQTw1nZ2dPPjggzz88MNMnz6dr33ta4c8P7JrW6Wuv/56lixZMu7z1QZxSVL+tWYQ374Zrl0KV3UVvm7fnOrbv+Utb+GJJ56gu7ubt7zlLZx77rksWbKEwcFB1q5dy+mnn86yZcu47rrrAIgxctlll3HCCSfwtre9jV27do2+14oVKxjZ3Obv/u7vOPXUUznppJNYtWoVTz31FF/72te49tprOfnkk/nHf/xHnn32Wc4//3xOP/10Tj/9dH74wx8CsHv3bs4++2xe//rXc8kll5C3TX4kqeFkHEvK0Xpr4sUF+ZB6Qf6BAwe4/fbbWbNmDQD/+q//ysMPP8zixYvZuHEjs2bN4r777uOVV17hjDPO4Oyzz2bbtm089thjPPLII/zyl79kyZIl/O7v/u4h7/vss8/ykY98hHvuuYfFixePHmv60Y9+lJkzZ/LpT38agA984ANcfvnlnHnmmTz99NOsXr2aRx99lD/5kz/hzDPPZN26ddx6663ccMMNU/5ZJallTRRLOLpmzWi9ID5RQf4Ugnh/fz8nn3wyUBiJf/jDH+ZHP/oRb3jDG0aPEP3+97/P9u3bR9e79+zZw+OPP84999zD+9//ftrb25k/fz5vfetbx7z/vffey1lnnTX6XuMda/oP//APh6yh/8d//Ad79+7lnnvu4eabbwbgne98J7Nnz676Z5WkljdRLDnlf9asGa0XxDMqyB9ZE0864ogjRr+PMfLlL3+Z1atXH3LPbbfdNqXPLjY0NMS9997LjBkzUntPSVJCnTZ3SWq9NfE6FeQDrF69mq9+9asMDAwA8NOf/pSXXnqJs846i29/+9sMDg7yzDPPcPfdd4957Zve9CbuuecennzySWD8Y03PPvtsvvzlL48+HvmHxVlnnTV6itrtt9/OCy+8kMnPKEktoY6xpFjrBfE6FeRDoVxsyZIlnHrqqSxdupTf+73f48CBA7z73e/m+OOPZ8mSJXzoQx/izW9+85jXvvrVr2bjxo285z3v4aSTTuLCCy8E4JxzzuF73/veaGLbhg0buP/++1m2bBlLliwZzZK/8sorueeee3j961/PzTffzGte85rMf15Jalp1jCXFWvIo0pHj4tjTU/hXUw0K8vPCo0hrw35Mj32ZHvuyQuPEkiz60aNIiy27wKAtSZqaBoglrTedLklSkzCIS5JUjgbY3CWpaabTY4yEEOrdjFzLW36EJNVMxhuFVaspRuIzZsxg9+7dBqEpiDGye/du68slqZSJNnepo6YYiS9cuJCenh6effbZks/v27fP4FSGGTNmsHBhbWscJSkXGmRzl6SmCOIdHR2j25GW0t3dzSmnnFLDFkmSmsqshYUp9FLX66gpptMlScpUg2zukmQQlyRpMssugHM2wKxFQCh8PWdD3evEM5tODyF8HXgXsCvGuLTE878FfAYIwIvA78cY/y2r9kiSNCUNsLlLUpYj8W8AayZ4/kng12OMJwJ/CmzMsC2SJDWdzIJ4jPEe4PkJnv9RjHHkKK17AdOiJUmNowE3d0nK9ACUEMJxwN+Wmk5P3Pdp4FdjjJeM8/ylwKUA8+bNO23Tpk0VtWPv3r3MnDmzoteoNPsyHfZjeuzL9NiXRfpfKGSjx6GD10JbYS28c/aEL82iH1euXNmYB6CEEFYCHwbOHO+eGONGhqfbly9fHis9HcaTedJjX6bDfkyPfZke+7LItUvHKSlbBJc/POFLa9mPdQ3iIYRlwPXAO2KMu+vZFkmSRjXo5i5JdSsxCyG8BrgZ+GCM8af1aockSWOMt4lLnTd3ScosiIcQbgT+CTghhNATQvhwCOGjIYSPDt+yDpgD/GUI4cEQwv1ZtUWSpIo06OYuSZlNp8cY3z/J85cAJRPZJEmqq5F68DuvLkyhz1pYCOANVide98Q2SZIaUgNu7pLktquSJEEu6sKTHIlLkrR9M9zy8YNnhu/ZUXgMDT0adyQuSdKdVx8M4CMG+gvXG5hBXJKknNSFJxnEJUnKSV14kkFckqSc1IUnGcQlSVp2AZyzobA3OqHw9ZwNDZ3UBmanS5JUkIO68CRH4pIk5ZRBXJLUmnK4uUuS0+mSpNaT081dkhyJS5JaT043d0kyiEuSWk9ON3dJMohLklpPTjd3STKIS5JaT043d0kyiEuSWk9ON3dJMjtdktSacri5S5IjcUmScsogLklqDU2wuUuS0+mSpObXJJu7JDkSlyQ1vybZ3CXJIC5Jan5NsrlLkkFcktT8mmRzlySDuCSp+TXJ5i5JBnFJUvNrks1dksxOlyS1hibY3CXJkbgkqTk1YV14kiNxSVLzadK68CRH4pKk5tOkdeFJBnFJUvNp0rrwJIO4JKn5NGldeJJBXJLUfJq0LjzJIC5Jaj5NWheeZHa6JKk5NWFdeJIjcUlSc2iBuvAkR+KSpPxrkbrwJEfikqT8a5G68CSDuCQp/1qkLjzJIC5Jyr8WqQtPMohLkvKvRerCkwzikqT8a5G68CSz0yVJzaEF6sKTHIlLkpRTBnFJUv604MYupTidLknKlxbd2KUUR+KSpHxp0Y1dSjGIS5LypUU3dinFIC5JypcW3dilFIO4JClfWnRjl1IM4pKkfGnRjV1KMTtdkpQ/LbixSymZjcRDCF8PIewKITw8zvMhhLAhhPBECGF7COHUrNoiSco568JLynI6/RvAmgmefwdw/PCfS4GvZtgWSVJejdSF79kBxIN14Q0WyLds6+WM9XfxUO8ezlh/F1u29Wb+mZkF8RjjPcDzE9xyHvBXseBeoCuEcGxW7ZEk5VQO6sK3bOvlszc/RG9foZ29ff189uaHMg/k9UxsWwDsKHrcM3xNkqSDclAXfs0dj9E/MHjItf6BQa6547FMPzfEGLN78xCOA/42xri0xHN/C6yPMf5g+PGdwGdijPeXuPdSClPuzJs377RNmzZV1I69e/cyc+bMyn8AjWFfpsN+TI99mZ6G7ctdj8Dg/rHX26fD0Utq354SHurdM/r9vE74ZdHEwYkLZk35/VeuXPlAjHF58no9s9N7gUVFjxcOXxsjxrgR2AiwfPnyuGLFioo+qLu7m0pfo9Lsy3TYj+mxL9PTsH25fdehe6VDoS78nA2wbEXdmlXsj9bfNTqV/qkTD/DFhwrhdUFXJx/7rRWZfW49p9O3Ah8azlJ/E7AnxvhMHdsjSWpEOagLX7v6BDo72g+51tnRztrVJ2T6uZmNxEMINwIrgLkhhB7gSqADIMb4NeA24DeAJ4CXgd/Jqi2SpJxrwLrwLdt6ueaOx9jZ18/8rk7OP20Bd//kWeBFFnR1snb1CfzmKdmmemUWxGOM75/k+Qj816w+X5KUY9s3F7LP9/QU9kRfta6hgvhINvpIMltvXz/ffaCXz73nRLr2PJ7pFHoxt12VJDWWHNSF1ysbPckgLklqLDmoC9/Z11/R9awYxCVJjSUHdeHzuzorup4Vg7gkqbHk4LzwemWjJxnEJUmNpUHPCx/ZG33xFbdyzR2Pcf5pC1jQ1UmgUA/+ufecmHk2epJHkUqSGstIFnoDZadPlI1e68BdzCAuSWo8DVYXPlE2ej2DuNPpkqT6a/DzwhslGz3JIC5Jqq8c1IU3SjZ6kkFcklRfOagLb5Rs9CTXxCVJ9dWgdeHj7Y0+8rgWe6NPxiAuSaqvWQuHp9JLXK+TRs1GT3I6XZJUXw1YF94oe6NPxiAuSaqvBjwvvFGz0ZOcTpck1V+D1YXP7+qkt0TArnc2epJBXJJUew14XnhxItuszg462gMDg3H0+UbIRk8yiEuSamukLnykrGykLhzqFsiTiWx9/QN0tAVmH95B38sDDZONnmQQlyTV1kR14XUK4qUS2QaGIodPn8a2dWfXpU3lMLFNklRbDVgXnpdEtiSDuCSpthrwvPBG3VZ1MgZxSVJtNWBdeKNuqzoZ18QlSbXVIOeF52Fb1ckYxCVJ2StVUnb5w3VrTl62VZ2M0+mSpGw14FGjedlWdTIGcUlSthrwqNG8ZqMnGcQlSdlqwJKyvGajJxnEJUnZasCSsrxmoyeZ2CZJytaqdYduswp1KSlrhmz0JIO4JClbDVBS1izZ6EkGcUlS9up81OhE2eh5DuKuiUuS0rd9M1y7FK7qKnytYzkZNE82epJBXJKUrgasC2+WbPQkg7gkKV0NWBfeLNnoSa6JS5LS1SB14c2YjZ5kEJckpWvWwuGp9BLXa6RZs9GTnE6XJKWrAY4abZa90SdjEJckpWvZBXDOBpi1CAiFr+dsqGmJWbNmoyc5nS5JmroGO2p0flcnvSUCdt6z0ZMM4pKkqRkpKRvJSB8pKYOa78o2ksg2q7ODjvbAwGAcfb4ZstGTnE6XJE1NA5SUjSSy9fb1E4G+/gGIMPvwDgKwoKuz6ZLawJG4JGmqGqCkrFQi28BQ5PDp09i27uyataPWHIlLkqamAY4abZVEtiSDuCRpahqgpKxZt1WdjEFckjQ1DVBS1qzbqk7GNXFJ0tTV+KjR5Jaqa1efwOfec+KYa82WyJZkEJckVa5UXXiNgnipLVU/e/NDfO49J/LDK95akzY0CqfTJUmVqfNRo62ypWo5DOKSpMrUuS68VTPRSzGIS5IqU+e68FbNRC/FNXFJUmXqcNRoK26pWg5H4pKkytS4LrxVt1QthyNxSVJlRrLQa5Sd3qpbqpYj0yAeQlgD/AXQDlwfY1yfeP41wDeBruF7rogx3pZlmyRJVajjUaMmso0vs+n0EEI78BXgHcAS4P0hhCWJ2/4Y2BxjPAW4CPjLrNojSapSnUvKTGQbX5Zr4m8Anogx/izGuB/YBJyXuCcCrxr+fhawM8P2SJKqUeeSslbdUrUcWU6nLwCK0xd7gDcm7rkK+H4I4WPAEcDbMmyPJKkadSgpS26rev5pC7j7J8+21Jaq5QgxxsnvquaNQ3gvsCbGeMnw4w8Cb4wxXlZ0zyeH2/DFEMKbgRuApTHGocR7XQpcCjBv3rzTNm3aVFFb9u7dy8yZM6f086jAvkyH/Zge+zI94/blrkdgcP/Y6+3T4ejkKunU9fUP0PtCP0NF8aktBBbM7qSrsyP1z0tbFr+TK1eufCDGuDx5PcuReC+wqOjxwuFrxT4MrAGIMf5TCGEGMBfYVXxTjHEjsBFg+fLlccWKFRU1pLu7m0pfo9Lsy3TYj+mxL9Mzbl9u31VYAy+eUu/oHD6prMT9U3TG+rvo7Wsfc31BVzs/vCL9z0tbLX8ns1wTvw84PoSwOIQwnULi2tbEPU8DqwBCCK8DZgDPZtgmSVKlanzUqNno5ctsJB5jPBBCuAy4g0L52NdjjD8OIVwN3B9j3Ap8CvhfIYTLKSS5XRyzmt+XJJVnvBPKanRK2fyuTnpLBGyz0cfKtE58uOb7tsS1dUXfPwKckWUbJEkVGCknG5k6Hykng0yDuNuqVsdtVyVJB9WhnMxtVavntquSpIPqUE7mtqrVcyQuSTpovJPIMjyhzES26hnEJUkH1fiEMnBb1alwOl2SdFCNTigzkS0dBnFJanUjJWXHXALXXpb5CWUjiWwj6+B9/QN0tAVmH95B38sDbqtaAYO4JLWy4pKyY6hJSZmJbOlxTVySWlkdSspMZEuPQVySWlkdSspMZEuPQVySWlkdSso8Hzw9rolLUitbta70CWUpl5R5Png2DOKS1MqKS8qgcEJZyiVlyWz03r5+vvtAr1uppsDpdElqNds3w7VL4aquwlcolJQde3Lha8pZ6aWy0fsHBrnmjsdS/ZxW5EhcklrJRKeUcXQmH2k2enYciUtSK6lDSZnZ6NlxJC5JraRGJWVuq1objsQlqZXUoKTM88Frx5G4JLWSiUrKnk/nI9xWtXYciUtSK1l2AZyzoVBKRih8PWdDqhnpJrLVjiNxSWp2I6eUFR8tmuEpZfO7OuktEbBNZEufQVySmtlEJWUpb+hiIlvtOZ0uSc2sBiVlJrLVjyNxSWpmNSgpM5GtfhyJS1Izq0FJmYls9WMQl6RmtmpdoYSsWMqnlLkjW/04nS5Jzaz4lLLi7PQpJrWZyNYYDOKS1GwyLilLHi3a1z9AR1tg9uEd9L084PngNWQQl6RmUoOSMhPZGodr4pLUTGpQUmYiW+MwiEtSM6lBSZmJbI3D6XRJaiazFham0EtdnwIT2RqTI3FJaiYZlJS5I1vjciQuSc0kg5IyE9kal0FckvIu45IyE9kal9PpkpRnIyVle3YA8WBJ2fbNqX2EiWyNy5G4JOXZRCVlVU6hFyexze/qZOWvvprvPtB7yJS6iWyNwZG4JOVZyiVlySS23r5+vvtAL+eftoAFXZ0msjUYR+KSlGcpl5SVSmLrHxjk7p88yw+veGtV76nsOBKXpDxLuaTMJLZ8cSQuSXmTzEY/6QPw+PdTKSmb39VJb4mAbRJbYzKIS1KelDrg5N++BedsmHIi20WLXuSlVzrdjS1HnE6XpDxJ+YCT4kQ2cDe2vHEkLkl5knI2urux5dukI/EQwsdCCLNr0RhJ0iTGyzqvMhvdRLZ8K2c6fR5wXwhhcwhhTQghZN0oSdI4Us5Gdze2fJs0iMcY/xg4HrgBuBh4PITwZyGE/5xx2yRJScsuKCSxzVoEhMLXCpPatmzr5Yz1d7H4ilt56ZUDdLQfOjYzkS0/yloTjzHGEMIvgF8AB4DZwE0hhL+PMf5hlg2UpJaX4gEnI4lsI+vgff0DdLQFZh/eARxgQVcna1efYCJbTkwaxEMInwA+BDwHXA+sjTEOhBDagMcBg7gkZaVUSdktHy98X0VJ2USJbCcumMXHfmvFFBusWipnTfwo4D0xxtUxxu/EGAcAYoxDwLsybZ0ktbqUS8pMZGsuk47EY4xXTvDco+k2R5J0iBRKyopPJWsLgcEYx9xjIls+udmLJDWyKZaUJU8lKxXATWTLL4O4JDWyKZaUlVoDB2gPwR3ZmoA7tklSo0nxgJPx1rqHYuTJ9e9Ms9Wqg0yDeAhhDfAXQDtwfYxxfYl7LgCuAiLwbzHGD2TZJklqaCkfcOKpZM0tsyAeQmgHvgK8HeihsOvb1hjjI0X3HA98FjgjxvhCCOHorNojSbkwUTZ6mUG8OJFtVmeHp5I1sSzXxN8APBFj/FmMcT+wCTgvcc9HgK/EGF8AiDHuyrA9ktT4ppiNnkxk81Sy5hZiiUzFVN44hPcCa2KMlww//iDwxhjjZUX3bAF+CpxBYcr9qhjj35V4r0uBSwHmzZt32qZNmypqy969e5k5c2aVP4mK2ZfpsB/T03R9uesRGNw/9nr7dDh6yaQvf+wXL7J/cGjM9entbZxwzJETvrbp+rJOsujHlStXPhBjXJ68Xu/EtmkU9mVfASwE7gkhnBhj7Cu+Kca4EdgIsHz58rhixYqKPqS7u5tKX6PS7Mt02I/pabq+3L7r0DVxKGSjn7MBlq2Y9OW/c8WtxBKTrAF4cv3Er2+6vqyTWvZjlkG8F1hU9Hjh8LViPcA/D+8C92QI4acUgvp9GbZLkhpLitnoJrK1liyD+H3A8SGExRSC90VAMvN8C/B+4P8LIcwFXgv8LMM2SVJjSSEb3US21pVZYluM8QBwGXAH8CiwOcb44xDC1SGEc4dvuwPYHUJ4BLibwuEqu7NqkyQ1nCnujW4iW2vLdE08xngbcFvi2rqi7yPwyeE/ktR6ppiNPtGpZNvWnT3V1qnBue2qJNXTFPdG91Sy1lbv7HRJam2r1pXORp9gb3RPJdMIR+KSVEvbN8O1S+GqrsJXKCSxzVoEhMLXCZLaPJVMxRyJS1KtlMpEv+XjhaB9+cNlvcVEp5INxcj8rk7Wrj7BRLYWYRCXpFpJYV90TyVTMafTJalWppiJDuOvdbsG3pociUtSrcxaWJhCL3V9Am7movE4EpekWlm1rpB5XqyMTHQ3c9F4HIlLUpamuC+6m7loIgZxScpKCvuiu5mLJmIQl6SsVJmN7mYuKpdBXJKyUkU2+sga+MgUupu5aCImtklSVqrYF32izVxMZFOSI3FJykoV+6K7mYsqYRCXpDRNMRt9flcnvSUCuWvgKsUgLklpqTIb3c1cVC3XxCUpLRNlo4/DzVw0FY7EJSktVWSju5mLpsKRuCSlpYpsdDdz0VQ4EpektJSRjV68/j2/q5Ouwzt44eWBMW9lIpvKYRCXpKmoIBs9uZFLb18/HW3BRDZVzSAuSdWqMBt9vPXvrs4Ojjhs2ujofO3qE0xkU1kM4pJUrQr3Rh9vnXtP/wAPXmkSmypnEJekapWRje5hJsqS2emSVK1JstGTNeAeZqK0GcQlqVqr1hWyz4sVZaN7mImy5nS6JFWigmx0DzNR1gziklSuCrPRPcxEWTOIS1K5yshG9zAT1ZJBXJLKNUk2enIzl77+ATraArMP76Dv5QFrwJU6g7gklWvWwsIUeqnreJiJas/sdEkq1yTZ6B5molpzJC5JE5kkG/2+//wx/tttc9n5rVvdzEU1ZxCXpPFMko1+cA288LybuajWnE6XpPFMlI2Om7mo/hyJS9J4JslGdzMX1ZsjcUkazzh7o/+CuSy+orAGXopr4KoVR+KSVKw4ka1zNrRPh8H9o0/3x+n82cD7PNBEDcEgLkkjkols/c9DWwd0HgX9L/AL5vJnA+9j69CZh7ysPQSGYnQzF9WcQVySRpRKZBsagOlHwGee5M1X3MrYsbdr4Kof18QlacQ4iWxDe3pcA1dDMohL0ohxEtl2Ds1xDVwNyel0Sa1tkkS2l+N0vnDg0GNGXQNXozCIS2pdkySy9QzN4QsHLhiTyOYauBqFQVxS6xonke0X/e28ed//cS90NTzXxCW1rnES2Y6Oz7kGrlwwiEtqXeMlssU5hzx2L3Q1KqfTJbWuVesOXROndCKba+BqVAZxSa0lcT74v88/jyN+fidHx+d4hjl8fmBsIptr4GpUBnFJraPE+eDz+27mMwOXjAncI1wDVyNzTVxS6yiRjd4Z9vOH0zYfcs01cOWFI3FJrWOcbPT5Yfchj10DV15kOhIPIawJITwWQngihHDFBPedH0KIIYTlWbZHUosrMxvdNXDlRWYj8RBCO/AV4O1AD3BfCGFrjPGRxH1HAp8A/jmrtkhqYUWJbK90vIoQpzE9HBh9OpmN7hq48iTLkfgbgCdijD+LMe4HNgHnlbjvT4HPA/sybIukVjSSyLZnBxA5bGAPkcjzcSZDMdAzNJcrBi7h1vgW18CVS1muiS8AdhQ97gHeWHxDCOFUYFGM8dYQwtoM2yKpFZVIZDssDPLs0AxO3b9x9FrANXDlU90S20IIbcCXgIvLuPdS4FKAefPm0d3dXdFn7d27t+LXqDT7Mh32Y3om7MtjLoFjSlyP8Kl4cEp9enub/z3w9zIttezHLIN4L7Co6PHC4WsjjgSWAt0hBCj8VdsaQjg3xnh/8RvFGDcCGwGWL18eV6xYUVFDuru7qfQ1Ks2+TIf9mJ6J+vLlz3+Uw/ufGXO9Z2guF+/fABTWwD/3nhNZ4RS6v5cpqWU/ZhnE7wOODyEsphC8LwI+MPJkjHEPMHfkcQihG/h0MoBLUtkSu7Hdvu8k3hF3c3g49Hzw/zF4IQE8D1y5l1kQjzEeCCFcBtwBtANfjzH+OIRwNXB/jHFrVp8tqQVt38yBv/kY0waHc2T37OAd8Zd8Z/AsVrU9yPywm52xcD74LUNnuAauppDpmniM8TbgtsS1dePcuyLLtkhqbi/fvo7DBw8tcjk87GdV24OcOTx1PmKBdeBqEm67KqkpzOj/Rcnryd3YrANXMzGIS2oKO4fmlL4e57Cgq9M6cDUl906XlFv3bb2OAy8NMHTleRweZrK/xG5s10//bX54xVvr2EopO47EJeXSfVuvY+kDf8w0DtAW4Kiwd8xubOvipZz8zkvr3VQpM47EJeXSon+9hs6i0jEo7Mb2XJzBaa9stHxMLcEgLimXjo7PQhh7/Vh2Wz6mlmEQl5QbW7b1cs0dj7Gzr58fHDaXBTw35p5dYW7JnValZmQQl5QLW7b18oPv/SXfZhPzD3uOF+JM9if+F9Yfp7PjtLUGcbUMg7ikXHjw1o1cHTaObqE6J+zlldjOYGxjKAZ2hbnsOG0tp5/7e3VuqVQ7BnFJDat4+vwfp/9vDm8bm8g2RBttf9LHMZQ+sExqZgZxSQ1py7ZePnvzQ/QPDAIwP4xd/wboKKoLl1qNdeKSGtI1dzw2GsABdsa5Je8bCh21apLUcByJS2pIO/v6ObftB/zhtM3MD8OJbIkd2Q60z6Bt1vw6tlKqL4O4pIZRvAZ+XvsP+bNp149JZOvjSLrYC7MWMm3VOnh+dp1bLdWPQVxSQ0iugX+6/dujAXzEYWGQwc6Z8Jmegxe7u2vYSqmxuCYuqSEk18DHS2Q7fJwjR6VW5EhcUt0UT59HOGQNfIg22hga+6JZC2veTqlRGcQl1UVy+vzcth+wvuPgGngbQ8QIoXh/9I5OWLWuDq2VGpPT6ZLqIjl9/ofTNo9ZAw8BhkIbEGDWIjhnAyy7oMYtlRqXI3FJdbGzr/+Qx+OtgbfFCFf11aBFUv4YxCXVTPEaeFsIDMY4+tzOOJeFpQK5a+DSuAzikmoiuQY+GOOkm7m4Bi5NzCAuqSaSa+DJRLY5YS+DYRrMOAr6XyiMwFetcw1cmoBBXFJmkiVkxUolsrXHAzD9CPjMk7VrpJRjBnFJmUhOnyeNl8jGnp7S1yWNYRCXlInk9Dm4mYuUNoO4pNRMNH1ecjMXoHgvFxPZpMoYxCWlYrLp85KbuQCEdohDJrJJVTCIS0pFqenzYuOugcchN3ORqmQQl1S1iabPiwVgV3g1x/Ds2CddA5eqZhCXVJXJps+hsA7+36d/h2N4Djpnw/7pMFg0pe4auDQlHoAiqSqTTZ+f2/YDPt9x/fDoO0L/8xAjdB6FB5pI6XAkLqkqyQNMigXgv0//Dp0cmsjG0ICbuUgpMohLKkvx+vf8rk66Du/ghZcHxty3oKuTH17xVrjqt0q/kZu5SKlxOl3SpEbWv3uHE9h6+/rZu+8AHe2HVHnz3uk/4u/DH8BVXRDG+d+LiWxSagzikiZVav17YChyxPRpLOjqJAAXz/yXwmYu/c8AEWKJ9XIT2aRUOZ0uqaRyysf29A/w4JVnFx5c+xnYs2/sTW7mImXGIC5pjHLKxwDmd3UefDDeWrebuUiZMYhLGmOy8jGAzo52/nzJ43DtxwsBPLSVnkJ3DVzKjEFcElDZ7mvzuzr58yWPc/pDV8LAcKmZa+BSzRnEJZU9fT5aPgaFEfhAiVpx18ClmjGISyp7+nzt6hMOXnANXKo7g7jUoqqaPu/+NPxNT2GU3Tm7sJVqkmvgUs0YxKUWVPH0+fbNcEvR+veeHdDWAe0eaCLVk5u9SC2o4unzO68eu/49NADTZxYOMvFAE6kuHIlLLaLS6fO1q0/gN09ZULg43vp3/wseZiLVkUFcagFVZZ9v3wzXXm0NuNTADOJSkyoeebeFwGCcaPydmD7fvhlu+bg14FKDM4hLTSg58p4ogJecPi+1Bg7WgEsNxiAuNaFyEtcgMX1ezBpwKRcM4lKTKDdxbcSYzVu2by6MwF0Dl3LDIC41gXIT19pDYCjGsdPnroFLuZRpEA8hrAH+AmgHro8xrk88/0ngEuAA8CzwuzHGn2fZJqkZlVv3/bn3nHgwcBdzDVzKpcyCeAihHfgK8HagB7gvhLA1xvhI0W3bgOUxxpdDCL8PfAG4MKs2Sc1kSnXfcOj0+Xjv4Bq41NCyHIm/AXgixvgzgBDCJuA8YDSIxxjvLrr/XuC3M2yP1DSqqvsulpw+H49r4FJDy3Lb1QXAjqLHPcPXxvNh4PYM2yM1japOHSs23vR5MdfApYYX4iQbQFT9xiG8F1gTY7xk+PEHgTfGGC8rce9vA5cBvx5jfKXE85cClwLMmzfvtE2bNlXUlr179zJz5szKfwiNYV+mo5p+7Osf4Jd79rF/cGjSe6e3tzFv1gy6OjtK3/DMgxO/Qft0OPLYwkllDc7fyfTYl+nIoh9Xrlz5QIxxefJ6ltPpvcCioscLh68dIoTwNuCPGCeAA8QYNwIbAZYvXx5XrFhRUUO6u7up9DUqzb5MR6X9uGVbL5+98yH6B9qYaAJt3OlzKLOEbBFc/nDZ7WoE/k6mx75MRy37Mcsgfh9wfAhhMYXgfRHwgeIbQginANdRGLHvyrAtUu5MadvUJEvIpKaUWRCPMR4IIVwG3EGhxOzrMcYfhxCuBu6PMW4FrgFmAt8JIQA8HWM8N6s2SXkx5W1Tkywhk5pSpnXiMcbbgNsS19YVff+2LD9fyqspb5sKlpBJLcAd26QGUDx1Pr+rk96+STLHqXD6fDyWkEm5ZhCX6iw5dd7b10+g9Nh53G1Tkywhk1qCQVyqg77+Ac5Yf9e4SWsRxgTyCbdNTRrvFDIovLNr4FJTMIhLNbZlWy+9L/TT29cOjJ+0FimseY9MsU848oamLSGTND6DuFRj19zxGBctmnyTpQmT1pIsIZNakkFcqoExh5Usmvj+CZPWSrGETGpJBnEpY1M+63s8lpBJLc8gLmWgmt3Wyk5aA0vIJAEGcSl1qe+2VoolZJIwiEupqHTkDRUmrkF50+eAJWRS6zCIS1NUych7RFsIlSWulT19bgmZ1EoM4tIUlbvPeXHi2oLZg06fS5oyg7hUhTElY5NIJq51d3dP/iKnzyVNwiAuVSizkrFiTp9LKoNBXCpD5iVjSU6fSyqDQVyaRE1KxoqnzmcthD07JrjZ6XNJBQZxqYSalIyNSE6d79nB2DPMhjl9LqmIQVxKqKZkrOK9zouVnDovcRip0+eSEgziEtWNvKeUuNb/Aly7dJLM81gYeY9MsTt9LinBIK6WV+3Iu+rEte2bYc8vJln3xqlzSZMyiKsl1XzkXezOq+GYSya+x6lzSWUwiKvl1HzkDWM3bjlmvBvNPJdUPoO4ml7xqHt+Vycv7z9Q8TapUxp5u3GLpIwYxNXUkqPu3r5JAumwVEfeoQ3iJP9ocPpcUhUM4mo61ax3Q4Yj7wkDuNPnkqpnEFdTqWa9G1IYeRcrZ8tUcPpc0pQZxJV71Yy8uzo7OOKwaaPr5FMaeUMFJ44NC21On0uaMoO4cq3aTPOrzn19OqNuKD9xLbRDHCpMn89aBMvenc7nS2pZBnHlTl1rvEdUk7h2zoaD697lnCcuSZMwiKvhFQftWZ0dvLT/AAODhcBdkxrvJBPXJDUIg7gaWnK6vK9/oKzXpT7yLmbimqQGYRBXw6m2RGxE6iNvqDxxzbpvSTVgEFdDqbZELNORdzWJa06fS6oBg7jqruFH3tUkrklSDRjEVXNTTVTraAvMnDGNvpcHajPyNnFNUoMyiKumGjJRDSofeYOJa5LqziCuzDXkdHmxikbew0xck9QADOJK3VSny6FBR94mrklqMAZxTUnyrO6Vv/pqvvtAb8XT5cUaduRt4pqkBmMQV9VKndX9f+59upwq6kPUJFFtZNQ9ayHsf6m8zVoceUtqcAZxVaSvf4Az1t817vp2uQE88+nyEclR954d5b3OkbekHDCIa0LJ9e1Ljt9Hb187UP76dlJNpssrXe8GR96ScscgrkNMlJTW1z9ALCNwBw4dkdd0urxzNuzfC4P7C8+VG8AdeUvKIYN4i5ssaFeqs6Od809bwN0/eXY02a2m0+X9z5fZ0KNg+hEH18kdeUvKIYN4i0k7aEMN17dHVDtdPqKjE97xeYO2pNwziDe5LIJ2sczXtyGd6XLXuyU1IYN4k8k6aAcCsw/vyG59O6na6fJirndLalKtG8STtcM5HZ1lHbSTSWkLjxpk22+9fcrvO6GpTpe3dcBhR0L/C7n+bytJk2nNIF6qdviWj8PT98Lj32/owF7roJ0caXd3d0/5M8ZwulySqtKaQfzOq8fu2DXQD/d/ndHiqD07YMsfwO2fqdmIrtQWpsVZ3mlsaZqUeflXKRMFbafLJalsrRnE9/SM80SiBnpo4GBQySCoTzSq7u3r53/f+/TovdVuaZrUFEHb6XJJAlo1iM9aWP72m8UqDOoTjayrmQqvJoA3RdAGp8slqYRMg3gIYQ3wF0A7cH2McX3i+cOAvwJOA3YDF8YYn8qyTUAhCBSviRdaQ8VhMhHUD9z8Ufbe/EleFV/kF2Eu//fAhfQeOAMYO7JOYyq8lJoH7WSC4PFnw799a2rZ5ElOl0tSSZkF8RBCO/AV4O1AD3BfCGFrjPGRots+DLwQY/x/QggXAZ8HLsyqTaOWXcB9T73Aon+9hqPjc+wKc3npV1bxKz1bmDa4r+q3ncYgXbwIAebzHOvbv8b/2/5NutjLzjiXO4dOZlXbg8wPz7EzzuULBy5g69CZFX1GXbc0nbUQ/tMVcO3S0qPsPTsOzSuoltPlklSWLEfibwCeiDH+DCCEsAk4DygO4ucBVw1/fxPwP0MIIZazQfcUbNnWy2fv+xX6B/5i9FrH44F3tc3hU23fZn7YzQvxCI4M+5geDlT9OYeFQQ5jLwALw3N8KPwDITD6+JqO67iKvyo7yGeypWmpkfRIhn6pIP3ycweXIkqOsqv4T2fQlqSqZBnEFwDFC889wBvHuyfGeCCEsAeYAzyXYbu45o7HRjO8RwwMRb43dAbf44zRa+e2/YA/nLY5taA+EsBHTBbkvzT9Oq7mr3lVfJFd4dXsOHUtpx/3H/Dk1TCjBw5bCL1nQ/f3SwfhyR6XHEnfcLCBaUyFl2LQlqRUhKwGvSGE9wJrYoyXDD/+IPDGGONlRfc8PHxPz/Djfx++57nEe10KXAowb9680zZt2lRRW/bu3cvMmTNHHz/Uu6eqn6mLvRwTXqCDAwzSRhuREDKdNEgY+VdALT/zUHsPm8/MV3ZW+KpwcNOW9ulw5LGFf0C0sOTvpKpnX6bHvkxHFv24cuXKB2KMy5PXsxyJ9wKLih4vHL5W6p6eEMI0YBaFBLdDxBg3AhsBli9fHlesWFFRQ7q7uyl+zR+tv4vevv7xXzCuruE/BWmP1POg+4Q/YcVjV45/Q0cnnPSBht80p96Sv5Oqnn2ZHvsyHbXsxyyD+H3A8SGExRSC9UXABxL3bAX+C/BPwHuBu7JeDwdYu/oEPnvzQ4dMqXe0BQiMlnyVkkwie9WvfoALf/L20fXpTxy9jbOe/ipHx+f4jzCTI8M+2mM2WegNw6lxSaqbzIL48Br3ZcAdFErMvh5j/HEI4Wrg/hjjVuAG4K9DCE8Az1MI9JkbSQQrruFeu/qEMdeSO6ZNnkT2VuBTwPB4vZKksUaVDNKHz4VZixxlS1IDyLROPMZ4G3Bb4tq6ou/3Ae/Lsg3j+c1TFpQMyKmWZy27YOIAV2mQb+soZMelGfiTQTqZCJcM0t3dcPnD6X2+JKlqrbljW6OoNMivGv73z3iBv5rHjqQlKbcM4o1svCBv0JUkAW31boAkSaqOQVySpJwyiEuSlFMGcUmScsogLklSThnEJUnKKYO4JEk5ZRCXJCmnDOKSJOWUQVySpJwyiEuSlFMGcUmScsogLklSThnEJUnKKYO4JEk5FWKM9W5DRUIIzwI/r/Blc4HnMmhOK7Iv02E/pse+TI99mY4s+vFXYoyvTl7MXRCvRgjh/hjj8nq3oxnYl+mwH9NjX6bHvkxHLfvR6XRJknLKIC5JUk61ShDfWO8GNBH7Mh32Y3rsy/TYl+moWT+2xJq4JEnNqFVG4pIkNZ2mCuIhhDUhhMdCCE+EEK4o8fxhIYRvDz//zyGE4+rQzIZXRj9+MoTwSAhhewjhzhDCr9SjnXkwWV8W3Xd+CCGGEMwMHkc5fRlCuGD4d/PHIYRv1bqNeVDG3+/XhBDuDiFsG/47/hv1aGejCyF8PYSwK4Tw8DjPhxDChuF+3h5CODWThsQYm+IP0A78O/CfgOnAvwFLEvf8AfC14e8vAr5d73Y32p8y+3ElcPjw979vP1bfl8P3HQncA9wLLK93uxvxT5m/l8cD24DZw4+Prne7G+1Pmf24Efj94e+XAE/Vu92N+Ac4CzgVeHic538DuB0IwJuAf86iHc00En8D8ESM8Wcxxv3AJuC8xD3nAd8c/v4mYFUIIdSwjXkwaT/GGO+OMb48/PBeYGGN25gX5fxOAvwp8HlgXy0blzPl9OVHgK/EGF8AiDHuqnEb86CcfozAq4a/nwXsrGH7ciPGeA/w/AS3nAf8VSy4F+gKIRybdjuaKYgvAHYUPe4ZvlbynhjjAWAPMKcmrcuPcvqx2Icp/GtTY03al8NTbItijLfWsmE5VM7v5WuB14YQfhhCuDeEsKZmrcuPcvrxKuC3Qwg9wG3Ax2rTtKZT6f9LqzIt7TdU6wgh/DawHPj1erclj0IIbcCXgIvr3JRmMY3ClPoKCrND94QQTowx9tWzUTn0fuAbMcYvhhDeDPx1CGFpjHGo3g3TWM00Eu8FFhU9Xjh8reQ9IYRpFKaKdtekdflRTj8SQngb8EfAuTHGV2rUtryZrC+PBJYC3SGEpyism201ua2kcn4ve4CtMcaBGOOTwE8pBHUdVE4/fhjYDBBj/CdgBoW9wFWZsv5fOlXNFMTvA44PISwOIUynkLi2NXHPVuC/DH//XuCuOJyBoFGT9mMI4RTgOgoB3HXH8U3YlzHGPTHGuTHG42KMx1HILzg3xnh/fZrb0Mr5+72FwiicEMJcCtPrP6thG/OgnH58GlgFEEJ4HYUg/mxNW9kctgIfGs5SfxOwJ8b4TNof0jTT6THGAyGEy4A7KGRgfj3G+OMQwtXA/THGrcANFKaGnqCQkHBR/VrcmMrsx2uAmcB3hvMCn44xnlu3RjeoMvtSZSizL+8Azg4hPAIMAmtjjM60FSmzHz8F/K8QwuUUktwudrAzVgjhRgr/aJw7nD9wJdABEGP8GoV8gt8AngBeBn4nk3b430aSpHxqpul0SZJaikFckqScMohLkpRTBnFJknLKIC5JUk4ZxCWNK4SwKITwZAjhqOHHs4cfH1fnpknCIC5pAjHGHcBXgfXDl9YDG2OMT9WtUZJGWScuaUIhhA7gAeDrFE4KOznGOFDfVkmCJtqxTVI2YowDIYS1wN8BZxvApcbhdLqkcrwDeIbCgS2SGoRBXNKEQggnA2+ncMra5SGEY+vbIkkjDOKSxhUKJ9x8FfhvMcanKRx+8z/q2ypJIwzikibyEQqn1P398OO/BF4XQvj1OrZJ0jCz0yVJyilH4pIk5ZRBXJKknDKIS5KUUwZxSZJyyiAuSVJOGcQlScopg7gkSTllEJckKaf+f4zUsG1OnFAbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X,y)\n",
    "plt.scatter(X,y_pred);\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend(['Actual','Predicted']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cca308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
