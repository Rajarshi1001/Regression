{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fc08b3",
   "metadata": {},
   "source": [
    "Linear Regression using the loss function: \\begin{equation}|x - x(i)|^{1}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d42691",
   "metadata": {},
   "source": [
    "Here we use this loss function to fit the line:\\begin{equation}y = mx + c\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2a504",
   "metadata": {},
   "source": [
    "where we calculate the slope and the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bd83e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "\n",
    "X = np.linspace(2,98,100)\n",
    "y = np.linspace(2,100,100)\n",
    "X = X/np.max(X) #Normalising the data by dividing the elements with the max value\n",
    "y=y/np.max(y)  #Scaling the data by dividing the elements with the max value\n",
    "plt.scatter(X,y)\n",
    "plt.grid()\n",
    "plt.xlabel(\"input\");\n",
    "plt.ylabel(\"output\");\n",
    "plt.legend(['Actual']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480c7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper-function to calculate the gradient\n",
    "losses=[]\n",
    "def calculate_gradient():\n",
    "    lr = 0.0001\n",
    "    iterations = 2000\n",
    "    N = float(len(X))\n",
    "    w= 1.4\n",
    "    bias = 0.004\n",
    "    weights_and_bias = []\n",
    "    for i in range(iterations):\n",
    "        y_pred = X*w + bias\n",
    "        loss = sum((y_pred-y))/N\n",
    "        der_w = sum(X)/100\n",
    "        der_b = 1\n",
    "        w = w - lr*der_w\n",
    "        bias = bias - lr*der_b\n",
    "        losses.append(loss)\n",
    "        print(\"Training loss: {}\".format(loss))\n",
    "    weights_and_bias.append(w)\n",
    "    weights_and_bias.append(bias)\n",
    "    return weights_and_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3db28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2082857142857142\n",
      "Training loss: 0.2081596834652228\n",
      "Training loss: 0.2080336526447314\n",
      "Training loss: 0.20790762182423997\n",
      "Training loss: 0.20778159100374854\n",
      "Training loss: 0.20765556018325715\n",
      "Training loss: 0.20752952936276572\n",
      "Training loss: 0.20740349854227427\n",
      "Training loss: 0.20727746772178285\n",
      "Training loss: 0.20715143690129145\n",
      "Training loss: 0.20702540608080006\n",
      "Training loss: 0.20689937526030858\n",
      "Training loss: 0.20677334443981718\n",
      "Training loss: 0.20664731361932576\n",
      "Training loss: 0.20652128279883442\n",
      "Training loss: 0.20639525197834296\n",
      "Training loss: 0.20626922115785148\n",
      "Training loss: 0.20614319033736014\n",
      "Training loss: 0.20601715951686878\n",
      "Training loss: 0.2058911286963773\n",
      "Training loss: 0.20576509787588584\n",
      "Training loss: 0.20563906705539445\n",
      "Training loss: 0.205513036234903\n",
      "Training loss: 0.2053870054144116\n",
      "Training loss: 0.2052609745939201\n",
      "Training loss: 0.20513494377342872\n",
      "Training loss: 0.2050089129529373\n",
      "Training loss: 0.2048828821324459\n",
      "Training loss: 0.20475685131195448\n",
      "Training loss: 0.2046308204914632\n",
      "Training loss: 0.2045047896709717\n",
      "Training loss: 0.20437875885048012\n",
      "Training loss: 0.20425272802998878\n",
      "Training loss: 0.20412669720949722\n",
      "Training loss: 0.20400066638900594\n",
      "Training loss: 0.20387463556851454\n",
      "Training loss: 0.2037486047480231\n",
      "Training loss: 0.20362257392753164\n",
      "Training loss: 0.20349654310704024\n",
      "Training loss: 0.20337051228654882\n",
      "Training loss: 0.20324448146605742\n",
      "Training loss: 0.203118450645566\n",
      "Training loss: 0.2029924198250746\n",
      "Training loss: 0.20286638900458318\n",
      "Training loss: 0.20274035818409175\n",
      "Training loss: 0.20261432736360036\n",
      "Training loss: 0.202488296543109\n",
      "Training loss: 0.20236226572261753\n",
      "Training loss: 0.20223623490212617\n",
      "Training loss: 0.20211020408163466\n",
      "Training loss: 0.20198417326114326\n",
      "Training loss: 0.20185814244065178\n",
      "Training loss: 0.2017321116201604\n",
      "Training loss: 0.201606080799669\n",
      "Training loss: 0.2014800499791775\n",
      "Training loss: 0.20135401915868611\n",
      "Training loss: 0.20122798833819466\n",
      "Training loss: 0.2011019575177033\n",
      "Training loss: 0.20097592669721187\n",
      "Training loss: 0.20084989587672045\n",
      "Training loss: 0.20072386505622902\n",
      "Training loss: 0.20059783423573768\n",
      "Training loss: 0.20047180341524623\n",
      "Training loss: 0.20034577259475456\n",
      "Training loss: 0.2002197417742634\n",
      "Training loss: 0.2000937109537719\n",
      "Training loss: 0.19996768013328053\n",
      "Training loss: 0.19984164931278908\n",
      "Training loss: 0.19971561849229783\n",
      "Training loss: 0.1995895876718063\n",
      "Training loss: 0.19946355685131498\n",
      "Training loss: 0.1993375260308234\n",
      "Training loss: 0.199211495210332\n",
      "Training loss: 0.19908546438984065\n",
      "Training loss: 0.19895943356934914\n",
      "Training loss: 0.19883340274885777\n",
      "Training loss: 0.1987073719283663\n",
      "Training loss: 0.1985813411078749\n",
      "Training loss: 0.19845531028738367\n",
      "Training loss: 0.19832927946689197\n",
      "Training loss: 0.19820324864640063\n",
      "Training loss: 0.19807721782590926\n",
      "Training loss: 0.1979511870054178\n",
      "Training loss: 0.19782515618492635\n",
      "Training loss: 0.19769912536443499\n",
      "Training loss: 0.19757309454394353\n",
      "Training loss: 0.1974470637234521\n",
      "Training loss: 0.1973210329029607\n",
      "Training loss: 0.19719500208246932\n",
      "Training loss: 0.19706897126197784\n",
      "Training loss: 0.19694294044148652\n",
      "Training loss: 0.19681690962099502\n",
      "Training loss: 0.19669087880050362\n",
      "Training loss: 0.19656484798001206\n",
      "Training loss: 0.19643881715952077\n",
      "Training loss: 0.19631278633902935\n",
      "Training loss: 0.19618675551853798\n",
      "Training loss: 0.1960607246980465\n",
      "Training loss: 0.19593469387755513\n",
      "Training loss: 0.19580866305706357\n",
      "Training loss: 0.19568263223657226\n",
      "Training loss: 0.1955566014160809\n",
      "Training loss: 0.19543057059558944\n",
      "Training loss: 0.19530453977509799\n",
      "Training loss: 0.1951785089546066\n",
      "Training loss: 0.1950524781341152\n",
      "Training loss: 0.19492644731362369\n",
      "Training loss: 0.19480041649313237\n",
      "Training loss: 0.19467438567264078\n",
      "Training loss: 0.19454835485214944\n",
      "Training loss: 0.19442232403165816\n",
      "Training loss: 0.1942962932111666\n",
      "Training loss: 0.19417026239067525\n",
      "Training loss: 0.19404423157018372\n",
      "Training loss: 0.19391820074969232\n",
      "Training loss: 0.19379216992920092\n",
      "Training loss: 0.19366613910870945\n",
      "Training loss: 0.19354010828821813\n",
      "Training loss: 0.19341407746772668\n",
      "Training loss: 0.19328804664723528\n",
      "Training loss: 0.19316201582674386\n",
      "Training loss: 0.19303598500625246\n",
      "Training loss: 0.192909954185761\n",
      "Training loss: 0.19278392336526962\n",
      "Training loss: 0.1926578925447782\n",
      "Training loss: 0.1925318617242868\n",
      "Training loss: 0.19240583090379532\n",
      "Training loss: 0.19227980008330403\n",
      "Training loss: 0.19215376926281252\n",
      "Training loss: 0.19202773844232104\n",
      "Training loss: 0.19190170762182973\n",
      "Training loss: 0.1917756768013383\n",
      "Training loss: 0.1916496459808468\n",
      "Training loss: 0.19152361516035543\n",
      "Training loss: 0.191397584339864\n",
      "Training loss: 0.19127155351937258\n",
      "Training loss: 0.1911455226988811\n",
      "Training loss: 0.19101949187838968\n",
      "Training loss: 0.19089346105789828\n",
      "Training loss: 0.19076743023740686\n",
      "Training loss: 0.19064139941691544\n",
      "Training loss: 0.190515368596424\n",
      "Training loss: 0.19038933777593273\n",
      "Training loss: 0.19026330695544125\n",
      "Training loss: 0.19013727613494985\n",
      "Training loss: 0.1900112453144584\n",
      "Training loss: 0.1898852144939669\n",
      "Training loss: 0.18975918367347558\n",
      "Training loss: 0.18963315285298407\n",
      "Training loss: 0.18950712203249276\n",
      "Training loss: 0.1893810912120013\n",
      "Training loss: 0.18925506039150988\n",
      "Training loss: 0.1891290295710185\n",
      "Training loss: 0.18900299875052703\n",
      "Training loss: 0.18887696793003564\n",
      "Training loss: 0.18875093710954413\n",
      "Training loss: 0.18862490628905276\n",
      "Training loss: 0.1884988754685613\n",
      "Training loss: 0.18837284464806991\n",
      "Training loss: 0.1882468138275785\n",
      "Training loss: 0.18812078300708712\n",
      "Training loss: 0.18799475218659567\n",
      "Training loss: 0.18786872136610422\n",
      "Training loss: 0.18774269054561285\n",
      "Training loss: 0.18761665972512145\n",
      "Training loss: 0.18749062890462997\n",
      "Training loss: 0.18736459808413863\n",
      "Training loss: 0.18723856726364724\n",
      "Training loss: 0.18711253644315576\n",
      "Training loss: 0.18698650562266425\n",
      "Training loss: 0.18686047480217285\n",
      "Training loss: 0.1867344439816815\n",
      "Training loss: 0.1866084131611901\n",
      "Training loss: 0.18648238234069864\n",
      "Training loss: 0.18635635152020721\n",
      "Training loss: 0.1862303206997158\n",
      "Training loss: 0.18610428987922428\n",
      "Training loss: 0.185978259058733\n",
      "Training loss: 0.18585222823824155\n",
      "Training loss: 0.18572619741775015\n",
      "Training loss: 0.18560016659725867\n",
      "Training loss: 0.18547413577676727\n",
      "Training loss: 0.18534810495627588\n",
      "Training loss: 0.18522207413578456\n",
      "Training loss: 0.18509604331529306\n",
      "Training loss: 0.18497001249480158\n",
      "Training loss: 0.1848439816743102\n",
      "Training loss: 0.18471795085381895\n",
      "Training loss: 0.18459192003332736\n",
      "Training loss: 0.184465889212836\n",
      "Training loss: 0.18433985839234457\n",
      "Training loss: 0.18421382757185312\n",
      "Training loss: 0.18408779675136167\n",
      "Training loss: 0.1839617659308703\n",
      "Training loss: 0.18383573511037885\n",
      "Training loss: 0.18370970428988745\n",
      "Training loss: 0.18358367346939597\n",
      "Training loss: 0.18345764264890455\n",
      "Training loss: 0.18333161182841312\n",
      "Training loss: 0.18320558100792167\n",
      "Training loss: 0.18307955018743033\n",
      "Training loss: 0.1829535193669389\n",
      "Training loss: 0.1828274885464475\n",
      "Training loss: 0.18270145772595606\n",
      "Training loss: 0.18257542690546466\n",
      "Training loss: 0.18244939608497324\n",
      "Training loss: 0.18232336526448173\n",
      "Training loss: 0.1821973344439904\n",
      "Training loss: 0.182071303623499\n",
      "Training loss: 0.18194527280300754\n",
      "Training loss: 0.18181924198251623\n",
      "Training loss: 0.18169321116202472\n",
      "Training loss: 0.18156718034153332\n",
      "Training loss: 0.18144114952104196\n",
      "Training loss: 0.1813151187005505\n",
      "Training loss: 0.18118908788005902\n",
      "Training loss: 0.18106305705956768\n",
      "Training loss: 0.18093702623907615\n",
      "Training loss: 0.18081099541858478\n",
      "Training loss: 0.18068496459809336\n",
      "Training loss: 0.18055893377760196\n",
      "Training loss: 0.18043290295711054\n",
      "Training loss: 0.18030687213661906\n",
      "Training loss: 0.18018084131612766\n",
      "Training loss: 0.1800548104956362\n",
      "Training loss: 0.17992877967514487\n",
      "Training loss: 0.17980274885465344\n",
      "Training loss: 0.17967671803416205\n",
      "Training loss: 0.17955068721367057\n",
      "Training loss: 0.17942465639317917\n",
      "Training loss: 0.1792986255726878\n",
      "Training loss: 0.17917259475219638\n",
      "Training loss: 0.1790465639317049\n",
      "Training loss: 0.17892053311121348\n",
      "Training loss: 0.17879450229072213\n",
      "Training loss: 0.17866847147023066\n",
      "Training loss: 0.17854244064973934\n",
      "Training loss: 0.17841640982924786\n",
      "Training loss: 0.17829037900875636\n",
      "Training loss: 0.17816434818826493\n",
      "Training loss: 0.17803831736777354\n",
      "Training loss: 0.17791228654728214\n",
      "Training loss: 0.17778625572679072\n",
      "Training loss: 0.1776602249062994\n",
      "Training loss: 0.17753419408580787\n",
      "Training loss: 0.1774081632653164\n",
      "Training loss: 0.177282132444825\n",
      "Training loss: 0.1771561016243336\n",
      "Training loss: 0.17703007080384223\n",
      "Training loss: 0.17690403998335077\n",
      "Training loss: 0.17677800916285943\n",
      "Training loss: 0.176651978342368\n",
      "Training loss: 0.17652594752187653\n",
      "Training loss: 0.17639991670138513\n",
      "Training loss: 0.17627388588089374\n",
      "Training loss: 0.17614785506040223\n",
      "Training loss: 0.1760218242399109\n",
      "Training loss: 0.1758957934194194\n",
      "Training loss: 0.17576976259892801\n",
      "Training loss: 0.17564373177843656\n",
      "Training loss: 0.17551770095794517\n",
      "Training loss: 0.17539167013745371\n",
      "Training loss: 0.17526563931696232\n",
      "Training loss: 0.17513960849647087\n",
      "Training loss: 0.17501357767597947\n",
      "Training loss: 0.17488754685548805\n",
      "Training loss: 0.1747615160349966\n",
      "Training loss: 0.17463548521450523\n",
      "Training loss: 0.1745094543940138\n",
      "Training loss: 0.17438342357352243\n",
      "Training loss: 0.17425739275303095\n",
      "Training loss: 0.17413136193253956\n",
      "Training loss: 0.17400533111204816\n",
      "Training loss: 0.1738793002915568\n",
      "Training loss: 0.17375326947106529\n",
      "Training loss: 0.1736272386505739\n",
      "Training loss: 0.17350120783008247\n",
      "Training loss: 0.173375177009591\n",
      "Training loss: 0.17324914618909967\n",
      "Training loss: 0.17312311536860828\n",
      "Training loss: 0.17299708454811685\n",
      "Training loss: 0.1728710537276252\n",
      "Training loss: 0.17274502290713387\n",
      "Training loss: 0.1726189920866425\n",
      "Training loss: 0.17249296126615113\n",
      "Training loss: 0.17236693044565968\n",
      "Training loss: 0.17224089962516828\n",
      "Training loss: 0.17211486880467688\n",
      "Training loss: 0.17198883798418543\n",
      "Training loss: 0.17186280716369395\n",
      "Training loss: 0.1717367763432026\n",
      "Training loss: 0.17161074552271113\n",
      "Training loss: 0.17148471470221976\n",
      "Training loss: 0.17135868388172834\n",
      "Training loss: 0.17123265306123694\n",
      "Training loss: 0.17110662224074552\n",
      "Training loss: 0.1709805914202542\n",
      "Training loss: 0.17085456059976262\n",
      "Training loss: 0.17072852977927114\n",
      "Training loss: 0.17060249895877988\n",
      "Training loss: 0.17047646813828846\n",
      "Training loss: 0.17035043731779706\n",
      "Training loss: 0.17022440649730552\n",
      "Training loss: 0.17009837567681418\n",
      "Training loss: 0.16997234485632273\n",
      "Training loss: 0.1698463140358313\n",
      "Training loss: 0.16972028321533986\n",
      "Training loss: 0.1695942523948484\n",
      "Training loss: 0.1694682215743571\n",
      "Training loss: 0.1693421907538656\n",
      "Training loss: 0.16921615993337422\n",
      "Training loss: 0.1690901291128828\n",
      "Training loss: 0.16896409829239137\n",
      "Training loss: 0.1688380674719\n",
      "Training loss: 0.16871203665140855\n",
      "Training loss: 0.16858600583091718\n",
      "Training loss: 0.16845997501042576\n",
      "Training loss: 0.16833394418993433\n",
      "Training loss: 0.16820791336944285\n",
      "Training loss: 0.16808188254895154\n",
      "Training loss: 0.16795585172846006\n",
      "Training loss: 0.16782982090796864\n",
      "Training loss: 0.16770379008747727\n",
      "Training loss: 0.16757775926698573\n",
      "Training loss: 0.16745172844649436\n",
      "Training loss: 0.1673256976260029\n",
      "Training loss: 0.1671996668055116\n",
      "Training loss: 0.16707363598502006\n",
      "Training loss: 0.16694760516452867\n",
      "Training loss: 0.16682157434403724\n",
      "Training loss: 0.16669554352354576\n",
      "Training loss: 0.16656951270305437\n",
      "Training loss: 0.16644348188256297\n",
      "Training loss: 0.16631745106207155\n",
      "Training loss: 0.16619142024158015\n",
      "Training loss: 0.16606538942108875\n",
      "Training loss: 0.1659393586005974\n",
      "Training loss: 0.1658133277801059\n",
      "Training loss: 0.16568729695961448\n",
      "Training loss: 0.1655612661391231\n",
      "Training loss: 0.16543523531863166\n",
      "Training loss: 0.16530920449814027\n",
      "Training loss: 0.1651831736776488\n",
      "Training loss: 0.1650571428571574\n",
      "Training loss: 0.164931112036666\n",
      "Training loss: 0.16480508121617465\n",
      "Training loss: 0.16467905039568312\n",
      "Training loss: 0.16455301957519172\n",
      "Training loss: 0.16442698875470033\n",
      "Training loss: 0.1643009579342089\n",
      "Training loss: 0.1641749271137174\n",
      "Training loss: 0.16404889629322603\n",
      "Training loss: 0.16392286547273457\n",
      "Training loss: 0.16379683465224318\n",
      "Training loss: 0.16367080383175178\n",
      "Training loss: 0.16354477301126036\n",
      "Training loss: 0.163418742190769\n",
      "Training loss: 0.16329271137027757\n",
      "Training loss: 0.16316668054978611\n",
      "Training loss: 0.16304064972929466\n",
      "Training loss: 0.16291461890880327\n",
      "Training loss: 0.1627885880883118\n",
      "Training loss: 0.16266255726782042\n",
      "Training loss: 0.162536526447329\n",
      "Training loss: 0.16241049562683751\n",
      "Training loss: 0.16228446480634612\n",
      "Training loss: 0.1621584339858547\n",
      "Training loss: 0.16203240316536324\n",
      "Training loss: 0.16190637234487185\n",
      "Training loss: 0.16178034152438045\n",
      "Training loss: 0.16165431070388903\n",
      "Training loss: 0.16152827988339757\n",
      "Training loss: 0.16140224906290623\n",
      "Training loss: 0.16127621824241475\n",
      "Training loss: 0.16115018742192333\n",
      "Training loss: 0.16102415660143188\n",
      "Training loss: 0.16089812578094048\n",
      "Training loss: 0.16077209496044909\n",
      "Training loss: 0.16064606413995772\n",
      "Training loss: 0.16052003331946618\n",
      "Training loss: 0.16039400249897476\n",
      "Training loss: 0.16026797167848336\n",
      "Training loss: 0.16014194085799194\n",
      "Training loss: 0.16001591003750046\n",
      "Training loss: 0.1598898792170091\n",
      "Training loss: 0.15976384839651764\n",
      "Training loss: 0.15963781757602624\n",
      "Training loss: 0.15951178675553485\n",
      "Training loss: 0.15938575593504342\n",
      "Training loss: 0.15925972511455203\n",
      "Training loss: 0.15913369429406055\n",
      "Training loss: 0.15900766347356915\n",
      "Training loss: 0.1588816326530778\n",
      "Training loss: 0.15875560183258636\n",
      "Training loss: 0.15862957101209493\n",
      "Training loss: 0.15850354019160345\n",
      "Training loss: 0.158377509371112\n",
      "Training loss: 0.15825147855062063\n",
      "Training loss: 0.15812544773012915\n",
      "Training loss: 0.15799941690963779\n",
      "Training loss: 0.15787338608914633\n",
      "Training loss: 0.1577473552686549\n",
      "Training loss: 0.15762132444816346\n",
      "Training loss: 0.1574952936276721\n",
      "Training loss: 0.15736926280718067\n",
      "Training loss: 0.15724323198668924\n",
      "Training loss: 0.15711720116619782\n",
      "Training loss: 0.15699117034570642\n",
      "Training loss: 0.15686513952521502\n",
      "Training loss: 0.15673910870472352\n",
      "Training loss: 0.15661307788423215\n",
      "Training loss: 0.15648704706374067\n",
      "Training loss: 0.15636101624324927\n",
      "Training loss: 0.15623498542275785\n",
      "Training loss: 0.15610895460226643\n",
      "Training loss: 0.15598292378177497\n",
      "Training loss: 0.15585689296128355\n",
      "Training loss: 0.15573086214079215\n",
      "Training loss: 0.15560483132030076\n",
      "Training loss: 0.15547880049980933\n",
      "Training loss: 0.15535276967931794\n",
      "Training loss: 0.15522673885882646\n",
      "Training loss: 0.15510070803833512\n",
      "Training loss: 0.15497467721784364\n",
      "Training loss: 0.15484864639735213\n",
      "Training loss: 0.1547226155768608\n",
      "Training loss: 0.15459658475636937\n",
      "Training loss: 0.1544705539358779\n",
      "Training loss: 0.1543445231153865\n",
      "Training loss: 0.15421849229489507\n",
      "Training loss: 0.15409246147440364\n",
      "Training loss: 0.15396643065391222\n",
      "Training loss: 0.15384039983342077\n",
      "Training loss: 0.15371436901292937\n",
      "Training loss: 0.15358833819243797\n",
      "Training loss: 0.1534623073719466\n",
      "Training loss: 0.15333627655145515\n",
      "Training loss: 0.15321024573096373\n",
      "Training loss: 0.15308421491047233\n",
      "Training loss: 0.1529581840899807\n",
      "Training loss: 0.15283215326948943\n",
      "Training loss: 0.15270612244899803\n",
      "Training loss: 0.1525800916285066\n",
      "Training loss: 0.15245406080801516\n",
      "Training loss: 0.15232802998752373\n",
      "Training loss: 0.15220199916703225\n",
      "Training loss: 0.15207596834654089\n",
      "Training loss: 0.15194993752604943\n",
      "Training loss: 0.15182390670555804\n",
      "Training loss: 0.1516978758850666\n",
      "Training loss: 0.15157184506457522\n",
      "Training loss: 0.1514458142440838\n",
      "Training loss: 0.1513197834235924\n",
      "Training loss: 0.15119375260310097\n",
      "Training loss: 0.1510677217826094\n",
      "Training loss: 0.1509416909621181\n",
      "Training loss: 0.15081566014162667\n",
      "Training loss: 0.15068962932113525\n",
      "Training loss: 0.15056359850064388\n",
      "Training loss: 0.15043756768015237\n",
      "Training loss: 0.15031153685966095\n",
      "Training loss: 0.15018550603916953\n",
      "Training loss: 0.1500594752186781\n",
      "Training loss: 0.14993344439818668\n",
      "Training loss: 0.14980741357769525\n",
      "Training loss: 0.14968138275720386\n",
      "Training loss: 0.14955535193671243\n",
      "Training loss: 0.14942932111622104\n",
      "Training loss: 0.14930329029572967\n",
      "Training loss: 0.1491772594752383\n",
      "Training loss: 0.14905122865474682\n",
      "Training loss: 0.1489251978342553\n",
      "Training loss: 0.14879916701376394\n",
      "Training loss: 0.14867313619327255\n",
      "Training loss: 0.1485471053727811\n",
      "Training loss: 0.14842107455228962\n",
      "Training loss: 0.14829504373179822\n",
      "Training loss: 0.1481690129113068\n",
      "Training loss: 0.14804298209081537\n",
      "Training loss: 0.1479169512703239\n",
      "Training loss: 0.14779092044983252\n",
      "Training loss: 0.1476648896293411\n",
      "Training loss: 0.1475388588088497\n",
      "Training loss: 0.14741282798835822\n",
      "Training loss: 0.14728679716786688\n",
      "Training loss: 0.14716076634737543\n",
      "Training loss: 0.14703473552688404\n",
      "Training loss: 0.14690870470639258\n",
      "Training loss: 0.14678267388590116\n",
      "Training loss: 0.14665664306540974\n",
      "Training loss: 0.1465306122449183\n",
      "Training loss: 0.14640458142442686\n",
      "Training loss: 0.14627855060393546\n",
      "Training loss: 0.14615251978344404\n",
      "Training loss: 0.14602648896295262\n",
      "Training loss: 0.14590045814246116\n",
      "Training loss: 0.14577442732196977\n",
      "Training loss: 0.14564839650147832\n",
      "Training loss: 0.14552236568098692\n",
      "Training loss: 0.14539633486049555\n",
      "Training loss: 0.1452703040400041\n",
      "Training loss: 0.1451442732195127\n",
      "Training loss: 0.14501824239902128\n",
      "Training loss: 0.14489221157852988\n",
      "Training loss: 0.14476618075803838\n",
      "Training loss: 0.14464014993754698\n",
      "Training loss: 0.14451411911705556\n",
      "Training loss: 0.14438808829656413\n",
      "Training loss: 0.1442620574760727\n",
      "Training loss: 0.14413602665558126\n",
      "Training loss: 0.14400999583508986\n",
      "Training loss: 0.14388396501459844\n",
      "Training loss: 0.14375793419410704\n",
      "Training loss: 0.1436319033736156\n",
      "Training loss: 0.14350587255312416\n",
      "Training loss: 0.14337984173263277\n",
      "Training loss: 0.1432538109121413\n",
      "Training loss: 0.14312778009164992\n",
      "Training loss: 0.14300174927115858\n",
      "Training loss: 0.14287571845066702\n",
      "Training loss: 0.14274968763017565\n",
      "Training loss: 0.1426236568096842\n",
      "Training loss: 0.1424976259891928\n",
      "Training loss: 0.14237159516870135\n",
      "Training loss: 0.14224556434820992\n",
      "Training loss: 0.14211953352771847\n",
      "Training loss: 0.14199350270722708\n",
      "Training loss: 0.14186747188673562\n",
      "Training loss: 0.14174144106624426\n",
      "Training loss: 0.14161541024575283\n",
      "Training loss: 0.14148937942526146\n",
      "Training loss: 0.14136334860476998\n",
      "Training loss: 0.14123731778427856\n",
      "Training loss: 0.14111128696378727\n",
      "Training loss: 0.14098525614329574\n",
      "Training loss: 0.14085922532280434\n",
      "Training loss: 0.14073319450231284\n",
      "Training loss: 0.14060716368182144\n",
      "Training loss: 0.14048113286133007\n",
      "Training loss: 0.14035510204083856\n",
      "Training loss: 0.14022907122034717\n",
      "Training loss: 0.14010304039985572\n",
      "Training loss: 0.13997700957936435\n",
      "Training loss: 0.13985097875887292\n",
      "Training loss: 0.13972494793838153\n",
      "Training loss: 0.13959891711789005\n",
      "Training loss: 0.13947288629739862\n",
      "Training loss: 0.13934685547690726\n",
      "Training loss: 0.13922082465641578\n",
      "Training loss: 0.13909479383592435\n",
      "Training loss: 0.13896876301543298\n",
      "Training loss: 0.13884273219494156\n",
      "Training loss: 0.1387167013744501\n",
      "Training loss: 0.13859067055395868\n",
      "Training loss: 0.1384646397334673\n",
      "Training loss: 0.13833860891297584\n",
      "Training loss: 0.13821257809248444\n",
      "Training loss: 0.13808654727199296\n",
      "Training loss: 0.13796051645150156\n",
      "Training loss: 0.13783448563101014\n",
      "Training loss: 0.1377084548105187\n",
      "Training loss: 0.1375824239900273\n",
      "Training loss: 0.13745639316953587\n",
      "Training loss: 0.13733036234904442\n",
      "Training loss: 0.13720433152855305\n",
      "Training loss: 0.13707830070806165\n",
      "Training loss: 0.13695226988757025\n",
      "Training loss: 0.13682623906707878\n",
      "Training loss: 0.13670020824658735\n",
      "Training loss: 0.13657417742609596\n",
      "Training loss: 0.1364481466056045\n",
      "Training loss: 0.13632211578511305\n",
      "Training loss: 0.13619608496462166\n",
      "Training loss: 0.1360700541441302\n",
      "Training loss: 0.13594402332363884\n",
      "Training loss: 0.13581799250314736\n",
      "Training loss: 0.13569196168265596\n",
      "Training loss: 0.13556593086216456\n",
      "Training loss: 0.13543990004167308\n",
      "Training loss: 0.13531386922118172\n",
      "Training loss: 0.1351878384006903\n",
      "Training loss: 0.13506180758019887\n",
      "Training loss: 0.13493577675970742\n",
      "Training loss: 0.13480974593921602\n",
      "Training loss: 0.1346837151187246\n",
      "Training loss: 0.13455768429823317\n",
      "Training loss: 0.13443165347774177\n",
      "Training loss: 0.13430562265725032\n",
      "Training loss: 0.1341795918367589\n",
      "Training loss: 0.13405356101626745\n",
      "Training loss: 0.13392753019577608\n",
      "Training loss: 0.13380149937528463\n",
      "Training loss: 0.1336754685547932\n",
      "Training loss: 0.13354943773430172\n",
      "Training loss: 0.13342340691381033\n",
      "Training loss: 0.13329737609331896\n",
      "Training loss: 0.13317134527282753\n",
      "Training loss: 0.13304531445233603\n",
      "Training loss: 0.1329192836318447\n",
      "Training loss: 0.13279325281135324\n",
      "Training loss: 0.13266722199086184\n",
      "Training loss: 0.13254119117037041\n",
      "Training loss: 0.13241516034987902\n",
      "Training loss: 0.13228912952938754\n",
      "Training loss: 0.13216309870889614\n",
      "Training loss: 0.1320370678884047\n",
      "Training loss: 0.13191103706791332\n",
      "Training loss: 0.13178500624742184\n",
      "Training loss: 0.13165897542693045\n",
      "Training loss: 0.131532944606439\n",
      "Training loss: 0.1314069137859476\n",
      "Training loss: 0.1312808829654562\n",
      "Training loss: 0.13115485214496464\n",
      "Training loss: 0.13102882132447335\n",
      "Training loss: 0.13090279050398196\n",
      "Training loss: 0.13077675968349048\n",
      "Training loss: 0.1306507288629991\n",
      "Training loss: 0.13052469804250763\n",
      "Training loss: 0.1303986672220162\n",
      "Training loss: 0.13027263640152478\n",
      "Training loss: 0.13014660558103336\n",
      "Training loss: 0.13002057476054193\n",
      "Training loss: 0.1298945439400505\n",
      "Training loss: 0.1297685131195591\n",
      "Training loss: 0.12964248229906766\n",
      "Training loss: 0.12951645147857627\n",
      "Training loss: 0.12939042065808487\n",
      "Training loss: 0.12926438983759334\n",
      "Training loss: 0.129138359017102\n",
      "Training loss: 0.12901232819661057\n",
      "Training loss: 0.12888629737611915\n",
      "Training loss: 0.12876026655562772\n",
      "Training loss: 0.1286342357351363\n",
      "Training loss: 0.12850820491464485\n",
      "Training loss: 0.1283821740941535\n",
      "Training loss: 0.128256143273662\n",
      "Training loss: 0.1281301124531706\n",
      "Training loss: 0.12800408163267915\n",
      "Training loss: 0.12787805081218775\n",
      "Training loss: 0.1277520199916963\n",
      "Training loss: 0.1276259891712049\n",
      "Training loss: 0.12749995835071348\n",
      "Training loss: 0.12737392753022206\n",
      "Training loss: 0.12724789670973066\n",
      "Training loss: 0.1271218658892393\n",
      "Training loss: 0.12699583506874781\n",
      "Training loss: 0.12686980424825645\n",
      "Training loss: 0.12674377342776494\n",
      "Training loss: 0.12661774260727354\n",
      "Training loss: 0.12649171178678217\n",
      "Training loss: 0.12636568096629072\n",
      "Training loss: 0.12623965014579927\n",
      "Training loss: 0.12611361932530785\n",
      "Training loss: 0.1259875885048164\n",
      "Training loss: 0.125861557684325\n",
      "Training loss: 0.12573552686383355\n",
      "Training loss: 0.12560949604334215\n",
      "Training loss: 0.12548346522285075\n",
      "Training loss: 0.1253574344023593\n",
      "Training loss: 0.12523140358186793\n",
      "Training loss: 0.12510537276137648\n",
      "Training loss: 0.12497934194088506\n",
      "Training loss: 0.12485331112039363\n",
      "Training loss: 0.1247272802999022\n",
      "Training loss: 0.12460124947941079\n",
      "Training loss: 0.12447521865891935\n",
      "Training loss: 0.12434918783842797\n",
      "Training loss: 0.12422315701793649\n",
      "Training loss: 0.12409712619744509\n",
      "Training loss: 0.12397109537695367\n",
      "Training loss: 0.12384506455646224\n",
      "Training loss: 0.12371903373597078\n",
      "Training loss: 0.12359300291547941\n",
      "Training loss: 0.12346697209498793\n",
      "Training loss: 0.12334094127449653\n",
      "Training loss: 0.12321491045400514\n",
      "Training loss: 0.12308887963351375\n",
      "Training loss: 0.12296284881302232\n",
      "Training loss: 0.12283681799253088\n",
      "Training loss: 0.12271078717203948\n",
      "Training loss: 0.12258475635154803\n",
      "Training loss: 0.1224587255310566\n",
      "Training loss: 0.12233269471056517\n",
      "Training loss: 0.12220666389007376\n",
      "Training loss: 0.12208063306958236\n",
      "Training loss: 0.1219546022490909\n",
      "Training loss: 0.12182857142859949\n",
      "Training loss: 0.12170254060810805\n",
      "Training loss: 0.12157650978761662\n",
      "Training loss: 0.12145047896712519\n",
      "Training loss: 0.12132444814663378\n",
      "Training loss: 0.12119841732614244\n",
      "Training loss: 0.12107238650565093\n",
      "Training loss: 0.1209463556851595\n",
      "Training loss: 0.12082032486466812\n",
      "Training loss: 0.12069429404417667\n",
      "Training loss: 0.12056826322368526\n",
      "Training loss: 0.12044223240319385\n",
      "Training loss: 0.12031620158270244\n",
      "Training loss: 0.120190170762211\n",
      "Training loss: 0.12006413994171955\n",
      "Training loss: 0.11993810912122814\n",
      "Training loss: 0.11981207830073673\n",
      "Training loss: 0.11968604748024532\n",
      "Training loss: 0.11956001665975385\n",
      "Training loss: 0.11943398583926246\n",
      "Training loss: 0.11930795501877105\n",
      "Training loss: 0.11918192419827961\n",
      "Training loss: 0.11905589337778823\n",
      "Training loss: 0.11892986255729678\n",
      "Training loss: 0.11880383173680535\n",
      "Training loss: 0.11867780091631396\n",
      "Training loss: 0.11855177009582249\n",
      "Training loss: 0.11842573927533108\n",
      "Training loss: 0.11829970845483966\n",
      "Training loss: 0.11817367763434822\n",
      "Training loss: 0.11804764681385681\n",
      "Training loss: 0.11792161599336541\n",
      "Training loss: 0.11779558517287395\n",
      "Training loss: 0.11766955435238252\n",
      "Training loss: 0.11754352353189107\n",
      "Training loss: 0.11741749271139967\n",
      "Training loss: 0.11729146189090825\n",
      "Training loss: 0.11716543107041684\n",
      "Training loss: 0.11703940024992544\n",
      "Training loss: 0.11691336942943402\n",
      "Training loss: 0.1167873386089426\n",
      "Training loss: 0.11666130778845117\n",
      "Training loss: 0.11653527696795975\n",
      "Training loss: 0.11640924614746836\n",
      "Training loss: 0.1162832153269769\n",
      "Training loss: 0.11615718450648543\n",
      "Training loss: 0.11603115368599402\n",
      "Training loss: 0.11590512286550263\n",
      "Training loss: 0.11577909204501126\n",
      "Training loss: 0.11565306122451977\n",
      "Training loss: 0.11552703040402834\n",
      "Training loss: 0.11540099958353686\n",
      "Training loss: 0.1152749687630455\n",
      "Training loss: 0.11514893794255414\n",
      "Training loss: 0.11502290712206266\n",
      "Training loss: 0.11489687630157125\n",
      "Training loss: 0.11477084548107985\n",
      "Training loss: 0.11464481466058839\n",
      "Training loss: 0.11451878384009699\n",
      "Training loss: 0.1143927530196056\n",
      "Training loss: 0.11426672219911413\n",
      "Training loss: 0.11414069137862275\n",
      "Training loss: 0.1140146605581313\n",
      "Training loss: 0.11388862973763984\n",
      "Training loss: 0.11376259891714842\n",
      "Training loss: 0.11363656809665695\n",
      "Training loss: 0.11351053727616557\n",
      "Training loss: 0.11338450645567417\n",
      "Training loss: 0.11325847563518276\n",
      "Training loss: 0.1131324448146913\n",
      "Training loss: 0.11300641399419993\n",
      "Training loss: 0.11288038317370852\n",
      "Training loss: 0.11275435235321708\n",
      "Training loss: 0.11262832153272567\n",
      "Training loss: 0.11250229071223422\n",
      "Training loss: 0.11237625989174281\n",
      "Training loss: 0.11225022907125137\n",
      "Training loss: 0.11212419825075993\n",
      "Training loss: 0.11199816743026851\n",
      "Training loss: 0.1118721366097771\n",
      "Training loss: 0.11174610578928566\n",
      "Training loss: 0.11162007496879423\n",
      "Training loss: 0.11149404414830283\n",
      "Training loss: 0.11136801332781139\n",
      "Training loss: 0.11124198250731993\n",
      "Training loss: 0.11111595168682856\n",
      "Training loss: 0.11098992086633716\n",
      "Training loss: 0.11086389004584575\n",
      "Training loss: 0.11073785922535433\n",
      "Training loss: 0.11061182840486287\n",
      "Training loss: 0.11048579758437142\n",
      "Training loss: 0.11035976676388007\n",
      "Training loss: 0.1102337359433886\n",
      "Training loss: 0.11010770512289719\n",
      "Training loss: 0.10998167430240574\n",
      "Training loss: 0.10985564348191429\n",
      "Training loss: 0.1097296126614229\n",
      "Training loss: 0.1096035818409315\n",
      "Training loss: 0.10947755102044006\n",
      "Training loss: 0.10935152019994854\n",
      "Training loss: 0.1092254893794572\n",
      "Training loss: 0.10909945855896583\n",
      "Training loss: 0.10897342773847439\n",
      "Training loss: 0.10884739691798301\n",
      "Training loss: 0.10872136609749157\n",
      "Training loss: 0.10859533527700012\n",
      "Training loss: 0.1084693044565087\n",
      "Training loss: 0.10834327363601727\n",
      "Training loss: 0.10821724281552587\n",
      "Training loss: 0.10809121199503442\n",
      "Training loss: 0.10796518117454296\n",
      "Training loss: 0.10783915035405155\n",
      "Training loss: 0.10771311953356016\n",
      "Training loss: 0.10758708871306873\n",
      "Training loss: 0.10746105789257722\n",
      "Training loss: 0.10733502707208586\n",
      "Training loss: 0.10720899625159447\n",
      "Training loss: 0.10708296543110304\n",
      "Training loss: 0.1069569346106116\n",
      "Training loss: 0.1068309037901202\n",
      "Training loss: 0.10670487296962879\n",
      "Training loss: 0.10657884214913736\n",
      "Training loss: 0.10645281132864591\n",
      "Training loss: 0.10632678050815451\n",
      "Training loss: 0.10620074968766312\n",
      "Training loss: 0.10607471886717164\n",
      "Training loss: 0.10594868804668021\n",
      "Training loss: 0.1058226572261888\n",
      "Training loss: 0.10569662640569738\n",
      "Training loss: 0.10557059558520589\n",
      "Training loss: 0.1054445647647145\n",
      "Training loss: 0.10531853394422308\n",
      "Training loss: 0.10519250312373168\n",
      "Training loss: 0.10506647230324032\n",
      "Training loss: 0.10494044148274892\n",
      "Training loss: 0.10481441066225747\n",
      "Training loss: 0.10468837984176606\n",
      "Training loss: 0.10456234902127459\n",
      "Training loss: 0.10443631820078317\n",
      "Training loss: 0.10431028738029177\n",
      "Training loss: 0.10418425655980032\n",
      "Training loss: 0.10405822573930895\n",
      "Training loss: 0.10393219491881749\n",
      "Training loss: 0.10380616409832605\n",
      "Training loss: 0.10368013327783458\n",
      "Training loss: 0.10355410245734321\n",
      "Training loss: 0.10342807163685182\n",
      "Training loss: 0.10330204081636031\n",
      "Training loss: 0.10317600999586897\n",
      "Training loss: 0.10304997917537753\n",
      "Training loss: 0.10292394835488614\n",
      "Training loss: 0.10279791753439473\n",
      "Training loss: 0.10267188671390329\n",
      "Training loss: 0.10254585589341185\n",
      "Training loss: 0.10241982507292041\n",
      "Training loss: 0.102293794252429\n",
      "Training loss: 0.10216776343193759\n",
      "Training loss: 0.10204173261144614\n",
      "Training loss: 0.10191570179095474\n",
      "Training loss: 0.10178967097046328\n",
      "Training loss: 0.10166364014997185\n",
      "Training loss: 0.1015376093294804\n",
      "Training loss: 0.10141157850898902\n",
      "Training loss: 0.10128554768849762\n",
      "Training loss: 0.10115951686800617\n",
      "Training loss: 0.10103348604751472\n",
      "Training loss: 0.10090745522702338\n",
      "Training loss: 0.1007814244065319\n",
      "Training loss: 0.10065539358604055\n",
      "Training loss: 0.10052936276554907\n",
      "Training loss: 0.10040333194505767\n",
      "Training loss: 0.10027730112456623\n",
      "Training loss: 0.10015127030407481\n",
      "Training loss: 0.10002523948358338\n",
      "Training loss: 0.09989920866309196\n",
      "Training loss: 0.09977317784260055\n",
      "Training loss: 0.09964714702210913\n",
      "Training loss: 0.09952111620161766\n",
      "Training loss: 0.09939508538112626\n",
      "Training loss: 0.09926905456063485\n",
      "Training loss: 0.0991430237401434\n",
      "Training loss: 0.099016992919652\n",
      "Training loss: 0.09889096209916057\n",
      "Training loss: 0.09876493127866916\n",
      "Training loss: 0.09863890045817775\n",
      "Training loss: 0.09851286963768632\n",
      "Training loss: 0.09838683881719486\n",
      "Training loss: 0.09826080799670349\n",
      "Training loss: 0.09813477717621205\n",
      "Training loss: 0.09800874635572064\n",
      "Training loss: 0.09788271553522918\n",
      "Training loss: 0.09775668471473781\n",
      "Training loss: 0.09763065389424637\n",
      "Training loss: 0.09750462307375493\n",
      "Training loss: 0.0973785922532635\n",
      "Training loss: 0.09725256143277211\n",
      "Training loss: 0.09712653061228067\n",
      "Training loss: 0.09700049979178921\n",
      "Training loss: 0.09687446897129785\n",
      "Training loss: 0.09674843815080642\n",
      "Training loss: 0.09662240733031499\n",
      "Training loss: 0.09649637650982357\n",
      "Training loss: 0.09637034568933213\n",
      "Training loss: 0.0962443148688407\n",
      "Training loss: 0.09611828404834931\n",
      "Training loss: 0.09599225322785784\n",
      "Training loss: 0.09586622240736642\n",
      "Training loss: 0.09574019158687501\n",
      "Training loss: 0.09561416076638364\n",
      "Training loss: 0.09548812994589215\n",
      "Training loss: 0.09536209912540075\n",
      "Training loss: 0.0952360683049093\n",
      "Training loss: 0.09511003748441788\n",
      "Training loss: 0.09498400666392648\n",
      "Training loss: 0.09485797584343507\n",
      "Training loss: 0.09473194502294366\n",
      "Training loss: 0.09460591420245222\n",
      "Training loss: 0.09447988338196081\n",
      "Training loss: 0.09435385256146935\n",
      "Training loss: 0.09422782174097798\n",
      "Training loss: 0.09410179092048651\n",
      "Training loss: 0.09397576009999507\n",
      "Training loss: 0.09384972927950366\n",
      "Training loss: 0.09372369845901224\n",
      "Training loss: 0.09359766763852079\n",
      "Training loss: 0.09347163681802942\n",
      "Training loss: 0.09334560599753797\n",
      "Training loss: 0.09321957517704652\n",
      "Training loss: 0.0930935443565551\n",
      "Training loss: 0.09296751353606371\n",
      "Training loss: 0.09284148271557231\n",
      "Training loss: 0.0927154518950809\n",
      "Training loss: 0.09258942107458948\n",
      "Training loss: 0.09246339025409805\n",
      "Training loss: 0.09233735943360663\n",
      "Training loss: 0.09221132861311516\n",
      "Training loss: 0.09208529779262375\n",
      "Training loss: 0.09195926697213233\n",
      "Training loss: 0.09183323615164088\n",
      "Training loss: 0.09170720533114947\n",
      "Training loss: 0.09158117451065807\n",
      "Training loss: 0.09145514369016663\n",
      "Training loss: 0.09132911286967517\n",
      "Training loss: 0.09120308204918379\n",
      "Training loss: 0.09107705122869234\n",
      "Training loss: 0.09095102040820095\n",
      "Training loss: 0.09082498958770953\n",
      "Training loss: 0.09069895876721815\n",
      "Training loss: 0.0905729279467267\n",
      "Training loss: 0.0904468971262353\n",
      "Training loss: 0.09032086630574383\n",
      "Training loss: 0.09019483548525241\n",
      "Training loss: 0.090068804664761\n",
      "Training loss: 0.08994277384426957\n",
      "Training loss: 0.08981674302377812\n",
      "Training loss: 0.08969071220328673\n",
      "Training loss: 0.08956468138279533\n",
      "Training loss: 0.08943865056230385\n",
      "Training loss: 0.08931261974181243\n",
      "Training loss: 0.089186588921321\n",
      "Training loss: 0.0890605581008296\n",
      "Training loss: 0.0889345272803382\n",
      "Training loss: 0.0888084964598468\n",
      "Training loss: 0.08868246563935536\n",
      "Training loss: 0.08855643481886397\n",
      "Training loss: 0.08843040399837251\n",
      "Training loss: 0.08830437317788106\n",
      "Training loss: 0.0881783423573897\n",
      "Training loss: 0.08805231153689824\n",
      "Training loss: 0.08792628071640683\n",
      "Training loss: 0.08780024989591541\n",
      "Training loss: 0.08767421907542396\n",
      "Training loss: 0.08754818825493249\n",
      "Training loss: 0.08742215743444111\n",
      "Training loss: 0.0872961266139497\n",
      "Training loss: 0.08717009579345826\n",
      "Training loss: 0.08704406497296685\n",
      "Training loss: 0.08691803415247543\n",
      "Training loss: 0.08679200333198404\n",
      "Training loss: 0.08666597251149262\n",
      "Training loss: 0.08653994169100115\n",
      "Training loss: 0.08641391087050974\n",
      "Training loss: 0.0862878800500183\n",
      "Training loss: 0.08616184922952691\n",
      "Training loss: 0.0860358184090355\n",
      "Training loss: 0.08590978758854406\n",
      "Training loss: 0.08578375676805262\n",
      "Training loss: 0.08565772594756119\n",
      "Training loss: 0.08553169512706978\n",
      "Training loss: 0.08540566430657837\n",
      "Training loss: 0.08527963348608694\n",
      "Training loss: 0.0851536026655955\n",
      "Training loss: 0.0850275718451041\n",
      "Training loss: 0.08490154102461268\n",
      "Training loss: 0.08477551020412126\n",
      "Training loss: 0.08464947938362982\n",
      "Training loss: 0.08452344856313843\n",
      "Training loss: 0.084397417742647\n",
      "Training loss: 0.08427138692215559\n",
      "Training loss: 0.08414535610166415\n",
      "Training loss: 0.08401932528117273\n",
      "Training loss: 0.08389329446068128\n",
      "Training loss: 0.08376726364018987\n",
      "Training loss: 0.08364123281969843\n",
      "Training loss: 0.08351520199920699\n",
      "Training loss: 0.08338917117871557\n",
      "Training loss: 0.08326314035822413\n",
      "Training loss: 0.08313710953773276\n",
      "Training loss: 0.08301107871724132\n",
      "Training loss: 0.08288504789674991\n",
      "Training loss: 0.08275901707625848\n",
      "Training loss: 0.08263298625576708\n",
      "Training loss: 0.08250695543527564\n",
      "Training loss: 0.08238092461478423\n",
      "Training loss: 0.08225489379429282\n",
      "Training loss: 0.08212886297380141\n",
      "Training loss: 0.08200283215330999\n",
      "Training loss: 0.08187680133281852\n",
      "Training loss: 0.08175077051232708\n",
      "Training loss: 0.08162473969183569\n",
      "Training loss: 0.08149870887134425\n",
      "Training loss: 0.0813726780508528\n",
      "Training loss: 0.08124664723036142\n",
      "Training loss: 0.08112061640986998\n",
      "Training loss: 0.08099458558937854\n",
      "Training loss: 0.08086855476888713\n",
      "Training loss: 0.08074252394839572\n",
      "Training loss: 0.0806164931279043\n",
      "Training loss: 0.08049046230741289\n",
      "Training loss: 0.08036443148692149\n",
      "Training loss: 0.08023840066643004\n",
      "Training loss: 0.08011236984593863\n",
      "Training loss: 0.07998633902544722\n",
      "Training loss: 0.07986030820495577\n",
      "Training loss: 0.0797342773844643\n",
      "Training loss: 0.0796082465639729\n",
      "Training loss: 0.0794822157434815\n",
      "Training loss: 0.07935618492299006\n",
      "Training loss: 0.07923015410249863\n",
      "Training loss: 0.07910412328200722\n",
      "Training loss: 0.07897809246151578\n",
      "Training loss: 0.0788520616410244\n",
      "Training loss: 0.07872603082053298\n",
      "Training loss: 0.07860000000004154\n",
      "Training loss: 0.07847396917955014\n",
      "Training loss: 0.07834793835905872\n",
      "Training loss: 0.07822190753856725\n",
      "Training loss: 0.07809587671807589\n",
      "Training loss: 0.07796984589758443\n",
      "Training loss: 0.07784381507709302\n",
      "Training loss: 0.07771778425660156\n",
      "Training loss: 0.07759175343611016\n",
      "Training loss: 0.07746572261561874\n",
      "Training loss: 0.07733969179512729\n",
      "Training loss: 0.07721366097463588\n",
      "Training loss: 0.07708763015414444\n",
      "Training loss: 0.07696159933365304\n",
      "Training loss: 0.07683556851316162\n",
      "Training loss: 0.07670953769267022\n",
      "Training loss: 0.07658350687217881\n",
      "Training loss: 0.07645747605168739\n",
      "Training loss: 0.07633144523119595\n",
      "Training loss: 0.07620541441070453\n",
      "Training loss: 0.0760793835902131\n",
      "Training loss: 0.0759533527697217\n",
      "Training loss: 0.07582732194923021\n",
      "Training loss: 0.07570129112873884\n",
      "Training loss: 0.0755752603082474\n",
      "Training loss: 0.07544922948775597\n",
      "Training loss: 0.07532319866726454\n",
      "Training loss: 0.07519716784677312\n",
      "Training loss: 0.0750711370262817\n",
      "Training loss: 0.07494510620579029\n",
      "Training loss: 0.07481907538529886\n",
      "Training loss: 0.07469304456480746\n",
      "Training loss: 0.07456701374431604\n",
      "Training loss: 0.07444098292382463\n",
      "Training loss: 0.07431495210333319\n",
      "Training loss: 0.07418892128284177\n",
      "Training loss: 0.07406289046235034\n",
      "Training loss: 0.07393685964185892\n",
      "Training loss: 0.07381082882136751\n",
      "Training loss: 0.07368479800087605\n",
      "Training loss: 0.07355876718038462\n",
      "Training loss: 0.07343273635989322\n",
      "Training loss: 0.07330670553940179\n",
      "Training loss: 0.07318067471891036\n",
      "Training loss: 0.07305464389841894\n",
      "Training loss: 0.0729286130779275\n",
      "Training loss: 0.07280258225743606\n",
      "Training loss: 0.0726765514369447\n",
      "Training loss: 0.07255052061645326\n",
      "Training loss: 0.07242448979596185\n",
      "Training loss: 0.07229845897547044\n",
      "Training loss: 0.07217242815497898\n",
      "Training loss: 0.07204639733448757\n",
      "Training loss: 0.07192036651399612\n",
      "Training loss: 0.07179433569350473\n",
      "Training loss: 0.0716683048730133\n",
      "Training loss: 0.07154227405252186\n",
      "Training loss: 0.07141624323203045\n",
      "Training loss: 0.071290212411539\n",
      "Training loss: 0.07116418159104758\n",
      "Training loss: 0.07103815077055615\n",
      "Training loss: 0.07091211995006473\n",
      "Training loss: 0.07078608912957333\n",
      "Training loss: 0.07066005830908191\n",
      "Training loss: 0.07053402748859053\n",
      "Training loss: 0.07040799666809909\n",
      "Training loss: 0.07028196584760767\n",
      "Training loss: 0.07015593502711626\n",
      "Training loss: 0.07002990420662478\n",
      "Training loss: 0.06990387338613338\n",
      "Training loss: 0.06977784256564196\n",
      "Training loss: 0.06965181174515053\n",
      "Training loss: 0.06952578092465911\n",
      "Training loss: 0.06939975010416768\n",
      "Training loss: 0.06927371928367626\n",
      "Training loss: 0.06914768846318484\n",
      "Training loss: 0.06902165764269341\n",
      "Training loss: 0.06889562682220199\n",
      "Training loss: 0.06876959600171055\n",
      "Training loss: 0.06864356518121918\n",
      "Training loss: 0.06851753436072776\n",
      "Training loss: 0.06839150354023633\n",
      "Training loss: 0.06826547271974488\n",
      "Training loss: 0.06813944189925347\n",
      "Training loss: 0.06801341107876203\n",
      "Training loss: 0.06788738025827062\n",
      "Training loss: 0.0677613494377792\n",
      "Training loss: 0.06763531861728776\n",
      "Training loss: 0.06750928779679635\n",
      "Training loss: 0.06738325697630491\n",
      "Training loss: 0.06725722615581349\n",
      "Training loss: 0.06713119533532205\n",
      "Training loss: 0.06700516451483063\n",
      "Training loss: 0.06687913369433923\n",
      "Training loss: 0.0667531028738478\n",
      "Training loss: 0.06662707205335643\n",
      "Training loss: 0.066501041232865\n",
      "Training loss: 0.06637501041237355\n",
      "Training loss: 0.06624897959188214\n",
      "Training loss: 0.06612294877139072\n",
      "Training loss: 0.06599691795089932\n",
      "Training loss: 0.06587088713040785\n",
      "Training loss: 0.06574485630991644\n",
      "Training loss: 0.065618825489425\n",
      "Training loss: 0.0654927946689336\n",
      "Training loss: 0.06536676384844217\n",
      "Training loss: 0.06524073302795072\n",
      "Training loss: 0.06511470220745931\n",
      "Training loss: 0.06498867138696789\n",
      "Training loss: 0.06486264056647646\n",
      "Training loss: 0.06473660974598507\n",
      "Training loss: 0.06461057892549366\n",
      "Training loss: 0.06448454810500225\n",
      "Training loss: 0.06435851728451081\n",
      "Training loss: 0.0642324864640194\n",
      "Training loss: 0.06410645564352796\n",
      "Training loss: 0.06398042482303654\n",
      "Training loss: 0.06385439400254513\n",
      "Training loss: 0.06372836318205369\n",
      "Training loss: 0.06360233236156225\n",
      "Training loss: 0.06347630154107081\n",
      "Training loss: 0.0633502707205794\n",
      "Training loss: 0.06322423990008795\n",
      "Training loss: 0.06309820907959654\n",
      "Training loss: 0.06297217825910512\n",
      "Training loss: 0.0628461474386137\n",
      "Training loss: 0.06272011661812231\n",
      "Training loss: 0.06259408579763087\n",
      "Training loss: 0.062468054977139476\n",
      "Training loss: 0.06234202415664805\n",
      "Training loss: 0.06221599333615664\n",
      "Training loss: 0.0620899625156652\n",
      "Training loss: 0.06196393169517376\n",
      "Training loss: 0.06183790087468235\n",
      "Training loss: 0.061711870054190904\n",
      "Training loss: 0.061585839233699494\n",
      "Training loss: 0.06145980841320806\n",
      "Training loss: 0.06133377759271665\n",
      "Training loss: 0.0612077467722252\n",
      "Training loss: 0.06108171595173378\n",
      "Training loss: 0.06095568513124236\n",
      "Training loss: 0.06082965431075097\n",
      "Training loss: 0.060703623490259526\n",
      "Training loss: 0.06057759266976812\n",
      "Training loss: 0.06045156184927672\n",
      "Training loss: 0.06032553102878531\n",
      "Training loss: 0.06019950020829388\n",
      "Training loss: 0.060073469387802406\n",
      "Training loss: 0.059947438567311\n",
      "Training loss: 0.05982140774681955\n",
      "Training loss: 0.05969537692632815\n",
      "Training loss: 0.05956934610583674\n",
      "Training loss: 0.0594433152853453\n",
      "Training loss: 0.059317284464853876\n",
      "Training loss: 0.05919125364436244\n",
      "Training loss: 0.05906522282387102\n",
      "Training loss: 0.05893919200337962\n",
      "Training loss: 0.058813161182888186\n",
      "Training loss: 0.05868713036239678\n",
      "Training loss: 0.05856109954190538\n",
      "Training loss: 0.05843506872141395\n",
      "Training loss: 0.05830903790092251\n",
      "Training loss: 0.05818300708043109\n",
      "Training loss: 0.058056976259939656\n",
      "Training loss: 0.05793094543944822\n",
      "Training loss: 0.0578049146189568\n",
      "Training loss: 0.05767888379846539\n",
      "Training loss: 0.05755285297797398\n",
      "Training loss: 0.05742682215748254\n",
      "Training loss: 0.05730079133699112\n",
      "Training loss: 0.05717476051649967\n",
      "Training loss: 0.057048729696008244\n",
      "Training loss: 0.05692269887551683\n",
      "Training loss: 0.05679666805502542\n",
      "Training loss: 0.056670637234534006\n",
      "Training loss: 0.056544606414042596\n",
      "Training loss: 0.05641857559355118\n",
      "Training loss: 0.056292544773059776\n",
      "Training loss: 0.05616651395256834\n",
      "Training loss: 0.0560404831320769\n",
      "Training loss: 0.05591445231158548\n",
      "Training loss: 0.055788421491094066\n",
      "Training loss: 0.05566239067060263\n",
      "Training loss: 0.055536359850111204\n",
      "Training loss: 0.055410329029619766\n",
      "Training loss: 0.05528429820912836\n",
      "Training loss: 0.05515826738863691\n",
      "Training loss: 0.055032236568145494\n",
      "Training loss: 0.054906205747654084\n",
      "Training loss: 0.054780174927162674\n",
      "Training loss: 0.054654144106671264\n",
      "Training loss: 0.05452811328617983\n",
      "Training loss: 0.05440208246568842\n",
      "Training loss: 0.054276051645197\n",
      "Training loss: 0.054150020824705596\n",
      "Training loss: 0.05402399000421413\n",
      "Training loss: 0.05389795918372271\n",
      "Training loss: 0.05377192836323129\n",
      "Training loss: 0.05364589754273986\n",
      "Training loss: 0.053519866722248455\n",
      "Training loss: 0.05339383590175701\n",
      "Training loss: 0.053267805081265586\n",
      "Training loss: 0.05314177426077416\n",
      "Training loss: 0.05301574344028274\n",
      "Training loss: 0.05288971261979132\n",
      "Training loss: 0.052763681799299904\n",
      "Training loss: 0.0526376509788085\n",
      "Training loss: 0.052511620158317084\n",
      "Training loss: 0.05238558933782568\n",
      "Training loss: 0.05225955851733424\n",
      "Training loss: 0.05213352769684281\n",
      "Training loss: 0.05200749687635138\n",
      "Training loss: 0.051881466055859964\n",
      "Training loss: 0.05175543523536854\n",
      "Training loss: 0.05162940441487711\n",
      "Training loss: 0.051503373594385664\n",
      "Training loss: 0.051377342773894254\n",
      "Training loss: 0.051251311953402844\n",
      "Training loss: 0.05112528113291141\n",
      "Training loss: 0.05099925031241998\n",
      "Training loss: 0.05087321949192855\n",
      "Training loss: 0.05074718867143715\n",
      "Training loss: 0.05062115785094572\n",
      "Training loss: 0.050495127030454334\n",
      "Training loss: 0.0503690962099629\n",
      "Training loss: 0.05024306538947147\n",
      "Training loss: 0.05011703456898005\n",
      "Training loss: 0.04999100374848863\n",
      "Training loss: 0.049864972927997187\n",
      "Training loss: 0.049738942107505776\n",
      "Training loss: 0.04961291128701434\n",
      "Training loss: 0.049486880466522915\n",
      "Training loss: 0.049360849646031504\n",
      "Training loss: 0.04923481882554008\n",
      "Training loss: 0.04910878800504864\n",
      "Training loss: 0.04898275718455722\n",
      "Training loss: 0.048856726364065794\n",
      "Training loss: 0.048730695543574364\n",
      "Training loss: 0.048604664723082974\n",
      "Training loss: 0.04847863390259155\n",
      "Training loss: 0.04835260308210015\n",
      "Training loss: 0.0482265722616087\n",
      "Training loss: 0.048100541441117306\n",
      "Training loss: 0.04797451062062585\n",
      "Training loss: 0.04784847980013444\n",
      "Training loss: 0.047722448979643006\n",
      "Training loss: 0.04759641815915158\n",
      "Training loss: 0.04747038733866016\n",
      "Training loss: 0.047344356518168734\n",
      "Training loss: 0.04721832569767731\n",
      "Training loss: 0.04709229487718588\n",
      "Training loss: 0.046966264056694434\n",
      "Training loss: 0.04684023323620304\n",
      "Training loss: 0.046714202415711614\n",
      "Training loss: 0.04658817159522023\n",
      "Training loss: 0.046462140774728794\n",
      "Training loss: 0.046336109954237384\n",
      "Training loss: 0.04621007913374597\n",
      "Training loss: 0.04608404831325453\n",
      "Training loss: 0.0459580174927631\n",
      "Training loss: 0.04583198667227168\n",
      "Training loss: 0.04570595585178025\n",
      "Training loss: 0.04557992503128885\n",
      "Training loss: 0.04545389421079744\n",
      "Training loss: 0.045327863390306034\n",
      "Training loss: 0.04520183256981465\n",
      "Training loss: 0.04507580174932322\n",
      "Training loss: 0.044949770928831796\n",
      "Training loss: 0.04482374010834041\n",
      "Training loss: 0.04469770928784899\n",
      "Training loss: 0.04457167846735759\n",
      "Training loss: 0.044445647646866177\n",
      "Training loss: 0.044319616826374766\n",
      "Training loss: 0.04419358600588334\n",
      "Training loss: 0.04406755518539194\n",
      "Training loss: 0.04394152436490052\n",
      "Training loss: 0.04381549354440914\n",
      "Training loss: 0.043689462723917716\n",
      "Training loss: 0.04356343190342629\n",
      "Training loss: 0.0434374010829349\n",
      "Training loss: 0.04331137026244353\n",
      "Training loss: 0.043185339441952096\n",
      "Training loss: 0.043059308621460686\n",
      "Training loss: 0.04293327780096926\n",
      "Training loss: 0.042807246980477866\n",
      "Training loss: 0.042681216159986456\n",
      "Training loss: 0.04255518533949504\n",
      "Training loss: 0.042429154519003635\n",
      "Training loss: 0.04230312369851221\n",
      "Training loss: 0.042177092878020794\n",
      "Training loss: 0.042051062057529405\n",
      "Training loss: 0.041925031237038\n",
      "Training loss: 0.0417990004165466\n",
      "Training loss: 0.04167296959605519\n",
      "Training loss: 0.04154693877556376\n",
      "Training loss: 0.041420907955072375\n",
      "Training loss: 0.04129487713458095\n",
      "Training loss: 0.04116884631408955\n",
      "Training loss: 0.04104281549359813\n",
      "Training loss: 0.04091678467310672\n",
      "Training loss: 0.04079075385261532\n",
      "Training loss: 0.040664723032123914\n",
      "Training loss: 0.040538692211632504\n",
      "Training loss: 0.0404126613911411\n",
      "Training loss: 0.04028663057064968\n",
      "Training loss: 0.04016059975015828\n",
      "Training loss: 0.04003456892966686\n",
      "Training loss: 0.03990853810917544\n",
      "Training loss: 0.039782507288684044\n",
      "Training loss: 0.039656476468192654\n",
      "Training loss: 0.03953044564770123\n",
      "Training loss: 0.03940441482720981\n",
      "Training loss: 0.03927838400671842\n",
      "Training loss: 0.039152353186226986\n",
      "Training loss: 0.0390263223657356\n",
      "Training loss: 0.03890029154524419\n",
      "Training loss: 0.03877426072475279\n",
      "Training loss: 0.038648229904261366\n",
      "Training loss: 0.03852219908376995\n",
      "Training loss: 0.038396168263278574\n",
      "Training loss: 0.038270137442787136\n",
      "Training loss: 0.03814410662229573\n",
      "Training loss: 0.038018075801804316\n",
      "Training loss: 0.0378920449813129\n",
      "Training loss: 0.0377660141608215\n",
      "Training loss: 0.037639983340330085\n",
      "Training loss: 0.037513952519838696\n",
      "Training loss: 0.037387921699347265\n",
      "Training loss: 0.03726189087885588\n",
      "Training loss: 0.037135860058364466\n",
      "Training loss: 0.037009829237873056\n",
      "Training loss: 0.03688379841738166\n",
      "Training loss: 0.03675776759689024\n",
      "Training loss: 0.03663173677639883\n",
      "Training loss: 0.03650570595590743\n",
      "Training loss: 0.036379675135416026\n",
      "Training loss: 0.036253644314924595\n",
      "Training loss: 0.03612761349443319\n",
      "Training loss: 0.0360015826739418\n",
      "Training loss: 0.03587555185345037\n",
      "Training loss: 0.035749521032959\n",
      "Training loss: 0.03562349021246755\n",
      "Training loss: 0.035497459391976155\n",
      "Training loss: 0.03537142857148473\n",
      "Training loss: 0.03524539775099335\n",
      "Training loss: 0.03511936693050194\n",
      "Training loss: 0.03499333611001052\n",
      "Training loss: 0.034867305289519104\n",
      "Training loss: 0.03474127446902768\n",
      "Training loss: 0.0346152436485363\n",
      "Training loss: 0.034489212828044895\n",
      "Training loss: 0.03436318200755348\n",
      "Training loss: 0.03423715118706209\n",
      "Training loss: 0.03411112036657066\n",
      "Training loss: 0.03398508954607926\n",
      "Training loss: 0.03385905872558784\n",
      "Training loss: 0.03373302790509643\n",
      "Training loss: 0.03360699708460503\n",
      "Training loss: 0.03348096626411361\n",
      "Training loss: 0.0333549354436222\n",
      "Training loss: 0.033228904623130766\n",
      "Training loss: 0.033102873802639404\n",
      "Training loss: 0.03297684298214799\n",
      "Training loss: 0.032850812161656584\n",
      "Training loss: 0.03272478134116517\n",
      "Training loss: 0.032598750520673764\n",
      "Training loss: 0.032472719700182354\n",
      "Training loss: 0.03234668887969094\n",
      "Training loss: 0.03222065805919952\n",
      "Training loss: 0.032094627238708116\n",
      "Training loss: 0.03196859641821672\n",
      "Training loss: 0.03184256559772528\n",
      "Training loss: 0.03171653477723388\n",
      "Training loss: 0.031590503956742476\n",
      "Training loss: 0.03146447313625107\n",
      "Training loss: 0.03133844231575967\n",
      "Training loss: 0.031212411495268242\n",
      "Training loss: 0.03108638067477684\n",
      "Training loss: 0.030960349854285432\n",
      "Training loss: 0.030834319033794043\n",
      "Training loss: 0.03070828821330263\n",
      "Training loss: 0.030582257392811243\n",
      "Training loss: 0.030456226572319823\n",
      "Training loss: 0.030330195751828402\n",
      "Training loss: 0.030204164931337006\n",
      "Training loss: 0.03007813411084559\n",
      "Training loss: 0.029952103290354196\n",
      "Training loss: 0.029826072469862748\n",
      "Training loss: 0.029700041649371345\n",
      "Training loss: 0.02957401082887994\n",
      "Training loss: 0.029447980008388525\n",
      "Training loss: 0.02932194918789713\n",
      "Training loss: 0.029195918367405715\n",
      "Training loss: 0.02906988754691433\n",
      "Training loss: 0.028943856726422898\n",
      "Training loss: 0.028817825905931488\n",
      "Training loss: 0.02869179508544008\n",
      "Training loss: 0.028565764264948675\n",
      "Training loss: 0.02843973344445727\n",
      "Training loss: 0.028313702623965903\n",
      "Training loss: 0.028187671803474448\n",
      "Training loss: 0.028061640982983044\n",
      "Training loss: 0.027935610162491638\n",
      "Training loss: 0.027809579342000214\n",
      "Training loss: 0.02768354852150881\n",
      "Training loss: 0.02755751770101739\n",
      "Training loss: 0.027431486880526008\n",
      "Training loss: 0.027305456060034556\n",
      "Training loss: 0.02717942523954318\n",
      "Training loss: 0.027053394419051777\n",
      "Training loss: 0.02692736359856037\n",
      "Training loss: 0.026801332778068954\n",
      "Training loss: 0.026675301957577554\n",
      "Training loss: 0.02654927113708614\n",
      "Training loss: 0.02642324031659472\n",
      "Training loss: 0.0262972094961033\n",
      "Training loss: 0.026171178675611913\n",
      "Training loss: 0.026045147855120482\n",
      "Training loss: 0.025919117034629097\n",
      "Training loss: 0.02579308621413767\n",
      "Training loss: 0.02566705539364629\n",
      "Training loss: 0.02554102457315487\n",
      "Training loss: 0.025414993752663446\n",
      "Training loss: 0.02528896293217204\n",
      "Training loss: 0.025162932111680646\n",
      "Training loss: 0.025036901291189243\n",
      "Training loss: 0.024910870470697826\n",
      "Training loss: 0.02478483965020641\n",
      "Training loss: 0.024658808829714992\n",
      "Training loss: 0.024532778009223603\n",
      "Training loss: 0.0244067471887322\n",
      "Training loss: 0.024280716368240755\n",
      "Training loss: 0.024154685547749365\n",
      "Training loss: 0.024028654727257955\n",
      "Training loss: 0.02390262390676656\n",
      "Training loss: 0.023776593086275152\n",
      "Training loss: 0.02365056226578373\n",
      "Training loss: 0.023524531445292336\n",
      "Training loss: 0.023398500624800936\n",
      "Training loss: 0.023272469804309512\n",
      "Training loss: 0.02314643898381812\n",
      "Training loss: 0.02302040816332667\n",
      "Training loss: 0.022894377342835292\n",
      "Training loss: 0.022768346522343864\n",
      "Training loss: 0.02264231570185247\n",
      "Training loss: 0.02251628488136107\n",
      "Training loss: 0.022390254060869683\n",
      "Training loss: 0.022264223240378245\n",
      "Training loss: 0.022138192419886835\n",
      "Training loss: 0.02201216159939543\n",
      "Training loss: 0.021886130778904004\n",
      "Training loss: 0.021760099958412608\n",
      "Training loss: 0.021634069137921187\n",
      "Training loss: 0.02150803831742979\n",
      "Training loss: 0.021382007496938363\n",
      "Training loss: 0.021255976676446978\n",
      "Training loss: 0.021129945855955557\n",
      "Training loss: 0.02100391503546416\n",
      "Training loss: 0.02087788421497276\n",
      "Training loss: 0.02075185339448133\n",
      "Training loss: 0.020625822573989917\n",
      "Training loss: 0.020499791753498534\n",
      "Training loss: 0.020373760933007117\n",
      "Training loss: 0.020247730112515693\n",
      "Training loss: 0.020121699292024297\n",
      "Training loss: 0.01999566847153288\n",
      "Training loss: 0.019869637651041466\n",
      "Training loss: 0.019743606830550046\n",
      "Training loss: 0.019617576010058656\n",
      "Training loss: 0.019491545189567232\n",
      "Training loss: 0.01936551436907584\n",
      "Training loss: 0.019239483548584423\n",
      "Training loss: 0.019113452728093023\n",
      "Training loss: 0.01898742190760163\n",
      "Training loss: 0.018861391087110213\n",
      "Training loss: 0.018735360266618782\n",
      "Training loss: 0.018609329446127393\n",
      "Training loss: 0.018483298625635993\n",
      "Training loss: 0.018357267805144573\n",
      "Training loss: 0.018231236984653156\n",
      "Training loss: 0.01810520616416175\n",
      "Training loss: 0.01797917534367035\n",
      "Training loss: 0.017853144523178936\n",
      "Training loss: 0.01772711370268753\n",
      "Training loss: 0.017601082882196122\n",
      "Training loss: 0.017475052061704716\n",
      "Training loss: 0.017349021241213302\n",
      "Training loss: 0.017222990420721892\n",
      "Training loss: 0.017096959600230503\n",
      "Training loss: 0.016970928779739075\n",
      "Training loss: 0.016844897959247665\n",
      "Training loss: 0.01671886713875626\n",
      "Training loss: 0.016592836318264862\n",
      "Training loss: 0.016466805497773428\n",
      "Training loss: 0.01634077467728202\n",
      "Training loss: 0.016214743856790628\n",
      "Training loss: 0.016088713036299218\n",
      "Training loss: 0.01596268221580781\n",
      "Training loss: 0.0158366513953164\n",
      "Training loss: 0.015710620574824995\n",
      "Training loss: 0.015584589754333583\n",
      "Training loss: 0.015458558933842157\n",
      "Training loss: 0.015332528113350771\n",
      "Training loss: 0.01520649729285935\n",
      "Training loss: 0.015080466472367948\n",
      "Training loss: 0.014954435651876541\n",
      "Training loss: 0.014828404831385122\n",
      "Training loss: 0.014702374010893702\n",
      "Training loss: 0.01457634319040233\n",
      "Training loss: 0.014450312369910892\n",
      "Training loss: 0.014324281549419502\n",
      "Training loss: 0.014198250728928085\n",
      "Training loss: 0.014072219908436687\n",
      "Training loss: 0.013946189087945262\n",
      "Training loss: 0.013820158267453843\n",
      "Training loss: 0.013694127446962459\n",
      "Training loss: 0.013568096626471042\n",
      "Training loss: 0.013442065805979628\n",
      "Training loss: 0.013316034985488216\n",
      "Training loss: 0.013190004164996794\n",
      "Training loss: 0.01306397334450542\n",
      "Training loss: 0.01293794252401401\n",
      "Training loss: 0.012811911703522588\n",
      "Training loss: 0.012685880883031181\n",
      "Training loss: 0.012559850062539759\n",
      "Training loss: 0.012433819242048371\n",
      "Training loss: 0.012307788421556972\n",
      "Training loss: 0.01218175760106553\n",
      "Training loss: 0.01205572678057412\n",
      "Training loss: 0.011929695960082727\n",
      "Training loss: 0.011803665139591342\n",
      "Training loss: 0.011677634319099899\n",
      "Training loss: 0.011551603498608502\n",
      "Training loss: 0.01142557267811709\n",
      "Training loss: 0.011299541857625656\n",
      "Training loss: 0.011173511037134307\n",
      "Training loss: 0.011047480216642855\n",
      "Training loss: 0.010921449396151448\n",
      "Training loss: 0.010795418575660036\n",
      "Training loss: 0.010669387755168649\n",
      "Training loss: 0.010543356934677214\n",
      "Training loss: 0.010417326114185794\n",
      "Training loss: 0.010291295293694422\n",
      "Training loss: 0.01016526447320299\n",
      "Training loss: 0.010039233652711577\n",
      "Training loss: 0.009913202832220172\n",
      "Training loss: 0.009787172011728781\n",
      "Training loss: 0.009661141191237356\n",
      "Training loss: 0.009535110370745954\n",
      "Training loss: 0.009409079550254561\n",
      "Training loss: 0.00928304872976312\n",
      "Training loss: 0.009157017909271722\n",
      "Training loss: 0.009030987088780328\n",
      "Training loss: 0.008904956268288928\n",
      "Training loss: 0.008778925447797507\n",
      "Training loss: 0.008652894627306104\n",
      "Training loss: 0.008526863806814654\n",
      "Training loss: 0.008400832986323289\n",
      "Training loss: 0.008274802165831895\n",
      "Training loss: 0.008148771345340467\n",
      "Training loss: 0.008022740524849047\n",
      "Training loss: 0.00789670970435765\n",
      "Training loss: 0.007770678883866244\n",
      "Training loss: 0.007644648063374825\n",
      "Training loss: 0.007518617242883428\n",
      "Training loss: 0.007392586422391992\n",
      "Training loss: 0.0072665556019006\n",
      "Training loss: 0.007140524781409199\n",
      "Training loss: 0.007014493960917792\n",
      "Training loss: 0.0068884631404263754\n",
      "Training loss: 0.006762432319934968\n",
      "Training loss: 0.006636401499443559\n",
      "Training loss: 0.00651037067895214\n",
      "Training loss: 0.006384339858460752\n",
      "Training loss: 0.006258309037969334\n",
      "Training loss: 0.006132278217477929\n",
      "Training loss: 0.006006247396986502\n",
      "Training loss: 0.005880216576495112\n",
      "Training loss: 0.00575418575600368\n",
      "Training loss: 0.005628154935512298\n",
      "Training loss: 0.005502124115020876\n",
      "Training loss: 0.005376093294529467\n",
      "Training loss: 0.005250062474038064\n",
      "Training loss: 0.0051240316535466615\n",
      "Training loss: 0.004998000833055238\n",
      "Training loss: 0.004871970012563838\n",
      "Training loss: 0.004745939192072435\n",
      "Training loss: 0.004619908371581001\n",
      "Training loss: 0.004493877551089595\n",
      "Training loss: 0.004367846730598195\n",
      "Training loss: 0.004241815910106785\n",
      "Training loss: 0.004115785089615362\n",
      "Training loss: 0.003989754269123975\n",
      "Training loss: 0.0038637234486325655\n",
      "Training loss: 0.0037376926281411805\n",
      "Training loss: 0.0036116618076497487\n",
      "Training loss: 0.0034856309871583568\n",
      "Training loss: 0.0033596001666669362\n",
      "Training loss: 0.003233569346175538\n",
      "Training loss: 0.0031075385256841303\n",
      "Training loss: 0.002981507705192694\n",
      "Training loss: 0.002855476884701298\n",
      "Training loss: 0.0027294460642098817\n",
      "Training loss: 0.002603415243718479\n",
      "Training loss: 0.0024773844232270515\n",
      "Training loss: 0.002351353602735664\n",
      "Training loss: 0.002225322782244239\n",
      "Training loss: 0.002099291961752847\n",
      "Training loss: 0.0019732611412614353\n",
      "Training loss: 0.0018472303207700369\n",
      "Training loss: 0.001721199500278614\n",
      "Training loss: 0.00159516867978721\n",
      "Training loss: 0.0014691378592957848\n",
      "Training loss: 0.0013431070388043864\n",
      "Training loss: 0.0012170762183130012\n",
      "Training loss: 0.0010910453978215718\n",
      "Training loss: 0.0009650145773301466\n",
      "Training loss: 0.0008389837568387525\n",
      "Training loss: 0.0007129529363473408\n",
      "Training loss: 0.0005869221158559601\n",
      "Training loss: 0.00046089129536452836\n",
      "Training loss: 0.0003348604748731365\n",
      "Training loss: 0.00020882965438169808\n",
      "Training loss: 8.279883389029407e-05\n",
      "Training loss: -4.3231986601084406e-05\n",
      "Training loss: -0.0001692628070924851\n",
      "Training loss: -0.0002952936275838924\n",
      "Training loss: -0.00042132444807535085\n",
      "Training loss: -0.0005473552685667071\n",
      "Training loss: -0.0006733860890581544\n",
      "Training loss: -0.0007994169095495506\n",
      "Training loss: -0.0009254477300409625\n",
      "Training loss: -0.00105147855053237\n",
      "Training loss: -0.0011775093710237904\n",
      "Training loss: -0.0013035401915151778\n",
      "Training loss: -0.0014295710120066252\n",
      "Training loss: -0.0015556018324980215\n",
      "Training loss: -0.0016816326529894199\n",
      "Training loss: -0.001807663473480834\n",
      "Training loss: -0.0019336942939722213\n",
      "Training loss: -0.0020597251144636596\n",
      "Training loss: -0.002185755934955056\n",
      "Training loss: -0.002311786755446457\n",
      "Training loss: -0.002437817575937846\n",
      "Training loss: -0.0025638483964292957\n",
      "Training loss: -0.0026898792169206807\n",
      "Training loss: -0.002815910037412104\n",
      "Training loss: -0.002941940857903489\n",
      "Training loss: -0.003067971678394912\n",
      "Training loss: -0.0031940024988863303\n",
      "Training loss: -0.0033200333193777174\n",
      "Training loss: -0.0034460641398691115\n",
      "Training loss: -0.0035720949603605303\n",
      "Training loss: -0.0036981257808519573\n",
      "Training loss: -0.0038241566013433627\n",
      "Training loss: -0.0039501874218347945\n",
      "Training loss: -0.0040762182423261465\n",
      "Training loss: -0.004202249062817583\n",
      "Training loss: -0.004328279883309003\n",
      "Training loss: -0.0044543107038004124\n",
      "Training loss: -0.004580341524291827\n",
      "Training loss: -0.004706372344783205\n",
      "Training loss: -0.0048324031652746255\n",
      "Training loss: -0.004958433985766016\n",
      "Training loss: -0.0050844648062574474\n",
      "Training loss: -0.005210495626748857\n",
      "Training loss: -0.005336526447240244\n",
      "Training loss: -0.005462557267731669\n",
      "Training loss: -0.005588588088223098\n",
      "Training loss: -0.005714618908714492\n",
      "Training loss: -0.0058406497292059255\n",
      "Training loss: -0.0059666805496972975\n",
      "Training loss: -0.006092711370188738\n",
      "Training loss: -0.006218742190680117\n",
      "Training loss: -0.006344773011171555\n",
      "Training loss: -0.006470803831662929\n",
      "Training loss: -0.0065968346521543255\n",
      "Training loss: -0.00672286547264577\n",
      "Training loss: -0.006848896293137178\n",
      "Training loss: -0.0069749271136285715\n",
      "Training loss: -0.007100957934119995\n",
      "Training loss: -0.007226988754611421\n",
      "Training loss: -0.0073530195751028374\n",
      "Training loss: -0.0074790503955942\n",
      "Training loss: -0.007605081216085616\n",
      "Training loss: -0.007731112036577042\n",
      "Training loss: -0.00785714285706847\n",
      "Training loss: -0.007983173677559845\n",
      "Training loss: -0.00810920449805127\n",
      "Training loss: -0.008235235318542655\n",
      "Training loss: -0.008361266139034072\n",
      "Training loss: -0.008487296959525497\n",
      "Training loss: -0.008613327780016887\n",
      "Training loss: -0.008739358600508276\n",
      "Training loss: -0.00886538942099973\n",
      "Training loss: -0.00899142024149111\n",
      "Training loss: -0.009117451061982496\n",
      "Training loss: -0.009243481882473942\n",
      "Training loss: -0.00936951270296536\n",
      "Training loss: -0.00949554352345674\n",
      "Training loss: -0.00962157434394817\n",
      "Training loss: -0.009747605164439559\n",
      "Training loss: -0.009873635984930969\n",
      "Training loss: -0.009999666805422372\n",
      "Training loss: -0.010125697625913824\n",
      "Training loss: -0.0102517284464052\n",
      "Training loss: -0.010377759266896656\n",
      "Training loss: -0.010503790087388023\n",
      "Training loss: -0.01062982090787944\n",
      "Training loss: -0.010755851728370855\n",
      "Training loss: -0.010881882548862269\n",
      "Training loss: -0.011007913369353686\n",
      "Training loss: -0.011133944189845075\n",
      "Training loss: -0.011259975010336572\n",
      "Training loss: -0.011386005830827873\n",
      "Training loss: -0.011512036651319285\n",
      "Training loss: -0.0116380674718107\n",
      "Training loss: -0.011764098292302117\n",
      "Training loss: -0.011890129112793508\n",
      "Training loss: -0.01201615993328496\n",
      "Training loss: -0.012142190753776327\n",
      "Training loss: -0.012268221574267761\n",
      "Training loss: -0.012394252394759178\n",
      "Training loss: -0.012520283215250585\n",
      "Training loss: -0.01264631403574197\n",
      "Training loss: -0.012772344856233391\n",
      "Training loss: -0.012898375676724791\n",
      "Training loss: -0.013024406497216208\n",
      "Training loss: -0.013150437317707611\n",
      "Training loss: -0.013276468138199004\n",
      "Training loss: -0.01340249895869039\n",
      "Training loss: -0.013528529779181842\n",
      "Training loss: -0.013654560599673245\n",
      "Training loss: -0.013780591420164665\n",
      "Training loss: -0.013906622240656079\n",
      "Training loss: -0.01403265306114745\n",
      "Training loss: -0.014158683881638894\n",
      "Training loss: -0.014284714702130297\n",
      "Training loss: -0.014410745522621692\n",
      "Training loss: -0.014536776343113107\n",
      "Training loss: -0.014662807163604514\n",
      "Training loss: -0.014788837984095901\n",
      "Training loss: -0.014914868804587327\n",
      "Training loss: -0.015040899625078747\n",
      "Training loss: -0.015166930445570168\n",
      "Training loss: -0.01529296126606154\n",
      "Training loss: -0.015418992086552943\n",
      "Training loss: -0.015545022907044398\n",
      "Training loss: -0.015671053727535803\n",
      "Training loss: -0.01579708454802719\n",
      "Training loss: -0.015923115368518603\n",
      "Training loss: -0.016049146189010023\n",
      "Training loss: -0.01617517700950142\n",
      "Training loss: -0.01630120782999281\n",
      "Training loss: -0.01642723865048424\n",
      "Training loss: -0.016553269470975632\n",
      "Training loss: -0.016679300291467036\n",
      "Training loss: -0.016805331111958456\n",
      "Training loss: -0.01693136193244987\n",
      "Training loss: -0.0170573927529413\n",
      "Training loss: -0.01718342357343259\n",
      "Training loss: -0.01730945439392412\n",
      "Training loss: -0.017435485214415514\n",
      "Training loss: -0.017561516034906934\n",
      "Training loss: -0.01768754685539835\n",
      "Training loss: -0.01781357767588976\n",
      "Training loss: -0.017939608496381106\n",
      "Training loss: -0.018065639316872547\n",
      "Training loss: -0.018191670137363957\n",
      "Training loss: -0.01831770095785537\n",
      "Training loss: -0.018443731778346753\n",
      "Training loss: -0.01856976259883818\n",
      "Training loss: -0.018695793419329573\n",
      "Training loss: -0.018821824239820997\n",
      "Training loss: -0.018947855060312414\n",
      "Training loss: -0.019073885880803835\n",
      "Training loss: -0.01919991670129524\n",
      "Training loss: -0.01932594752178665\n",
      "Training loss: -0.019451978342278017\n",
      "Training loss: -0.01957800916276946\n",
      "Training loss: -0.019704039983260854\n",
      "Training loss: -0.01983007080375226\n",
      "Training loss: -0.01995610162424369\n",
      "Training loss: -0.020082132444735102\n",
      "Training loss: -0.020208163265226498\n",
      "Training loss: -0.020334194085717936\n",
      "Training loss: -0.020460224906209315\n",
      "Training loss: -0.020586255726700708\n",
      "Training loss: -0.020712286547192118\n",
      "Training loss: -0.02083831736768353\n",
      "Training loss: -0.020964348188174907\n",
      "Training loss: -0.021090379008666373\n",
      "Training loss: -0.021216409829157745\n",
      "Training loss: -0.021342440649649175\n",
      "Training loss: -0.02146847147014058\n",
      "Training loss: -0.02159450229063202\n",
      "Training loss: -0.02172053311112344\n",
      "Training loss: -0.021846563931614806\n",
      "Training loss: -0.02197259475210624\n",
      "Training loss: -0.022098625572597605\n",
      "Training loss: -0.022224656393089067\n",
      "Training loss: -0.022350687213580463\n",
      "Training loss: -0.022476718034071853\n",
      "Training loss: -0.022602748854563277\n",
      "Training loss: -0.022728779675054683\n",
      "Training loss: -0.022854810495546055\n",
      "Training loss: -0.02298084131603746\n",
      "Training loss: -0.023106872136528997\n",
      "Training loss: -0.023232902957020317\n",
      "Training loss: -0.02335893377751173\n",
      "Training loss: -0.02348496459800313\n",
      "Training loss: -0.023610995418494544\n",
      "Training loss: -0.023737026238985947\n",
      "Training loss: -0.02386305705947735\n",
      "Training loss: -0.02398908787996874\n",
      "Training loss: -0.024115118700460167\n",
      "Training loss: -0.024241149520951577\n",
      "Training loss: -0.02436718034144298\n",
      "Training loss: -0.024493211161934397\n",
      "Training loss: -0.024619241982425804\n",
      "Training loss: -0.024745272802917197\n",
      "Training loss: -0.024871303623408635\n",
      "Training loss: -0.024997334443900003\n",
      "Training loss: -0.025123365264391406\n",
      "Training loss: -0.0252493960848829\n",
      "Training loss: -0.025375426905374265\n",
      "Training loss: -0.025501457725865654\n",
      "Training loss: -0.02562748854635709\n",
      "Training loss: -0.02575351936684849\n",
      "Training loss: -0.025879550187339905\n",
      "Training loss: -0.02600558100783132\n",
      "Training loss: -0.02613161182832269\n",
      "Training loss: -0.02625764264881411\n",
      "Training loss: -0.026383673469305532\n",
      "Training loss: -0.02650970428979693\n",
      "Training loss: -0.026635735110288355\n",
      "Training loss: -0.026761765930779755\n",
      "Training loss: -0.02688779675127118\n",
      "Training loss: -0.027013827571762537\n",
      "Training loss: -0.02713985839225403\n",
      "Training loss: -0.02726588921274538\n",
      "Training loss: -0.02739192003323682\n",
      "Training loss: -0.02751795085372821\n",
      "Training loss: -0.027643981674219605\n",
      "Training loss: -0.027770012494711026\n",
      "Training loss: -0.02789604331520244\n",
      "Training loss: -0.028022074135693846\n",
      "Training loss: -0.028148104956185253\n",
      "Training loss: -0.028274135776676656\n",
      "Training loss: -0.028400166597168094\n",
      "Training loss: -0.02852619741765946\n",
      "Training loss: -0.028652228238150855\n",
      "Training loss: -0.028778259058642272\n",
      "Training loss: -0.028904289879133644\n",
      "Training loss: -0.029030320699625144\n",
      "Training loss: -0.02915635152011654\n",
      "Training loss: -0.02928238234060788\n",
      "Training loss: -0.029408413161099337\n",
      "Training loss: -0.02953444398159077\n",
      "Training loss: -0.02966047480208215\n",
      "Training loss: -0.02978650562257357\n",
      "Training loss: -0.029912536443064967\n",
      "Training loss: -0.03003856726355638\n",
      "Training loss: -0.03016459808404778\n",
      "Training loss: -0.0302906289045392\n",
      "Training loss: -0.030416659725030614\n",
      "Training loss: -0.03054269054552202\n",
      "Training loss: -0.030668721366013427\n",
      "Training loss: -0.030794752186504817\n",
      "Training loss: -0.03092078300699628\n",
      "Training loss: -0.0310468138274876\n",
      "Training loss: -0.031172844647979047\n",
      "Training loss: -0.031298875468470475\n",
      "Training loss: -0.03142490628896187\n",
      "Training loss: -0.0315509371094533\n",
      "Training loss: -0.031676967929944684\n",
      "Training loss: -0.03180299875043609\n",
      "Training loss: -0.0319290295709275\n",
      "Training loss: -0.03205506039141893\n",
      "Training loss: -0.03218109121191035\n",
      "Training loss: -0.03230712203240174\n",
      "Training loss: -0.03243315285289314\n",
      "Training loss: -0.03255918367338455\n",
      "Training loss: -0.03268521449387595\n",
      "Training loss: -0.03281124531436737\n",
      "Training loss: -0.032937276134858806\n",
      "Training loss: -0.033063306955350195\n",
      "Training loss: -0.033189337775841585\n",
      "Training loss: -0.03331536859633302\n",
      "Training loss: -0.03344139941682443\n",
      "Training loss: -0.03356743023731583\n",
      "Training loss: -0.03369346105780724\n",
      "Training loss: -0.033819491878298635\n",
      "Training loss: -0.033945522698790045\n",
      "Training loss: -0.03407155351928149\n",
      "Training loss: -0.034197584339772845\n",
      "Training loss: -0.03432361516026429\n",
      "Training loss: -0.034449645980755665\n",
      "Training loss: -0.0345756768012471\n",
      "Training loss: -0.03470170762173848\n",
      "Training loss: -0.03482773844222993\n",
      "Training loss: -0.03495376926272128\n",
      "Training loss: -0.03507980008321272\n",
      "Training loss: -0.03520583090370415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: -0.035331861724195564\n",
      "Training loss: -0.03545789254468697\n",
      "Training loss: -0.03558392336517837\n",
      "Training loss: -0.035709954185669794\n",
      "Training loss: -0.035835985006161176\n",
      "Training loss: -0.03596201582665257\n",
      "Training loss: -0.03608804664714397\n",
      "Training loss: -0.03621407746763541\n",
      "Training loss: -0.03634010828812681\n",
      "Training loss: -0.03646613910861819\n",
      "Training loss: -0.03659216992910962\n",
      "Training loss: -0.03671820074960106\n",
      "Training loss: -0.03684423157009247\n",
      "Training loss: -0.03697026239058383\n",
      "Training loss: -0.037096293211075285\n",
      "Training loss: -0.037222324031566674\n",
      "Training loss: -0.037348354852058084\n",
      "Training loss: -0.037474385672549494\n",
      "Training loss: -0.037600416493040904\n",
      "Training loss: -0.0377264473135323\n",
      "Training loss: -0.03785247813402371\n",
      "Training loss: -0.03797850895451513\n",
      "Training loss: -0.03810453977500653\n",
      "Training loss: -0.03823057059549792\n",
      "Training loss: -0.03835660141598933\n",
      "Training loss: -0.038482632236480754\n",
      "Training loss: -0.03860866305697215\n",
      "Training loss: -0.038734693877463595\n",
      "Training loss: -0.03886072469795495\n",
      "Training loss: -0.03898675551844641\n",
      "Training loss: -0.03911278633893785\n",
      "Training loss: -0.03923881715942922\n",
      "Training loss: -0.03936484797992062\n",
      "Training loss: -0.03949087880041203\n",
      "Training loss: -0.03961690962090341\n",
      "Training loss: -0.03974294044139487\n",
      "Training loss: -0.039868971261886245\n",
      "Training loss: -0.03999500208237767\n",
      "Training loss: -0.04012103290286905\n",
      "Training loss: -0.04024706372336048\n",
      "Training loss: -0.04037309454385191\n",
      "Training loss: -0.04049912536434329\n",
      "Training loss: -0.0406251561848347\n",
      "Training loss: -0.040751187005326116\n",
      "Training loss: -0.040877217825817595\n",
      "Training loss: -0.041003248646308915\n",
      "Training loss: -0.04112927946680037\n",
      "Training loss: -0.04125531028729176\n",
      "Training loss: -0.04138134110778315\n",
      "Training loss: -0.04150737192827456\n",
      "Training loss: -0.04163340274876596\n",
      "Training loss: -0.04175943356925736\n",
      "Training loss: -0.04188546438974883\n",
      "Training loss: -0.0420114952102402\n",
      "Training loss: -0.04213752603073163\n",
      "Training loss: -0.042263556851223\n",
      "Training loss: -0.04238958767171444\n",
      "Training loss: -0.04251561849220582\n",
      "Training loss: -0.042641649312697254\n",
      "Training loss: -0.04276768013318865\n",
      "Training loss: -0.0428937109536801\n",
      "Training loss: -0.04301974177417146\n",
      "Training loss: -0.04314577259466288\n",
      "Training loss: -0.0432718034151543\n",
      "Training loss: -0.04339783423564572\n",
      "Training loss: -0.04352386505613712\n",
      "Training loss: -0.043649895876628514\n",
      "The final loss for this model: -0.043649895876628514\n"
     ]
    }
   ],
   "source": [
    "params = calculate_gradient() #returns the optimal weights and bias for the data\n",
    "y_pred = X*params[0] + params[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2670a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final loss for this model: -0.043649895876628514\n",
      "The final parameters are:\n",
      "weight:1.2979591836736346 , bias:-0.1959999999999947\n"
     ]
    }
   ],
   "source": [
    "#printing the params and the losses for the regression model.\n",
    "print(\"The final loss for this model: {}\".format(losses[-1]))\n",
    "print(\"The final parameters are:\\nweight:{} , bias:{}\".format(params[0],params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad22b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(X,y)\n",
    "plt.scatter(X,y_pred);\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend(['Actual','Predicted']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88513d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
